{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:18.483380Z",
     "iopub.status.busy": "2025-02-08T23:38:18.482623Z",
     "iopub.status.idle": "2025-02-08T23:38:19.466594Z",
     "shell.execute_reply": "2025-02-08T23:38:19.465483Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspyred\n",
    "from inspyred import ec, benchmarks\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import random\n",
    "import pynvml\n",
    "\n",
    "from scipy.stats import chisquare, kstest\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.473306Z",
     "iopub.status.busy": "2025-02-08T23:38:19.472853Z",
     "iopub.status.idle": "2025-02-08T23:38:19.483730Z",
     "shell.execute_reply": "2025-02-08T23:38:19.482779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean folder names\n",
    "def clean_folder_name(folder_name):\n",
    "    # Remove invalid characters\n",
    "    cleaned_name = re.sub(r'[<>:\"/\\\\|?*]', '', folder_name)\n",
    "    # Remove trailing dots and spaces\n",
    "    cleaned_name = cleaned_name.rstrip('. ')\n",
    "    return cleaned_name\n",
    "\n",
    "def CPU_monitor_memory_usage():\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_usage = memory_info.percent\n",
    "        \n",
    "    print(f\"CPU Current memory usage: {memory_usage}%\")\n",
    "\n",
    "    if memory_usage >= 100:\n",
    "        print(\"CPU Memory usage is too high. Pausing execution...\")\n",
    "        gc.collect()  # Trigger garbage collection manually\n",
    "        while memory_usage > 30:\n",
    "            time.sleep(10)\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            memory_usage = memory_info.percent\n",
    "        print(\"CPU Memory usage is low enough. Resuming execution...\")\n",
    "\n",
    "    # time.sleep(5)\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    # Initialize NVML\n",
    "    pynvml.nvmlInit()\n",
    "    \n",
    "    try:\n",
    "        # Get handle for the first GPU\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        # Get memory info\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        total_memory = mem_info.total\n",
    "        used_memory = mem_info.used\n",
    "\n",
    "        # Calculate the percentage of GPU memory used\n",
    "        memory_usage = (used_memory / total_memory) * 100\n",
    "        print(f\"Current GPU memory usage: {memory_usage:.2f}%\")\n",
    "\n",
    "        # Check if memory usage is too high\n",
    "        if memory_usage >= 95:\n",
    "            print(\"GPU memory usage is too high. Pausing execution...\")\n",
    "            while memory_usage > 30:\n",
    "                time.sleep(10)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                used_memory = mem_info.used\n",
    "                memory_usage = (used_memory / total_memory) * 100\n",
    "            print(\"GPU memory usage is low enough. Resuming execution...\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.489628Z",
     "iopub.status.busy": "2025-02-08T23:38:19.489259Z",
     "iopub.status.idle": "2025-02-08T23:38:19.573899Z",
     "shell.execute_reply": "2025-02-08T23:38:19.573117Z"
    }
   },
   "outputs": [],
   "source": [
    "def CatList(cat_, list_, type_ = \"and\"):\n",
    "        if type_ == \"and\":\n",
    "            return cat_ in list_\n",
    "        elif type_ == \"or\":\n",
    "            return cat_ not in list_\n",
    "        else:\n",
    "            return True\n",
    "def cats_used(max_cat_others, data, output_cat, min_cats = 2):\n",
    "    if max_cat_others > 0:\n",
    "        cats_values = (data[output_cat].value_counts()/data.shape[0])*100\n",
    "        sum_perc = 0\n",
    "        cats_ = []\n",
    "        for i in range(len(cats_values)):\n",
    "            if len(cats_) < min_cats or sum_perc < (100-max_cat_others):\n",
    "                cats_.append(str(cats_values.index[i]))\n",
    "                sum_perc = sum_perc + cats_values[i]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        cats_ = list(pd.unique(data[output_cat].tolist()))\n",
    "\n",
    "    return cats_\n",
    "\n",
    "def cats_levels(data, max_cat_others, output_cat, min_cats,prev_cats= {}):\n",
    "    cats_to_use = []\n",
    "    if len(list(prev_cats.keys())) != 0:\n",
    "        for cat_name in prev_cats.keys():\n",
    "            print(cat_name)\n",
    "            for case in prev_cats[cat_name]:\n",
    "                print(case)\n",
    "                data_cat = data[data[cat_name] == case]\n",
    "                cats_to_use_i = cats_used(max_cat_others, data_cat, output_cat, min_cats)\n",
    "                cats_to_use = cats_to_use + cats_to_use_i\n",
    "    else:\n",
    "        cats_to_use = cats_used(max_cat_others, data, output_cat, min_cats)\n",
    "\n",
    "    return cats_to_use\n",
    "\n",
    "\n",
    "def Cats_Filter(instance, list_categories, name_prev_cat = None):\n",
    "    if name_prev_cat != None:\n",
    "        if instance in list_categories:\n",
    "            return instance\n",
    "        else:\n",
    "            return \"Other_TEIS_\"+name_prev_cat\n",
    "    else:\n",
    "        if instance in list_categories:\n",
    "            return instance\n",
    "        else:\n",
    "            return \"Other_TEIS\"\n",
    "\n",
    "def ManageTextFeature(text_):\n",
    "    if type(text_) != str:\n",
    "        return \"No valid text\"\n",
    "    else:\n",
    "        return text_\n",
    "    \n",
    "\n",
    "def classification_report_to_df(report, y_true, y_pred):\n",
    "    global bch_class_df\n",
    "    global topic_dict\n",
    "    global iteration\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    order_labels = list(topic_dict.values())\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    labels = df.index[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Extracting TP, FP, TN, FN for each class\n",
    "    TP = cm.diagonal()\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    sens = sum(TP) / (sum(TP)+sum(FN))\n",
    "    spec = sum(TN) / (sum(TN)+sum(FP))\n",
    "    \n",
    "    # Calculate Sensitivity (same as recall)\n",
    "    df['Sensitivity'] = df['recall']\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Assign computed specificity to dataframe except for the last three rows\n",
    "    df.loc[df.index[:-3], 'Specificity'] = specificity\n",
    "    \n",
    "    # Handling special cases\n",
    "    # Set 'accuracy' row sensitivity and specificity to the accuracy value\n",
    "    accuracy = df.loc['accuracy', 'precision']  # assuming 'precision' contains the accuracy\n",
    "    df.loc['accuracy', ['Sensitivity', 'Specificity']] = sens, spec\n",
    "    \n",
    "    # Calculate 'macro avg' and 'weighted avg' for sensitivity and specificity\n",
    "    df.loc['macro avg', 'Sensitivity'] = df.iloc[:-3]['Sensitivity'].mean()\n",
    "    df.loc['weighted avg', 'Sensitivity'] = np.average(df.iloc[:-3]['Sensitivity'], weights=df.iloc[:-3]['support'])\n",
    "    \n",
    "    df.loc['macro avg', 'Specificity'] = df.iloc[:-3]['Specificity'].mean()\n",
    "    df.loc['weighted avg', 'Specificity'] = np.average(df.iloc[:-3]['Specificity'], weights=df.iloc[:-3]['support'])\n",
    "\n",
    "    # Calculate Balanced Accuracy for each row, including special averages\n",
    "    df['Balanced Accuracy'] = (df['Sensitivity'] + df['Specificity']) / 2\n",
    "\n",
    "    df.loc['accuracy', 'precision'] = sum(TP) / (sum(TP) + sum(FP))\n",
    "    df.loc['accuracy', 'recall'] = sum(TP) / (sum(TP) + sum(FN))\n",
    "    df.loc['accuracy', 'f1-score'] = 2* sum(TP) / (2 * sum(TP) + sum(FP) + sum(FN))\n",
    "\n",
    "    if iteration > 1:\n",
    "        bch_class_df_noFr = bch_class_df.drop(columns=['TP', 'FP', 'TN', 'FN'])\n",
    "    else: \n",
    "        bch_class_df_noFr = bch_class_df\n",
    "\n",
    "    diff_df = df - bch_class_df_noFr\n",
    "    # Renaming columns for clarity\n",
    "    diff_df.columns = ['Diff ' + col for col in diff_df.columns]\n",
    "\n",
    "    # Concatenating the original dataframe with the differences\n",
    "    combined_df = pd.concat([df, diff_df], axis=1)\n",
    "\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    combined_df.loc[labels, 'Accuracy'] = class_accuracy\n",
    "    # Copying f1-score to 'Accuracy' for the last three rows\n",
    "    combined_df.loc[['accuracy', 'macro avg', 'weighted avg'], 'Accuracy'] = combined_df.loc[['accuracy', 'macro avg', 'weighted avg'], 'f1-score']\n",
    "\n",
    "    # Calculate and append TP, FP, TN, FN metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"TN\": TN,\n",
    "        \"FN\": FN\n",
    "    }, index=labels)\n",
    "\n",
    "    # Merge the new metrics into the existing DataFrame\n",
    "    combined_df = combined_df.merge(metrics_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Reorder DataFrame based on specified order labels\n",
    "    combined_df = combined_df.reindex(order_labels + ['macro avg', 'weighted avg'])\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def fitness_evaluation(data_syn, chromosome, selected_indexes, indices_frozenset, generation, process):\n",
    "    global history_IndexesList_dict\n",
    "    global topic_name\n",
    "    global X_train_r\n",
    "    global Y_train_r\n",
    "    global X_test_re\n",
    "    global Y_test_re\n",
    "    global catboost_params\n",
    "    global nsgaii_results_path\n",
    "    global history_dict_name\n",
    "    global gen_eval_df\n",
    "    global topic_name\n",
    "    global topic_number\n",
    "    global sum_GPU_seconds\n",
    "    global total_gpu_seconds\n",
    "    global GPU_limit\n",
    "    global metric_name\n",
    "    global bch_m0\n",
    "    global X_test_re_Test\n",
    "    global Y_test_re_Test\n",
    "    global test_gen_eval_df\n",
    "\n",
    "    CPU_monitor_memory_usage()\n",
    "    monitor_gpu_memory()\n",
    "\n",
    "    if indices_frozenset in history_IndexesList_dict:\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "        classification_df = history_IndexesList_dict[indices_frozenset]['classification_df']\n",
    "        history_IndexesList_dict[indices_frozenset]['generation_process_chromosome'].append((generation, process, chromosome))\n",
    "        if metric_name == \"overall_balanced_accuracy\":\n",
    "            fitness_objective = (classification_df.loc['accuracy', 'Balanced Accuracy'], 0)\n",
    "        elif metric_name == \"overall_f1-score\":\n",
    "            fitness_objective = (classification_df.loc['accuracy', 'f1-score'], 0)\n",
    "        else:\n",
    "            fitness_objective = (classification_df.loc[topic_name, metric_name], 0)\n",
    "    else:\n",
    "        if GPU_limit == True:\n",
    "            return ()\n",
    "        individual_IndexesList_dict = {}\n",
    "        individual_IndexesList_dict['generation_process_chromosome'] = []\n",
    "        filtered_syn_df = data_syn[data_syn['index_meta'].isin(selected_indexes)]\n",
    "\n",
    "        X_train_re = pd.concat([X_train_r, filtered_syn_df.drop(columns=['topic_name'])])\n",
    "        Y_train_re = pd.concat([Y_train_r, filtered_syn_df['topic_name']])\n",
    "\n",
    "        train_pool_re = Pool(\n",
    "            X_train_re[[\"text\", \"area_TEIS\"]],\n",
    "            Y_train_re,\n",
    "            text_features=[\"text\"],\n",
    "            cat_features=[\"area_TEIS\"]\n",
    "        )\n",
    "        valid_pool_re = Pool(\n",
    "            X_test_re[[\"text\", \"area_TEIS\"]],\n",
    "            Y_test_re,\n",
    "            text_features=[\"text\"],\n",
    "            cat_features=[\"area_TEIS\"]\n",
    "        )\n",
    "\n",
    "        catboost_params = catboost_params\n",
    "            \n",
    "        # Model Training\n",
    "        model_re = CatBoostClassifier(**catboost_params)\n",
    "        start_time = time.time()  # Start timing\n",
    "        model_re.fit(train_pool_re, eval_set=valid_pool_re)\n",
    "        training_time = time.time() - start_time  # End timing\n",
    "\n",
    "        sum_GPU_seconds += training_time\n",
    "        if sum_GPU_seconds >= total_gpu_seconds:\n",
    "            GPU_limit = True\n",
    "\n",
    "        # Save the retrain performances\n",
    "        predictions = model_re.predict(X_test_re[[\"text\", \"area_TEIS\"]])\n",
    "        accuracy = accuracy_score(Y_test_re, predictions)\n",
    "        report = classification_report(Y_test_re, predictions, digits=3, output_dict=True)\n",
    "        classification_df = classification_report_to_df(report, Y_test_re, predictions)\n",
    "\n",
    "        fitness_score = (accuracy, classification_df.loc[topic_name, 'recall'])\n",
    "            \n",
    "        # Save the trained model, classification_df, and fitness_score\n",
    "        individual_IndexesList_dict['model'] = model_re\n",
    "        individual_IndexesList_dict['true_labels'] = []  # Convert to list if Y_test_re is a pandas Series or numpy array\n",
    "        individual_IndexesList_dict['predicted_labels'] = []  # Convert to list for consistency\n",
    "        individual_IndexesList_dict['classification_df'] = classification_df\n",
    "        individual_IndexesList_dict['fitness_score'] = fitness_score\n",
    "        individual_IndexesList_dict['number_of_syn_sample'] = len(filtered_syn_df)\n",
    "        individual_IndexesList_dict['retraining_time'] = training_time  # Save the training time\n",
    "        individual_IndexesList_dict['generation_process_chromosome'].append((generation, process, chromosome))\n",
    "\n",
    "        # Save the individual dictionary\n",
    "        history_IndexesList_dict[indices_frozenset] = individual_IndexesList_dict\n",
    "        # with open(f'{nsgaii_results_path}/{history_dict_name}.pkl', 'wb') as file:\n",
    "        #     pickle.dump(history_IndexesList_dict, file)\n",
    "        print(fitness_score)\n",
    "\n",
    "        new_row_index = len(gen_eval_df)\n",
    "        class_DF_path = f'{nsgaii_results_path}/Class_DF'\n",
    "        os.makedirs(class_DF_path, exist_ok=True)\n",
    "        classification_df.to_csv(f'{class_DF_path}/{topic_number}_NSGA-II_{new_row_index}_AllEval_ClassDF.csv', index=True)\n",
    "        classification_df.to_pickle(f'{class_DF_path}/{topic_number}_NSGA-II_{new_row_index}_AllEval_ClassDF.pkl')\n",
    "\n",
    "        # Collect all generation data into a new DataFrame row\n",
    "        gen_eval_row = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"topic_number\": topic_number,\n",
    "            \"generation\": generation,\n",
    "            'fitness_score': fitness_score,\n",
    "            \"accuracy\": fitness_score[0],\n",
    "            \"topic_recall\": fitness_score[1],\n",
    "            'balanced_fitness_score': (classification_df.loc['accuracy', 'Balanced Accuracy'], classification_df.loc[topic_name, 'Balanced Accuracy']),\n",
    "            'overall_balanced_accuracy': classification_df.loc['accuracy', 'Balanced Accuracy'],\n",
    "            'topic_balanced_accuracy': classification_df.loc[topic_name, 'Balanced Accuracy'],\n",
    "            'balanced_acc_rec_score': (classification_df.loc[topic_name, 'Balanced Accuracy'], classification_df.loc[topic_name, 'recall']),\n",
    "            'topic_F1': classification_df.loc[topic_name, 'f1-score'],\n",
    "            'overall_F1': classification_df.loc['accuracy', 'f1-score'],\n",
    "            'overall_recall': classification_df.loc['accuracy', 'recall'],\n",
    "            \"retraining_time\": training_time,\n",
    "            \"number_of_syn_sample\": len(filtered_syn_df),\n",
    "            \"retrained_dots_list\": filtered_syn_df['index_meta'].tolist(),\n",
    "            'true_labels': [],\n",
    "            'predicted_labels': [],\n",
    "            'chromosome': chromosome,\n",
    "            'classDF_path': f'{class_DF_path}/{topic_number}_NSGA-II_{new_row_index}_AllEval_ClassDF.csv',\n",
    "            'T12_CR_Imp': classification_df.loc['Email security and attachments.', 'recall'] - bch_m0.loc['Email security and attachments.', 'recall']\n",
    "            # 'T12_TBA_Imp': classification_df.loc['Email security and attachments.', 'Balanced Accuracy'] - bch_m0.loc['Email security and attachments.', 'Balanced Accuracy']\n",
    "        }\n",
    "        # # Transform DataFrame to dict format\n",
    "        # for idx in classification_df.index:\n",
    "        #     if idx != 'accuracy' and idx != 'macro avg' and idx != 'weighted avg':\n",
    "        #         gen_eval_row[idx] = classification_df.loc[idx].dropna().to_dict()\n",
    "        # Convert the dictionary to a DataFrame for a single row\n",
    "        gen_eval_row_df = pd.DataFrame([gen_eval_row])\n",
    "        # Concatenate this new row DataFrame to the existing DataFrame\n",
    "        gen_eval_df = pd.concat([gen_eval_df, gen_eval_row_df], ignore_index=True)\n",
    "        gen_eval_df.to_csv(f'{nsgaii_results_path}/GenAllEvals_{NSGA_II_results_name}.csv', index=True)\n",
    "        gen_eval_df.to_pickle(f'{nsgaii_results_path}/GenAllEvals_{NSGA_II_results_name}.pkl')\n",
    "\n",
    "        \"\"\"Testing results below\"\"\"\n",
    "\n",
    "        test_predictions = model_re.predict(X_test_re_Test[[\"text\", \"area_TEIS\"]])\n",
    "        test_accuracy = accuracy_score(Y_test_re_Test, test_predictions)\n",
    "        test_report = classification_report(Y_test_re_Test, test_predictions, digits=3, output_dict=True)\n",
    "        test_classification_df = classification_report_to_df(test_report, Y_test_re_Test, test_predictions)\n",
    "\n",
    "        test_new_row_index = len(test_gen_eval_df)\n",
    "        test_class_DF_path = f'{nsgaii_results_path}/test_Class_DF'\n",
    "        os.makedirs(test_class_DF_path, exist_ok=True)\n",
    "        test_classification_df.to_csv(f'{test_class_DF_path}/test_{topic_number}_NSGA-II_{test_new_row_index}_AllEval_ClassDF.csv', index=True)\n",
    "        test_classification_df.to_pickle(f'{test_class_DF_path}/test_{topic_number}_NSGA-II_{test_new_row_index}_AllEval_ClassDF.pkl')\n",
    "\n",
    "        # Collect all generation data into a new DataFrame row\n",
    "        test_gen_eval_row = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"topic_number\": topic_number,\n",
    "            \"generation\": generation,\n",
    "            'fitness_score': (test_classification_df.loc['accuracy', 'recall'], test_classification_df.loc[topic_name, 'recall']),\n",
    "            \"accuracy\": test_classification_df.loc['accuracy', 'recall'],\n",
    "            \"topic_recall\": test_classification_df.loc[topic_name, 'recall'],\n",
    "            'balanced_fitness_score': (test_classification_df.loc['accuracy', 'Balanced Accuracy'], test_classification_df.loc[topic_name, 'Balanced Accuracy']),\n",
    "            'overall_balanced_accuracy': test_classification_df.loc['accuracy', 'Balanced Accuracy'],\n",
    "            'topic_balanced_accuracy': test_classification_df.loc[topic_name, 'Balanced Accuracy'],\n",
    "            'balanced_acc_rec_score': (test_classification_df.loc[topic_name, 'Balanced Accuracy'], test_classification_df.loc[topic_name, 'recall']),\n",
    "            'topic_F1': test_classification_df.loc[topic_name, 'f1-score'],\n",
    "            'overall_F1': test_classification_df.loc['accuracy', 'f1-score'],\n",
    "            'overall_recall': test_classification_df.loc['accuracy', 'recall'],\n",
    "            \"retraining_time\": training_time,\n",
    "            \"number_of_syn_sample\": len(filtered_syn_df),\n",
    "            \"retrained_dots_list\": filtered_syn_df['index_meta'].tolist(),\n",
    "            'true_labels': [],\n",
    "            'predicted_labels': [],\n",
    "            'chromosome': chromosome,\n",
    "            'classDF_path': f'{test_class_DF_path}/test_{topic_number}_NSGA-II_{test_new_row_index}_AllEval_ClassDF.csv',\n",
    "            'T12_TBA_Imp': test_classification_df.loc['Email security and attachments.', 'Balanced Accuracy'] - bch_m0.loc['Email security and attachments.', 'Balanced Accuracy'],\n",
    "            'T7_TBA_Imp': test_classification_df.loc[\"\\\"Access and login issues\\\"\", 'Balanced Accuracy'] - bch_m0.loc[\"\\\"Access and login issues\\\"\", 'Balanced Accuracy'],\n",
    "            'T12_CR_Imp': test_classification_df.loc['Email security and attachments.', 'recall'] - bch_m0.loc['Email security and attachments.', 'recall'],\n",
    "            'T7_CR_Imp': test_classification_df.loc[\"\\\"Access and login issues\\\"\", 'recall'] - bch_m0.loc[\"\\\"Access and login issues\\\"\", 'recall'],\n",
    "            'OF1_Imp': test_classification_df.loc['accuracy', 'f1-score'] - bch_m0.loc['accuracy', 'f1-score'],\n",
    "            'Super_Obj_T12T7_CBA': 0.5*((test_classification_df.loc['Email security and attachments.', 'Balanced Accuracy'] - bch_m0.loc['Email security and attachments.', 'Balanced Accuracy']) + (test_classification_df.loc[\"\\\"Access and login issues\\\"\", 'Balanced Accuracy'] - bch_m0.loc[\"\\\"Access and login issues\\\"\", 'Balanced Accuracy'])),\n",
    "            'Super_Obj_T12T7_CR': 0.5*((test_classification_df.loc['Email security and attachments.', 'recall'] - bch_m0.loc['Email security and attachments.', 'recall']) + (test_classification_df.loc[\"\\\"Access and login issues\\\"\", 'recall'] - bch_m0.loc[\"\\\"Access and login issues\\\"\", 'recall']))\n",
    "    }\n",
    "        # # Transform DataFrame to dict format\n",
    "        # for idx in classification_df.index:\n",
    "        #     if idx != 'accuracy' and idx != 'macro avg' and idx != 'weighted avg':\n",
    "        #         gen_eval_row[idx] = classification_df.loc[idx].dropna().to_dict()\n",
    "        # Convert the dictionary to a DataFrame for a single row\n",
    "        test_gen_eval_row_df = pd.DataFrame([test_gen_eval_row])\n",
    "        # Concatenate this new row DataFrame to the existing DataFrame\n",
    "        test_gen_eval_df = pd.concat([test_gen_eval_df, test_gen_eval_row_df], ignore_index=True)\n",
    "        test_gen_eval_df.to_csv(f'{nsgaii_results_path}/test_GenAllEvals_{NSGA_II_results_name}.csv', index=True)\n",
    "        test_gen_eval_df.to_pickle(f'{nsgaii_results_path}/test_GenAllEvals_{NSGA_II_results_name}.pkl')\n",
    "\n",
    "        if metric_name == \"overall_balanced_accuracy\":\n",
    "            fitness_objective = (classification_df.loc['accuracy', 'Balanced Accuracy'], 0)\n",
    "        elif metric_name == \"overall_f1-score\":\n",
    "            fitness_objective = (classification_df.loc['accuracy', 'f1-score'], 0)\n",
    "        else:\n",
    "            fitness_objective = (classification_df.loc[topic_name, metric_name], 0)\n",
    "        \n",
    "    return fitness_objective\n",
    "\n",
    "def polynomial_mutation(individual, mutation_rate, eta=20):\n",
    "    \"\"\"\n",
    "    Perform polynomial mutation on an individual.\n",
    "    Args:\n",
    "    - individual: A list of tuples (index, priority)\n",
    "    - mutation_rate: Probability of mutation per gene.\n",
    "    - eta: Distribution index for mutation (controls the spread).\n",
    "    \"\"\"\n",
    "    mutated_individual = []\n",
    "    for gene in individual:\n",
    "        if random.random() < mutation_rate:\n",
    "            u = random.random()\n",
    "            delta = 0\n",
    "            if u < 0.5:\n",
    "                delta = (2*u)**(1/(eta+1)) - 1\n",
    "            else:\n",
    "                delta = 1 - (2*(1 - u))**(1/(eta+1))\n",
    "\n",
    "            # Mutate the priority value, ensuring it remains an integer within bounds\n",
    "            min_priority, max_priority = 1, len(individual)  # Assuming priority bounds\n",
    "            new_priority = int(min(max_priority, max(min_priority, gene[1] + delta * (max_priority - min_priority))))\n",
    "            mutated_individual.append((gene[0], new_priority))\n",
    "        else:\n",
    "            mutated_individual.append(gene)\n",
    "    return mutated_individual\n",
    "\n",
    "def adaptive_polynomial_mutation(individual, generation, max_generations, initial_rate=0.1):\n",
    "    \"\"\"\n",
    "    Adaptive mutation adjusts the mutation rate based on the generation number.\n",
    "    Args:\n",
    "    - individual: The individual to mutate.\n",
    "    - generation: Current generation number.\n",
    "    - max_generations: Total number of generations planned.\n",
    "    - initial_rate: Initial mutation rate.\n",
    "    \"\"\"\n",
    "    # Adjust mutation rate based on the progress\n",
    "    mutation_rate = initial_rate * (1 - generation / max_generations)\n",
    "    return polynomial_mutation(individual, mutation_rate)\n",
    "\n",
    "def dominates(score1, score2):\n",
    "    return (score1[0] > score2[0] and score1[1] >= score2[1]) or (score1[0] >= score2[0] and score1[1] > score2[1])\n",
    "\n",
    "def all_pareto_observer(history_pareto_selections_list):\n",
    "    global topic_name\n",
    "    global topic_number\n",
    "    global nsgaii_results_path\n",
    "    global gen_stats_df_name\n",
    "    global gen_stats_df\n",
    "    global NSGA_II_results_name\n",
    "    global fold_pfs_df\n",
    "    global history_IndexesList_dict\n",
    "\n",
    "    objective_1_values = [fitness[0] for _, _, fitness in history_pareto_selections_list]\n",
    "    objective_2_values = [fitness[1] for _, _, fitness in history_pareto_selections_list]\n",
    "\n",
    "    # Check if both lists are empty or filled with zeros\n",
    "    if not objective_1_values or not objective_2_values or all(value == 0 for value in objective_1_values + objective_2_values):\n",
    "        print(\"All objectives are None or 0, skipping further processes.\")\n",
    "        return  # Exit the function\n",
    "\n",
    "    non_dominated_segments = {}\n",
    "    for i1, tuple1 in enumerate(history_pareto_selections_list):\n",
    "        if tuple1[2] == (None, None):\n",
    "            continue  # Skip non-evaluable segments\n",
    "\n",
    "        dominated = False\n",
    "        for i2, tuple2 in enumerate(history_pareto_selections_list):\n",
    "            if i1 != i2 and dominates(tuple2[2], tuple1[2]):\n",
    "                dominated = True\n",
    "                break\n",
    "        if not dominated:\n",
    "            non_dominated_segments[i1] = tuple1\n",
    "    \n",
    "    pareto_fitness_tuples = [(ft[0], ft[1]) for ft in [sel[2] for sel in non_dominated_segments.values()] if ft[0] is not None and ft[1] is not None]\n",
    "    pareto_selections_tuples = [sel for sel in non_dominated_segments.values()]\n",
    "\n",
    "    # Calculate statistics\n",
    "    worst_fitness = (min(objective_1_values), min(objective_2_values))\n",
    "    best_fitness = (max(objective_1_values), max(objective_2_values))\n",
    "    mean_OverallAcc = np.mean(objective_1_values)\n",
    "    mean_ClassRecall = np.mean(objective_2_values)\n",
    "\n",
    "    print(f\"All Pareto Selections, Evaluations: {len(history_pareto_selections_list)}\")\n",
    "    print(f\"Best Fitness: {best_fitness}\")\n",
    "    print(f\"Worst Fitness: {worst_fitness}\")\n",
    "    print(f\"Mean Overall Accuracy: {mean_OverallAcc}\")\n",
    "    print(f\"Mean {topic_name} Recall: {mean_ClassRecall}\")\n",
    "\n",
    "    print(\"Pareto Front Selections:---------------------\")\n",
    "    for i, sel in enumerate(non_dominated_segments.values()):\n",
    "        print(f\"Selection {i+1}: {sel[0]} \\nFitness: {sel[2]}\")\n",
    "        indices_frozenset = frozenset(sel[0])\n",
    "        classification_df = history_IndexesList_dict[indices_frozenset]['classification_df']\n",
    "        # Collect all generation data into a new DataFrame row\n",
    "        new_PFs_row = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"topic_number\": topic_number,\n",
    "            \"generation\": \"Final_Evaluate\",\n",
    "            'fitness_score': sel[2],\n",
    "            \"accuracy\": sel[2][0],\n",
    "            \"topic_recall\": sel[2][1],\n",
    "            \"retraining_time\": history_IndexesList_dict[indices_frozenset]['retraining_time'],\n",
    "            \"number_of_syn_sample\": len(sel[0]),\n",
    "            \"retrained_dots_list\": sel[0],\n",
    "            'true_labels': history_IndexesList_dict[indices_frozenset]['true_labels'],\n",
    "            'predicted_labels': history_IndexesList_dict[indices_frozenset]['predicted_labels'],\n",
    "            'chromosome': sel[1]\n",
    "        }\n",
    "        # # Transform DataFrame to dict format\n",
    "        # for idx in classification_df.index:\n",
    "        #     if idx != 'accuracy' and idx != 'macro avg' and idx != 'weighted avg':\n",
    "        #         new_PFs_row[idx] = classification_df.loc[idx].dropna().to_dict()\n",
    "        # Convert the dictionary to a DataFrame for a single row\n",
    "        new_PFs_row_df = pd.DataFrame([new_PFs_row])\n",
    "        # Concatenate this new row DataFrame to the existing DataFrame\n",
    "        fold_pfs_df = pd.concat([fold_pfs_df, new_PFs_row_df], ignore_index=True)\n",
    "        fold_pfs_df.to_csv(f'{nsgaii_results_path}/TopicPFs_{NSGA_II_results_name}.csv', index=False)\n",
    "        fold_pfs_df.to_pickle(f'{nsgaii_results_path}/TopicPFs_{NSGA_II_results_name}.pkl')\n",
    "        print('---')\n",
    "    print('------------------------')\n",
    "\n",
    "    recall_key = f\"Mean {topic_name} Recall\"\n",
    "    # Collect all generation data into a new DataFrame row\n",
    "    new_row = {\n",
    "        \"Generation\": \"All Pareto Selections in History\",\n",
    "        \"Number of Evaluations\": len(history_pareto_selections_list),\n",
    "        \"Best Fitness\": best_fitness,\n",
    "        \"Worst Fitness\": worst_fitness,\n",
    "        \"Mean Overall Accuracy\": mean_OverallAcc,\n",
    "        recall_key: mean_ClassRecall,\n",
    "        \"Pareto Front Selections\": [pareto_fitness_tuples, pareto_selections_tuples]\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame for a single row\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    # Concatenate this new row DataFrame to the existing DataFrame\n",
    "    gen_stats_df = pd.concat([gen_stats_df, new_row_df], ignore_index=True)\n",
    "    gen_stats_df.to_csv(f'{nsgaii_results_path}/{gen_stats_df_name}.csv', index=False)\n",
    "    gen_stats_df.to_pickle(f'{nsgaii_results_path}/{gen_stats_df_name}.pkl')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    plt.scatter(objective_1_values, objective_2_values, c='blue', alpha=0.5, label='Population')\n",
    "    plt.scatter([ft[0] for ft in pareto_fitness_tuples], [ft[1] for ft in pareto_fitness_tuples], c='red', alpha=0.9, label='Pareto Front')\n",
    "    \n",
    "    plt.xlabel('Objective 1: Overall Accuracy')\n",
    "    plt.ylabel(f'Objective 2: {topic_name} Recall')\n",
    "    plt.title(f'{topic_number}, Population and Pareto Front for All Pareto Selections in History')\n",
    "    plt.legend()\n",
    "    pareto_plots_dir = f\"{nsgaii_results_path}/{gen_stats_df_name}\"\n",
    "    os.makedirs(pareto_plots_dir, exist_ok=True)\n",
    "    filename = f\"All Pareto Selections in History.png\"\n",
    "    plt.savefig(os.path.join(pareto_plots_dir, filename), dpi=200, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "    return gen_stats_df\n",
    "\n",
    "\n",
    "def select_string_based_on_probabilities_no_duplicates(nested_list, probabilities, rnd, selected_set):\n",
    "    \"\"\"\n",
    "    Select a string from a list of lists based on given probabilities for each inner list, ensuring no duplicates.\n",
    "\n",
    "    :param nested_list: List of lists containing strings\n",
    "    :param probabilities: List of probabilities for selecting from each inner list\n",
    "    :param rnd: Random instance to use for selection\n",
    "    :param selected_set: Set of already selected strings to avoid duplicates\n",
    "    :return: Selected string\n",
    "    \"\"\"\n",
    "    # Normalize probabilities to sum to 1\n",
    "    total = sum(probabilities)\n",
    "    normalized_probabilities = [p / total for p in probabilities]\n",
    "\n",
    "    while True:\n",
    "        # Select which inner list to choose from\n",
    "        chosen_list_index = rnd.choices(range(len(nested_list)), weights=normalized_probabilities, k=1)[0]\n",
    "        chosen_list = nested_list[chosen_list_index]\n",
    "\n",
    "        # Select a string from the chosen inner list uniformly, ensuring no duplicates\n",
    "        available_choices = [s for s in chosen_list if s not in selected_set]\n",
    "        if available_choices:\n",
    "            selected_string = rnd.choice(available_choices)\n",
    "            selected_set.add(selected_string)\n",
    "            return selected_string\n",
    "\n",
    "def move_to_first_list(nested_list, element):\n",
    "    # Remove the element from its current list\n",
    "    for inner_list in nested_list:\n",
    "        if element in inner_list:\n",
    "            inner_list.remove(element)\n",
    "            break\n",
    "    # Add the element to the first inner list\n",
    "    nested_list[0].append(element)\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.579365Z",
     "iopub.status.busy": "2025-02-08T23:38:19.579008Z",
     "iopub.status.idle": "2025-02-08T23:38:19.611151Z",
     "shell.execute_reply": "2025-02-08T23:38:19.610387Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_dominated(score1, score2):\n",
    "    return (score1[0] > score2[0] and score1[1] >= score2[1]) or (score1[0] >= score2[0] and score1[1] > score2[1])\n",
    "\n",
    "def identify_pareto_fronts(population):\n",
    "    population = np.array(population)\n",
    "    fronts = []\n",
    "    remaining_population = population.copy()\n",
    "    \n",
    "    while len(remaining_population) > 0:\n",
    "        front = []\n",
    "        non_dominated_indices = []\n",
    "        \n",
    "        for i, p1 in enumerate(remaining_population):\n",
    "            dominated = False\n",
    "            for j, p2 in enumerate(remaining_population):\n",
    "                if is_dominated(p2, p1):\n",
    "                    dominated = True\n",
    "                    break\n",
    "            if not dominated:\n",
    "                non_dominated_indices.append(i)\n",
    "                front.append(p1)\n",
    "        \n",
    "        fronts.append(front)\n",
    "        remaining_population = np.delete(remaining_population, non_dominated_indices, axis=0)\n",
    "    \n",
    "    return fronts\n",
    "\n",
    "def plot_pareto_fronts(fronts):\n",
    "    colors = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "    \n",
    "    for i, front in enumerate(fronts):\n",
    "        front = np.array(front)\n",
    "        plt.scatter(front[:, 0], front[:, 1], color=colors[i % len(colors)], alpha=0.5, label=f'Front {i+1}')\n",
    "    \n",
    "    plt.xlabel('Objective 1')\n",
    "    plt.ylabel('Objective 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def relaxed_chi_square_test(data, bins=10, tolerance=0):\n",
    "    observed_freq, _ = np.histogram(data, bins=bins)\n",
    "    expected_freq = [len(data) / bins] * bins\n",
    "    \n",
    "    chi_square_stat, p_value = chisquare(observed_freq, expected_freq)\n",
    "    \n",
    "    relaxed_threshold = 0.025 + tolerance\n",
    "    \n",
    "    return chi_square_stat, p_value, p_value > relaxed_threshold\n",
    "\n",
    "def relaxed_ks_test(data, tolerance=0):\n",
    "    d_stat, p_value = kstest(data, 'uniform', args=(np.min(data), np.ptp(data)))\n",
    "    \n",
    "    relaxed_threshold = 0.025 + tolerance\n",
    "    \n",
    "    return d_stat, p_value, p_value > relaxed_threshold\n",
    "\n",
    "def check_uniform_distribution_with_tolerance(data, chi_tolerance=0, ks_tolerance=0):\n",
    "    chi_square_stat, chi_p_value, chi_uniform = relaxed_chi_square_test(data, tolerance=chi_tolerance)\n",
    "    d_stat, ks_p_value, ks_uniform = relaxed_ks_test(data, tolerance=ks_tolerance)\n",
    "    \n",
    "    is_uniform = chi_uniform and ks_uniform\n",
    "    \n",
    "    return {\n",
    "        'chi_square_stat': chi_square_stat,\n",
    "        'chi_p_value': chi_p_value,\n",
    "        'ks_stat': d_stat,\n",
    "        'ks_p_value': ks_p_value,\n",
    "        'is_uniform': is_uniform\n",
    "    }\n",
    "\n",
    "def determine_distribution_type(front_sizes):\n",
    "    flat_front_sizes = np.array(front_sizes).flatten()\n",
    "    uniformity_check = check_uniform_distribution_with_tolerance(flat_front_sizes)\n",
    "    \n",
    "    if uniformity_check['is_uniform']:\n",
    "        return 'Uniform'\n",
    "    elif (max(front_sizes) == front_sizes[-1] or max(front_sizes) == front_sizes[-2] or min(front_sizes) == front_sizes[0]) and front_sizes[-1] != front_sizes[0]:\n",
    "        return 'Reversed Quadratic'\n",
    "    elif max(front_sizes) == front_sizes[0] or (len(front_sizes) <= 3 and all(size > len(front_sizes) for size in front_sizes)):\n",
    "        return 'Quadratic'\n",
    "    else:\n",
    "        return 'Random'\n",
    "\n",
    "def generate_tournament_size(S, distribution_type, rng):\n",
    "    if distribution_type == 'Uniform' or distribution_type == 'Quadratic':\n",
    "        probabilities = [0.6 / min(4, (S-1)) if i < 4 else 0.4 / max(1, (S-5)) for i in range(2, S+1)]\n",
    "    elif distribution_type == 'Reversed Quadratic':\n",
    "        probabilities = [0.4 / max(1, (max(S-1, 7)//2-1)) if i >= max(S-1, 7)//2 else 0.6 / max(1, (max(S-1, 7)//2-1)) for i in range(2, max(S, 7))]\n",
    "    elif distribution_type == 'Random':\n",
    "        probabilities = [0.6 / max(1, (max(S//2, 4)-1)) if i < max(S//2, 4) else 0.4 / max(1, (max(S//2, 4)-1)) for i in range(2, max(S, 7))]\n",
    "    \n",
    "    # Normalize probabilities to sum to 1\n",
    "    total = sum(probabilities)\n",
    "    probabilities = [p / total for p in probabilities]\n",
    "    \n",
    "    return rng.choice(range(2, S+1 if distribution_type in ['Uniform', 'Quadratic'] else max(S, 7)), p=probabilities)\n",
    "\n",
    "def cluster_population(population, fronts, distribution_type, S):\n",
    "    clusters = {}\n",
    "    \n",
    "    if distribution_type in ['Uniform', 'Quadratic']:\n",
    "        for i, ind in enumerate(population):\n",
    "            fitness_value = ind.fitness\n",
    "            if fitness_value not in clusters:\n",
    "                clusters[fitness_value] = []\n",
    "            clusters[fitness_value].append(i)\n",
    "    \n",
    "    elif distribution_type == 'Reversed Quadratic':\n",
    "        for i in range(S-1):\n",
    "            clusters[i+1] = [idx for idx, _ in enumerate(fronts[i])]\n",
    "        clusters[S] = [idx for i in range(S-1, len(fronts)) for idx in range(len(fronts[i]))]\n",
    "    \n",
    "    elif distribution_type == 'Random':\n",
    "        for i in range(len(fronts)):\n",
    "            clusters[i+1] = [idx for idx, _ in enumerate(fronts[i])]\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def population_observer(population, prng):\n",
    "    # Convert population to appropriate structure\n",
    "    fitness_values = [ind.fitness for ind in population]\n",
    "    \n",
    "    fronts = identify_pareto_fronts(fitness_values)\n",
    "    front_sizes = [len(front) for front in fronts]\n",
    "    \n",
    "    distribution_type = determine_distribution_type(front_sizes)\n",
    "    \n",
    "    if distribution_type == 'Uniform':\n",
    "        S = len(set(individual.fitness for individual in population))\n",
    "    elif distribution_type == 'Reversed Quadratic':\n",
    "        S = math.ceil(len(fronts) / 2)\n",
    "    elif distribution_type == 'Quadratic':\n",
    "        S = len(set(individual.fitness for individual in population))\n",
    "    elif distribution_type == 'Random':\n",
    "        S = len(fronts)\n",
    "    \n",
    "    rng = np.random.default_rng(seed=42)  # Use the same seed for reproducibility\n",
    "    tournament_size = generate_tournament_size(S, distribution_type, rng)\n",
    "\n",
    "    clusters = cluster_population(population, fronts, distribution_type, S)\n",
    "    \n",
    "    plot_pareto_fronts(fronts)\n",
    "    print(distribution_type)\n",
    "    \n",
    "    return {\n",
    "        'fronts': fronts,\n",
    "        'front_sizes': front_sizes,\n",
    "        'distribution_type': distribution_type,\n",
    "        'S': S,\n",
    "        'tournament_size': tournament_size,\n",
    "        'clusters': clusters\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.619149Z",
     "iopub.status.busy": "2025-02-08T23:38:19.618736Z",
     "iopub.status.idle": "2025-02-08T23:38:19.746408Z",
     "shell.execute_reply": "2025-02-08T23:38:19.745275Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_index_meta_pool(indices, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)  # Set the random seed if provided\n",
    "    # Encoding: Assign random priority\n",
    "    chromosome = [(index, random.randint(1, len(indices))) for index in indices]\n",
    "    return chromosome\n",
    "\n",
    "def decode_chromosome(chromosome):\n",
    "    # Decoding: Calculate mean priority using round() for 四舍五入\n",
    "    mean_priority = round(sum(x[1] for x in chromosome) / len(chromosome))\n",
    "\n",
    "    # Sort by priority and select indices\n",
    "    sorted_chromosome = sorted(chromosome, key=lambda x: x[1])\n",
    "    selected_indices = [x[0] for x in sorted_chromosome[:mean_priority]]\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def custom_generator(random, args):\n",
    "    global init_counter\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    class_index_meta_pool = inner_args.get(\"class_index_meta_pool\")\n",
    "\n",
    "    individual_chromosome = encode_index_meta_pool(class_index_meta_pool, seed=init_counter)\n",
    "    individual = inspyred.ec.Individual(individual_chromosome)\n",
    "    init_counter += 1\n",
    "\n",
    "    return individual  # Return the initial individual of chromosome\n",
    "\n",
    "\n",
    "def evaluate(candidates, args):\n",
    "    global generation_counter\n",
    "    generation_counter += 1\n",
    "    global history_IndexesList_dict\n",
    "    global topic_name\n",
    "    global X_train_r\n",
    "    global Y_train_r\n",
    "    global X_test_re\n",
    "    global Y_test_re\n",
    "    global catboost_params\n",
    "    global GPU_limit\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "\n",
    "    results = []\n",
    "    for candidate in candidates:\n",
    "        if GPU_limit == True:\n",
    "            return results\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        \n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "\n",
    "        fitness_score = fitness_evaluation(data_syn, candidate, selected_indexes, indices_frozenset, generation_counter, 'Evaluation')\n",
    "\n",
    "        results.append(fitness_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def lexicase_elitist_with_cluster_tournament_selection(random, population, args):\n",
    "    global history_IndexesList_dict\n",
    "    global generation_counter\n",
    "    global Smode\n",
    "    global topic_number\n",
    "    global topic_name\n",
    "    global topic_name_cases_order\n",
    "    global topic_group_probabilities\n",
    "    global topic_name_sizes_order\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "\n",
    "    topic_name_cases_order = move_to_first_list(topic_name_cases_order, topic_name)\n",
    "    topic_name_sizes_order = move_to_first_list(topic_name_sizes_order, topic_name)\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    num_selected = inner_args.get(\"num_selected\")\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "    flattened_length = sum(len(sublist) for sublist in topic_name_sizes_order)\n",
    "\n",
    "    # Call population_observer to analyze the population\n",
    "    result = population_observer(population, random)\n",
    "    \n",
    "    # Retrieve the clusters from the result\n",
    "    clusters = result['clusters']\n",
    "\n",
    "    fitness_scores = []\n",
    "    chromosomes_indexes_pair = {}\n",
    "    for pop_i, individual in enumerate(population):\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(individual, inspyred.ec.Individual) and depth < max_depth:\n",
    "            individual = individual.candidate\n",
    "            depth += 1\n",
    "\n",
    "        selected_indexes = decode_chromosome(individual)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "        fitness_scores.append(fitness_score)\n",
    "        chromosomes_indexes_pair[pop_i] = (individual, indices_frozenset)\n",
    "    \n",
    "    # Reserve the best individuals\n",
    "    best_OverallAcc_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][0])\n",
    "    best_ClassRecall_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][1])\n",
    "    elitism_indices = {best_OverallAcc_index, best_ClassRecall_index}\n",
    "    mating_pool = [population[i] for i in elitism_indices]\n",
    "    already_selected = set(elitism_indices)  # Keep track of already selected candidates\n",
    "\n",
    "    while len(mating_pool) < num_selected: # (num_selected + len(elitism_indices))/2: # (num_selected + len(elitism_indices))/2:\n",
    "        selected_set = set()\n",
    "        lexicase = [select_string_based_on_probabilities_no_duplicates(topic_name_sizes_order, topic_group_probabilities, random, selected_set) for _ in range(flattened_length)]\n",
    "\n",
    "        # Initialize candidates as the entire population\n",
    "        candidates = list(range(len(population)))\n",
    "        cases = lexicase[:]\n",
    "\n",
    "        while cases and len(candidates) > 1:\n",
    "            case = cases.pop(0)\n",
    "            # Get fitness for each candidate\n",
    "            case_fitness = {i: history_IndexesList_dict[chromosomes_indexes_pair[i][1]]['classification_df'].loc[case, 'recall'] for i in candidates}\n",
    "            # Find the maximum fitness for the current case\n",
    "            max_fitness = max(case_fitness.values())\n",
    "            # Filter candidates with the maximum fitness\n",
    "            candidates = [i for i in candidates if case_fitness[i] == max_fitness]\n",
    "\n",
    "            # Ensure uniqueness based on indices_frozenset\n",
    "            unique_candidates = {}\n",
    "            for i in candidates:\n",
    "                indices_frozenset = chromosomes_indexes_pair[i][1]\n",
    "                if indices_frozenset not in unique_candidates:\n",
    "                    unique_candidates[indices_frozenset] = i\n",
    "\n",
    "            # Update candidates to unique ones\n",
    "            candidates = list(unique_candidates.values())\n",
    "\n",
    "        # If we have exactly one candidate, select it\n",
    "        if len(candidates) == 1:\n",
    "            if Smode != \"Sovl\":\n",
    "                if population[candidates[0]] not in already_selected:\n",
    "                    mating_pool.append(population[candidates[0]])\n",
    "                    already_selected.add(candidates[0])\n",
    "                else:\n",
    "                    # Find the second best from the previous case fitness\n",
    "                    if case_fitness:\n",
    "                        sorted_fitness = sorted(case_fitness.items(), key=lambda x: x[1], reverse=True)\n",
    "                        for idx, fitness in sorted_fitness:\n",
    "                            if population[idx] not in already_selected:\n",
    "                                mating_pool.append(population[idx])\n",
    "                                already_selected.add(idx)\n",
    "                                break\n",
    "            else:\n",
    "                mating_pool.append(population[candidates[0]])\n",
    "        elif len(candidates) > 1:\n",
    "            for index in candidates:\n",
    "                mating_pool.append(population[index])\n",
    "                already_selected.add(index)\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "    \n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    while len(mating_pool) < num_selected:\n",
    "        if Smode != \"Sovl\":\n",
    "            # Create a set of indexes whose corresponding elements in `population` are in `mating_pool`\n",
    "            excluded_indexes = {i for i, individual in enumerate(population) if individual in mating_pool}\n",
    "\n",
    "            # Update dictionary by removing excluded indexes\n",
    "            for coords in list(clusters.keys()):\n",
    "                clusters[coords] = [index for index in clusters[coords] if index not in excluded_indexes]\n",
    "                # Remove the key if the list becomes empty\n",
    "                if not clusters[coords]:\n",
    "                    del clusters[coords]\n",
    "\n",
    "        current_tournament_size = generate_tournament_size(min(result['S'], len(clusters)), result['distribution_type'], rng)\n",
    "        if current_tournament_size < 2:  # Ensuring there's at least a minimal pool for a tournament\n",
    "            print(\"Warning: Not enough individuals to continue meaningful selection.\")\n",
    "            break\n",
    "        selected_clusters = random.sample(list(clusters.keys()), min(len(list(clusters.keys())), current_tournament_size))\n",
    "        if result['distribution_type'] in ['Uniform', 'Quadratic']:\n",
    "            non_dominated_indices = []\n",
    "            for i, f1 in enumerate(selected_clusters):\n",
    "                dominated = False\n",
    "                for j, f2 in enumerate(selected_clusters):\n",
    "                    if is_dominated(f2, f1):\n",
    "                        dominated = True\n",
    "                        break\n",
    "                if not dominated:\n",
    "                    non_dominated_indices.append(f1)\n",
    "            for cluster in non_dominated_indices:\n",
    "                # Select a random individual from the best cluster\n",
    "                selected_individual = random.choice(clusters[cluster])\n",
    "                mating_pool.append(population[selected_individual])\n",
    "        elif result['distribution_type'] in ['Reversed Quadratic', 'Random']:\n",
    "            # Select the best cluster based on some criteria (e.g., smallest index for simplicity)\n",
    "            best_cluster = min(selected_clusters)\n",
    "            # Select a random individual from the best cluster\n",
    "            selected_individual = random.choice(clusters[best_cluster])\n",
    "            # Append the selected individual to the selected_parents list\n",
    "            mating_pool.append(population[selected_individual])\n",
    "\n",
    "        if len(clusters) < current_tournament_size:\n",
    "            break  # If we can't fill the tournament, break the loop\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "        \n",
    "    for candidate in mating_pool:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = fitness_evaluation(data_syn, candidate, selected_indexes, indices_frozenset, generation_counter, 'Selected')\n",
    "        \n",
    "    return mating_pool\n",
    "\n",
    "\n",
    "def lexicase_elitist_with_stochastic_tournament_selection(random, population, args):\n",
    "    global history_IndexesList_dict\n",
    "    global generation_counter\n",
    "    global Smode\n",
    "    global topic_number\n",
    "    global topic_name\n",
    "    global topic_name_cases_order\n",
    "    global topic_group_probabilities\n",
    "    global topic_name_sizes_order\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "\n",
    "    topic_name_cases_order = move_to_first_list(topic_name_cases_order, topic_name)\n",
    "    topic_name_sizes_order = move_to_first_list(topic_name_sizes_order, topic_name)\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    num_selected = inner_args.get(\"num_selected\")\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "    flattened_length = sum(len(sublist) for sublist in topic_name_sizes_order)\n",
    "\n",
    "    fitness_scores = []\n",
    "    chromosomes_indexes_pair = {}\n",
    "    for pop_i, individual in enumerate(population):\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(individual, inspyred.ec.Individual) and depth < max_depth:\n",
    "            individual = individual.candidate\n",
    "            depth += 1\n",
    "\n",
    "        selected_indexes = decode_chromosome(individual)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "        fitness_scores.append(fitness_score)\n",
    "        chromosomes_indexes_pair[pop_i] = (individual, indices_frozenset)\n",
    "    \n",
    "    # Reserve the best individuals\n",
    "    best_OverallAcc_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][0])\n",
    "    best_ClassRecall_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][1])\n",
    "    elitism_indices = {best_OverallAcc_index, best_ClassRecall_index}\n",
    "    mating_pool = [population[i] for i in elitism_indices]\n",
    "    already_selected = set(elitism_indices)  # Keep track of already selected candidates\n",
    "\n",
    "    while len(mating_pool) < (num_selected + len(elitism_indices))/2: # (num_selected + len(elitism_indices))/2:\n",
    "        selected_set = set()\n",
    "        lexicase = [select_string_based_on_probabilities_no_duplicates(topic_name_sizes_order, topic_group_probabilities, random, selected_set) for _ in range(flattened_length)]\n",
    "\n",
    "        # Initialize candidates as the entire population\n",
    "        candidates = list(range(len(population)))\n",
    "        cases = lexicase[:]\n",
    "\n",
    "        while cases and len(candidates) > 1:\n",
    "            case = cases.pop(0)\n",
    "            # Get fitness for each candidate\n",
    "            case_fitness = {i: history_IndexesList_dict[chromosomes_indexes_pair[i][1]]['classification_df'].loc[case, 'recall'] for i in candidates}\n",
    "            # Find the maximum fitness for the current case\n",
    "            max_fitness = max(case_fitness.values())\n",
    "            # Filter candidates with the maximum fitness\n",
    "            candidates = [i for i in candidates if case_fitness[i] == max_fitness]\n",
    "\n",
    "            # Ensure uniqueness based on indices_frozenset\n",
    "            unique_candidates = {}\n",
    "            for i in candidates:\n",
    "                indices_frozenset = chromosomes_indexes_pair[i][1]\n",
    "                if indices_frozenset not in unique_candidates:\n",
    "                    unique_candidates[indices_frozenset] = i\n",
    "\n",
    "            # Update candidates to unique ones\n",
    "            candidates = list(unique_candidates.values())\n",
    "\n",
    "        # If we have exactly one candidate, select it\n",
    "        if len(candidates) == 1:\n",
    "            if Smode != \"Sovl\":\n",
    "                if population[candidates[0]] not in already_selected:\n",
    "                    mating_pool.append(population[candidates[0]])\n",
    "                    already_selected.add(candidates[0])\n",
    "                else:\n",
    "                    # Find the second best from the previous case fitness\n",
    "                    if case_fitness:\n",
    "                        sorted_fitness = sorted(case_fitness.items(), key=lambda x: x[1], reverse=True)\n",
    "                        for idx, fitness in sorted_fitness:\n",
    "                            if population[idx] not in already_selected:\n",
    "                                mating_pool.append(population[idx])\n",
    "                                already_selected.add(idx)\n",
    "                                break\n",
    "            else:\n",
    "                mating_pool.append(population[candidates[0]])\n",
    "        elif len(candidates) > 1:\n",
    "            for index in candidates:\n",
    "                mating_pool.append(population[index])\n",
    "                already_selected.add(index)\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "    \n",
    "    while len(mating_pool) < num_selected:\n",
    "        current_tournament_size = random.randrange(2, len(population)+1)\n",
    "        if current_tournament_size < 2:  # Ensuring there's at least a minimal pool for a tournament\n",
    "            print(\"Warning: Not enough individuals to continue meaningful selection.\")\n",
    "            break\n",
    "        tournament_indices = random.sample(range(len(population)), current_tournament_size)\n",
    "        if Smode != \"Sovl\":\n",
    "            # Filter out indices already selected for the mating pool to avoid duplicates\n",
    "            tournament_indices = [i for i in tournament_indices if population[i] not in mating_pool]\n",
    "\n",
    "        if len(tournament_indices) < current_tournament_size:\n",
    "            break  # If we can't fill the tournament, break the loop\n",
    "\n",
    "        # Create the tournament from the selected indices\n",
    "        tournament = [population[i] for i in tournament_indices]\n",
    "        tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n",
    "\n",
    "        # Find the best individuals based on different criteria\n",
    "        best_OverallAcc_tournament_index = tournament_indices[max(range(len(tournament)), key=lambda i: tournament_fitness[i][0])]\n",
    "        best_ClassRecall_tournament_index = tournament_indices[max(range(len(tournament)), key=lambda i: tournament_fitness[i][1])]\n",
    "\n",
    "        # Append the best individual by time to the mating pool\n",
    "        mating_pool.append(population[best_OverallAcc_tournament_index])\n",
    "\n",
    "        # # Append the best individual by crowding if it's different from the best by time\n",
    "        # if best_OverallAcc_tournament_index != best_ClassRecall_tournament_index:\n",
    "        # if len(mating_pool) < num_selected:  # Check capacity before adding\n",
    "        mating_pool.append(population[best_ClassRecall_tournament_index])\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "        \n",
    "    for candidate in mating_pool:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = fitness_evaluation(data_syn, candidate, selected_indexes, indices_frozenset, generation_counter, 'Selected')\n",
    "        \n",
    "    return mating_pool\n",
    "\n",
    "\n",
    "\n",
    "def cluster_tournament_selection_with_elitism(random, population, args):\n",
    "    global history_IndexesList_dict\n",
    "    global tournament_size\n",
    "    global generation_counter\n",
    "    global Smode\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    num_selected = inner_args.get(\"num_selected\")\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "\n",
    "    # Call population_observer to analyze the population\n",
    "    result = population_observer(population, random)\n",
    "    \n",
    "    # Retrieve the clusters from the result\n",
    "    clusters = result['clusters']\n",
    "\n",
    "    fitness_scores = []\n",
    "    for individual in population:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(individual, inspyred.ec.Individual) and depth < max_depth:\n",
    "            individual = individual.candidate\n",
    "            depth += 1\n",
    "\n",
    "        selected_indexes = decode_chromosome(individual)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "        fitness_scores.append(fitness_score)\n",
    "    \n",
    "    # Reserve the best individuals\n",
    "    best_OverallAcc_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][0])\n",
    "    best_ClassRecall_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][1])\n",
    "    elitism_indices = {best_OverallAcc_index, best_ClassRecall_index}\n",
    "    mating_pool = [population[i] for i in elitism_indices]\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    while len(mating_pool) < num_selected:\n",
    "        if Smode != \"Sovl\":\n",
    "            # Create a set of indexes whose corresponding elements in `population` are in `mating_pool`\n",
    "            excluded_indexes = {i for i, individual in enumerate(population) if individual in mating_pool}\n",
    "\n",
    "            # Update dictionary by removing excluded indexes\n",
    "            for coords in list(clusters.keys()):\n",
    "                clusters[coords] = [index for index in clusters[coords] if index not in excluded_indexes]\n",
    "                # Remove the key if the list becomes empty\n",
    "                if not clusters[coords]:\n",
    "                    del clusters[coords]\n",
    "\n",
    "        current_tournament_size = generate_tournament_size(min(result['S'], len(clusters)), result['distribution_type'], rng)\n",
    "        if current_tournament_size < 2:  # Ensuring there's at least a minimal pool for a tournament\n",
    "            print(\"Warning: Not enough individuals to continue meaningful selection.\")\n",
    "            break\n",
    "        selected_clusters = random.sample(list(clusters.keys()), min(len(list(clusters.keys())), current_tournament_size))\n",
    "        if result['distribution_type'] in ['Uniform', 'Quadratic']:\n",
    "            non_dominated_indices = []\n",
    "            for i, f1 in enumerate(selected_clusters):\n",
    "                dominated = False\n",
    "                for j, f2 in enumerate(selected_clusters):\n",
    "                    if is_dominated(f2, f1):\n",
    "                        dominated = True\n",
    "                        break\n",
    "                if not dominated:\n",
    "                    non_dominated_indices.append(f1)\n",
    "            for cluster in non_dominated_indices:\n",
    "                # Select a random individual from the best cluster\n",
    "                selected_individual = random.choice(clusters[cluster])\n",
    "                mating_pool.append(population[selected_individual])\n",
    "        elif result['distribution_type'] in ['Reversed Quadratic', 'Random']:\n",
    "            # Select the best cluster based on some criteria (e.g., smallest index for simplicity)\n",
    "            best_cluster = min(selected_clusters)\n",
    "            # Select a random individual from the best cluster\n",
    "            selected_individual = random.choice(clusters[best_cluster])\n",
    "            # Append the selected individual to the selected_parents list\n",
    "            mating_pool.append(population[selected_individual])\n",
    "\n",
    "        if len(clusters) < current_tournament_size:\n",
    "            break  # If we can't fill the tournament, break the loop\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "        \n",
    "    for candidate in mating_pool:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = fitness_evaluation(data_syn, candidate, selected_indexes, indices_frozenset, generation_counter, 'Selected')\n",
    "        \n",
    "    return mating_pool\n",
    "\n",
    "\n",
    "\n",
    "def nsgaii_tournament_selection_with_priority_and_elitism(random, population, args):\n",
    "    global history_IndexesList_dict\n",
    "    global tournament_size\n",
    "    global generation_counter\n",
    "    global Smode\n",
    "    global GPU_limit\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    num_selected = inner_args.get(\"num_selected\")\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "\n",
    "    fitness_scores = []\n",
    "    for individual in population:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(individual, inspyred.ec.Individual) and depth < max_depth:\n",
    "            individual = individual.candidate\n",
    "            depth += 1\n",
    "\n",
    "        selected_indexes = decode_chromosome(individual)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "        fitness_scores.append(fitness_score)\n",
    "    \n",
    "    # Reserve the best individuals\n",
    "    best_OverallAcc_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][0])\n",
    "    best_ClassRecall_index = max(range(len(fitness_scores)), key=lambda i: fitness_scores[i][1])\n",
    "    elitism_indices = {best_OverallAcc_index, best_ClassRecall_index}\n",
    "    mating_pool = [population[i] for i in elitism_indices]\n",
    "\n",
    "    while len(mating_pool) < num_selected:\n",
    "        current_tournament_size = random.randrange(2, len(population)+1) # random.randrange(2, len(population)+1) ; min(tournament_size, len(population))\n",
    "        if current_tournament_size < 2:  # Ensuring there's at least a minimal pool for a tournament\n",
    "            print(\"Warning: Not enough individuals to continue meaningful selection.\")\n",
    "            break\n",
    "        tournament_indices = random.sample(range(len(population)), current_tournament_size)\n",
    "        if Smode != \"Sovl\":\n",
    "            # Filter out indices already selected for the mating pool to avoid duplicates\n",
    "            tournament_indices = [i for i in tournament_indices if population[i] not in mating_pool]\n",
    "\n",
    "        if len(tournament_indices) < current_tournament_size:\n",
    "            break  # If we can't fill the tournament, break the loop\n",
    "\n",
    "        # Create the tournament from the selected indices\n",
    "        tournament = [population[i] for i in tournament_indices]\n",
    "        tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n",
    "\n",
    "        # Find the best individuals based on different criteria\n",
    "        best_OverallAcc_tournament_index = tournament_indices[max(range(len(tournament)), key=lambda i: tournament_fitness[i][0])]\n",
    "        best_ClassRecall_tournament_index = tournament_indices[max(range(len(tournament)), key=lambda i: tournament_fitness[i][1])]\n",
    "\n",
    "        # Append the best individual by time to the mating pool\n",
    "        mating_pool.append(population[best_OverallAcc_tournament_index])\n",
    "\n",
    "        # # Append the best individual by crowding if it's different from the best by time\n",
    "        # if best_OverallAcc_tournament_index != best_ClassRecall_tournament_index:\n",
    "        # if len(mating_pool) < num_selected:  # Check capacity before adding\n",
    "        mating_pool.append(population[best_ClassRecall_tournament_index])\n",
    "\n",
    "        # If the mating pool is already filled, break the loop\n",
    "        if len(mating_pool) >= num_selected:\n",
    "            break\n",
    "        \n",
    "    for candidate in mating_pool:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = fitness_evaluation(data_syn, candidate, selected_indexes, indices_frozenset, generation_counter, 'Selected')\n",
    "        \n",
    "    return mating_pool\n",
    "\n",
    "\n",
    "def nsgaii_weight_mapping_crossover(random, candidates, args):\n",
    "    global generation_counter\n",
    "    global gen_stats_df\n",
    "    global Xmode\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "    \n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "    crossover_rate = inner_args.get(\"crossover_rate\")\n",
    "\n",
    "    if Xmode == \"Xnp\":\n",
    "        offsprings = []\n",
    "    else:\n",
    "        offsprings = candidates\n",
    "    \n",
    "    candidate_pairs = list(itertools.combinations(candidates, 2))\n",
    "    \n",
    "    for parent1, parent2 in candidate_pairs:\n",
    "        # Check if crossover should occur\n",
    "        if random.random() > crossover_rate:\n",
    "            # If not, append the original parents to the offspring list\n",
    "            if Xmode == \"Xnp\":\n",
    "                offsprings.extend([parent1, parent2])\n",
    "            continue  # Skip to the next pair\n",
    "\n",
    "        max_depth_1 = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth_1 = 0\n",
    "        while isinstance(parent1, inspyred.ec.Individual) and depth_1 < max_depth_1:\n",
    "            parent1 = parent1.candidate\n",
    "            depth_1 += 1\n",
    "        max_depth_2 = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth_2 = 0\n",
    "        while isinstance(parent2, inspyred.ec.Individual) and depth_2 < max_depth_2:\n",
    "            parent2 = parent2.candidate\n",
    "            depth_2 += 1\n",
    "\n",
    "        # Select a crossover point that is not at the ends of the lists\n",
    "        cut_point = random.randint(1, len(parent1) - 1)\n",
    "\n",
    "        # Split each parent into two parts\n",
    "        parent1_A, parent1_B = parent1[:cut_point], parent1[cut_point:]\n",
    "        parent2_A, parent2_B = parent2[:cut_point], parent2[cut_point:]\n",
    "        \n",
    "        # Extract priority values and sort by these values\n",
    "        parent1_B_sorted = sorted(parent1_B, key=lambda x: x[1])\n",
    "        parent2_B_sorted = sorted(parent2_B, key=lambda x: x[1])\n",
    "\n",
    "        # Extract priority values, sort by these values, and remember original indices\n",
    "        indices1 = list(range(len(parent1_B)))\n",
    "        indices2 = list(range(len(parent2_B)))\n",
    "\n",
    "        # Create mapping from parent1_B to parent2_B based on their sorted priorities\n",
    "        for i in range(len(parent1_B_sorted)):\n",
    "            parent1_B_sorted[i] = (parent1_B_sorted[i][0], parent2_B_sorted[i][1])\n",
    "            parent2_B_sorted[i] = (parent2_B_sorted[i][0], parent1_B_sorted[i][1])\n",
    "                \n",
    "        # Restore the original order of elements based on remembered indices\n",
    "        restored_parent1_B = [None] * len(parent1_B_sorted)\n",
    "        restored_parent2_B = [None] * len(parent2_B_sorted)\n",
    "        for item, index in zip(parent1_B_sorted, indices1):\n",
    "            restored_parent1_B[index] = item\n",
    "        for item, index in zip(parent2_B_sorted, indices2):\n",
    "            restored_parent2_B[index] = item\n",
    "        \n",
    "        # Reconstruct the modified parent lists\n",
    "        new_parent1 = parent1_A + restored_parent1_B\n",
    "        new_parent2 = parent2_A + restored_parent2_B\n",
    "\n",
    "        # Attempt to evaluate the new individuals\n",
    "        try:\n",
    "            np1_selected_indexes = decode_chromosome(new_parent1)\n",
    "            # Convert list to frozenset\n",
    "            np1_indices_frozenset = frozenset(np1_selected_indexes)\n",
    "            fitness1 = fitness_evaluation(data_syn, new_parent1, np1_selected_indexes, np1_indices_frozenset, generation_counter, 'Crossover')\n",
    "            # Convert new parent1 back to an inspyred individual\n",
    "            offsprings.append(inspyred.ec.Individual(new_parent1))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to evaluate new_parent1 '{new_parent1}' due to error: {e}\")\n",
    "\n",
    "        try:\n",
    "            np2_selected_indexes = decode_chromosome(new_parent2)\n",
    "            # Convert list to frozenset\n",
    "            np2_indices_frozenset = frozenset(np2_selected_indexes)\n",
    "            fitness2 = fitness_evaluation(data_syn, new_parent2, np2_selected_indexes, np2_indices_frozenset, generation_counter, 'Crossover')\n",
    "            # Convert new parent2 back to an inspyred individual\n",
    "            offsprings.append(inspyred.ec.Individual(new_parent2))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to evaluate new_parent2 '{new_parent2}' due to error: {e}\")\n",
    "\n",
    "    print(len(offsprings))\n",
    "    return offsprings\n",
    "\n",
    "\n",
    "def nsgaii_mutate_individual(random, candidates, args):\n",
    "    global generation_counter\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return []\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "    max_generations = inner_args.get(\"max_generations\")\n",
    "    initial_mutation_rate = inner_args.get(\"initial_mutation_rate\")\n",
    "\n",
    "    mutated_candidates = []\n",
    "\n",
    "    for individual in candidates:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(individual, inspyred.ec.Individual) and depth < max_depth:\n",
    "            individual = individual.candidate\n",
    "            depth += 1\n",
    "        \n",
    "        selected_indexes = decode_chromosome(individual)\n",
    "        # Convert list to frozenset\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = history_IndexesList_dict[indices_frozenset]['fitness_score']\n",
    "\n",
    "        individual = adaptive_polynomial_mutation(individual, generation_counter, max_generations, initial_rate=initial_mutation_rate)\n",
    "\n",
    "        # Attempt to evaluate the new individuals\n",
    "        try:\n",
    "            selected_indexes = decode_chromosome(individual)\n",
    "            # Convert list to frozenset\n",
    "            indices_frozenset = frozenset(selected_indexes)\n",
    "            fitness = fitness_evaluation(data_syn, individual, selected_indexes, indices_frozenset, generation_counter, 'Mutation')\n",
    "            # Convert new parent1 back to an inspyred individual\n",
    "            mutated_candidates.append(inspyred.ec.Individual(individual))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to evaluate individual '{individual}' due to error: {e}\")\n",
    "    \n",
    "    return mutated_candidates\n",
    "\n",
    "\n",
    "def custom_observer(population, num_generations, num_evaluations, args):\n",
    "    global topic_name\n",
    "    global topic_number\n",
    "    global generation_counter\n",
    "    global nsgaii_results_path\n",
    "    global gen_stats_df_name\n",
    "    global gen_stats_df\n",
    "    global history_pareto_selections_list\n",
    "    global NSGA_II_results_name\n",
    "    global gen_pfs_df\n",
    "    global history_IndexesList_dict\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return\n",
    "\n",
    "    inner_args = args.get(\"args\")  # Retrieve the nested dictionary\n",
    "    data_syn = inner_args.get(\"data_syn\")\n",
    "    \"\"\"\n",
    "    Custom observer to handle, display, and plot both objectives.\n",
    "    \"\"\"\n",
    "    # Extract fitness values\n",
    "    fitness_tuples = [ind.fitness for ind in population]\n",
    "\n",
    "    # Convert to separate lists for plotting\n",
    "    objective_1_values = [ft[0] for ft in fitness_tuples]\n",
    "    objective_2_values = [ft[1] for ft in fitness_tuples]\n",
    "\n",
    "    # Identify Pareto front\n",
    "    def is_dominated(ind, other):\n",
    "        \"\"\" Check if ind is dominated by another individual \"\"\"\n",
    "        return all(o >= i for i, o in zip(ind.fitness, other.fitness)) and any(o > i for i, o in zip(ind.fitness, other.fitness))\n",
    "\n",
    "    pareto_front = [ind for ind in population if not any(is_dominated(ind, other) for other in population if ind != other)]\n",
    "\n",
    "    pareto_fitness_tuples = [ind.fitness for ind in pareto_front]\n",
    "\n",
    "    # Statistics\n",
    "    best_individual = max(population, key=lambda ind: (ind.fitness[0], ind.fitness[1]))\n",
    "    best_fitness = best_individual.fitness\n",
    "\n",
    "    worst_individual = min(population, key=lambda ind: (ind.fitness[0], ind.fitness[1]))\n",
    "    worst_fitness = worst_individual.fitness\n",
    "\n",
    "    mean_OverallAcc = sum(ind.fitness[0] for ind in population) / len(population)\n",
    "    mean_ClassRecall = sum(ind.fitness[1] for ind in population) / len(population)\n",
    "\n",
    "    print(f\"Generation: {num_generations}, Evaluations: {num_evaluations}\")\n",
    "    print(f\"Best Fitness: {best_fitness}\")\n",
    "    print(f\"Worst Fitness: {worst_fitness}\")\n",
    "    print(f\"Mean Overall Accuracy: {mean_OverallAcc}\")\n",
    "    print(f\"Mean {topic_name} Recall: {mean_ClassRecall}\")\n",
    "\n",
    "    # Print out the unique paths on the Pareto front\n",
    "    pareto_paths = []\n",
    "    pareto_chromosomes = []\n",
    "    pareto_fitnesses = []\n",
    "    for ind in pareto_front:\n",
    "        max_depth = 10  # Prevent infinite loops by setting a maximum depth\n",
    "        depth = 0\n",
    "        while isinstance(ind, inspyred.ec.Individual) and depth < max_depth:\n",
    "            ind = ind.candidate\n",
    "            depth += 1\n",
    "        chromosome = ind\n",
    "        decoded_path = decode_chromosome(chromosome)\n",
    "        pareto_paths.append(decoded_path)\n",
    "        pareto_chromosomes.append(chromosome)\n",
    "\n",
    "        selected_indexes = decoded_path\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        fitness_score = fitness_evaluation(data_syn, chromosome, selected_indexes, indices_frozenset, generation_counter, 'On Pareto Front')\n",
    "        pareto_fitnesses.append(fitness_score)\n",
    "\n",
    "    # Dictionary to store unique paths\n",
    "    unique_paths_dict = {}\n",
    "    for path in pareto_paths:\n",
    "        # Sort the path to create a key (as a tuple for immutability)\n",
    "        key = tuple(sorted(path))\n",
    "        # Only add the original path if the key hasn't been seen before\n",
    "        if key not in unique_paths_dict:\n",
    "            unique_paths_dict[key] = path\n",
    "    # Extract the paths; they maintain their original order from the first appearance\n",
    "    unique_pareto_paths = list(unique_paths_dict.values())\n",
    "    unique_pareto_chromosomes = [pareto_chromosomes[pareto_paths.index(list(path))] for path in unique_pareto_paths]\n",
    "    unique_pareto_fitnesses = [pareto_fitnesses[pareto_paths.index(list(path))] for path in unique_pareto_paths]\n",
    "\n",
    "    print(\"Pareto Front Selections:---------------------\")\n",
    "    for i, (path, chromosome, fitness) in enumerate(zip(unique_pareto_paths, unique_pareto_chromosomes, unique_pareto_fitnesses)):\n",
    "        print(f\"Selection {i+1}: {path} \\nFitness: {fitness}\")\n",
    "        history_pareto_selections_list.append((path, chromosome, fitness))\n",
    "        indices_frozenset = frozenset(path)\n",
    "        classification_df = history_IndexesList_dict[indices_frozenset]['classification_df']\n",
    "        # Collect all generation data into a new DataFrame row\n",
    "        gen_PFs_row = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"topic_number\": topic_number,\n",
    "            \"generation\": num_generations,\n",
    "            'fitness_score': fitness,\n",
    "            \"accuracy\": fitness[0],\n",
    "            \"topic_recall\": fitness[1],\n",
    "            'balanced_fitness_score': (classification_df.loc['accuracy', 'Balanced Accuracy'], classification_df.loc[topic_name, 'Balanced Accuracy']),\n",
    "            'overall_balanced_accuracy': classification_df.loc['accuracy', 'Balanced Accuracy'],\n",
    "            'topic_balanced_accuracy': classification_df.loc[topic_name, 'Balanced Accuracy'],\n",
    "            'balanced_acc_rec_score': (classification_df.loc[topic_name, 'Balanced Accuracy'], classification_df.loc[topic_name, 'recall']),\n",
    "            'topic_F1': classification_df.loc[topic_name, 'f1-score'],\n",
    "            'overall_F1': classification_df.loc['accuracy', 'f1-score'],\n",
    "            'overall_recall': classification_df.loc['accuracy', 'recall'],\n",
    "            \"retraining_time\": history_IndexesList_dict[indices_frozenset]['retraining_time'],\n",
    "            \"number_of_syn_sample\": len(path),\n",
    "            \"retrained_dots_list\": path,\n",
    "            'true_labels': history_IndexesList_dict[indices_frozenset]['true_labels'],\n",
    "            'predicted_labels': history_IndexesList_dict[indices_frozenset]['predicted_labels'],\n",
    "            'chromosome': chromosome\n",
    "        }\n",
    "        # # Transform DataFrame to dict format\n",
    "        # for idx in classification_df.index:\n",
    "        #     if idx != 'accuracy' and idx != 'macro avg' and idx != 'weighted avg':\n",
    "        #         gen_PFs_row[idx] = classification_df.loc[idx].dropna().to_dict()\n",
    "        # Convert the dictionary to a DataFrame for a single row\n",
    "        gen_PFs_row_df = pd.DataFrame([gen_PFs_row])\n",
    "        # Concatenate this new row DataFrame to the existing DataFrame\n",
    "        gen_pfs_df = pd.concat([gen_pfs_df, gen_PFs_row_df], ignore_index=True)\n",
    "        gen_pfs_df.to_csv(f'{nsgaii_results_path}/GenPFs_{NSGA_II_results_name}.csv', index=False)\n",
    "        gen_pfs_df.to_pickle(f'{nsgaii_results_path}/GenPFs_{NSGA_II_results_name}.pkl')\n",
    "        print('---')\n",
    "    print('------------------------')\n",
    "    \n",
    "    recall_key = f\"Mean {topic_name} Recall\"\n",
    "    # Collect all generation data into a new DataFrame row\n",
    "    new_row = {\n",
    "        \"Generation\": num_generations,\n",
    "        \"Number of Evaluations\": num_evaluations,\n",
    "        \"Best Fitness\": best_fitness,\n",
    "        \"Worst Fitness\": worst_fitness,\n",
    "        \"Mean Overall Accuracy\": mean_OverallAcc,\n",
    "        recall_key: mean_ClassRecall,\n",
    "        \"Pareto Front Selections\": [unique_pareto_paths, unique_pareto_fitnesses]\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame for a single row\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    # Concatenate this new row DataFrame to the existing DataFrame\n",
    "    gen_stats_df = pd.concat([gen_stats_df, new_row_df], ignore_index=True)\n",
    "    gen_stats_df.to_csv(f'{nsgaii_results_path}/{gen_stats_df_name}.csv', index=False)\n",
    "    gen_stats_df.to_pickle(f'{nsgaii_results_path}/{gen_stats_df_name}.pkl')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    plt.scatter(objective_1_values, objective_2_values, c='blue', alpha=0.5, label='Population')\n",
    "    plt.scatter([ft[0] for ft in pareto_fitness_tuples], [ft[1] for ft in pareto_fitness_tuples], c='red', alpha=0.9, label='Pareto Front')\n",
    "    \n",
    "    plt.xlabel('Objective 1: Overall Accuracy')\n",
    "    plt.ylabel(f'Objective 2: {topic_name} Recall')\n",
    "    plt.title(f'{topic_number}, Population and Pareto Front at Generation {num_generations}')\n",
    "    plt.legend()\n",
    "    pareto_plots_dir = f\"{nsgaii_results_path}/{gen_stats_df_name}\"\n",
    "    os.makedirs(pareto_plots_dir, exist_ok=True)\n",
    "    filename = f\"Gen_{num_generations}.png\"\n",
    "    plt.savefig(os.path.join(pareto_plots_dir, filename), dpi=200, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def custom_terminator(population, num_generations, num_evaluations, args):\n",
    "    global GPU_limit\n",
    "\n",
    "    if GPU_limit == True:\n",
    "        return True\n",
    "    # Define constants and initialization\n",
    "    max_depth = 10\n",
    "    unique_frozensets = set()\n",
    "    \n",
    "    # Loop through each individual to access the genetic encoding\n",
    "    for ind in population:\n",
    "        candidate = ind.candidate\n",
    "        depth = 0\n",
    "        \n",
    "        # Traverse to the actual candidate if wrapped in Individual class\n",
    "        while isinstance(candidate, inspyred.ec.Individual) and depth < max_depth:\n",
    "            candidate = candidate.candidate\n",
    "            depth += 1\n",
    "        \n",
    "        # Decode the chromosome to get selected indices\n",
    "        selected_indexes = decode_chromosome(candidate)\n",
    "        \n",
    "        # Convert the list of selected indexes to frozenset and add to the set\n",
    "        indices_frozenset = frozenset(selected_indexes)\n",
    "        unique_frozensets.add(indices_frozenset)\n",
    "    \n",
    "    # Check if all individuals have identical indices_frozenset\n",
    "    if len(unique_frozensets) == 1:\n",
    "        return True\n",
    "    \n",
    "    # Call the built-in generation termination condition\n",
    "    return ec.terminators.generation_termination(population, num_generations, num_evaluations, args)\n",
    "\n",
    "\n",
    "\n",
    "def run_nsga2(args, population_size=10, maximize=True, max_generations=20, num_selected=5, seed=42):\n",
    "    prng = random.Random(seed)\n",
    "\n",
    "    # Create an NSGA-II instance\n",
    "    ea = ec.emo.NSGA2(prng)\n",
    "\n",
    "    # Configure the algorithm\n",
    "    ea.observer = [custom_observer]\n",
    "    ea.terminator = custom_terminator # ec.terminators.generation_termination  # or use another termination condition\n",
    "\n",
    "    # Set custom functions\n",
    "    ea.selector = lexicase_elitist_with_cluster_tournament_selection\n",
    "    ea.variator = [nsgaii_weight_mapping_crossover, nsgaii_mutate_individual]  # Or define a simple no-op mutation function\n",
    "\n",
    "    # Run the algorithm\n",
    "    final_pop = ea.evolve(\n",
    "        generator=custom_generator,\n",
    "        evaluator=evaluate,  # Or use your own evaluate function\n",
    "        pop_size=population_size,\n",
    "        maximize=maximize,\n",
    "        max_generations=max_generations,\n",
    "        num_selected=num_selected,\n",
    "        args=args,\n",
    "    )\n",
    "\n",
    "\n",
    "    return final_pop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.752730Z",
     "iopub.status.busy": "2025-02-08T23:38:19.752349Z",
     "iopub.status.idle": "2025-02-08T23:38:19.772054Z",
     "shell.execute_reply": "2025-02-08T23:38:19.771276Z"
    }
   },
   "outputs": [],
   "source": [
    "def dominates(score1, score2):\n",
    "    \"\"\"\n",
    "    Determines if one score dominates another.\n",
    "    A score1 dominates score2 if it is better in all the objectives or equal in some and better in at least one.\n",
    "    \"\"\"\n",
    "    return (score1[0] > score2[0] and score1[1] >= score2[1]) or (score1[0] >= score2[0] and score1[1] > score2[1])\n",
    "\n",
    "def find_pareto_front(df):\n",
    "    \"\"\"\n",
    "    Marks rows as 'Yes' if they are on the Pareto front, 'No' otherwise.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Copy DataFrame to avoid modifying the original\n",
    "    df['Pareto'] = 'No'  # Initialize the Pareto column with 'No'\n",
    "    \n",
    "    scores = df['balanced_acc_rec_score'].tolist()\n",
    "    is_pareto = np.ones(len(scores), dtype=bool)  # Initialize all as True\n",
    "    \n",
    "    for i1 in range(len(scores)):\n",
    "        for i2 in range(len(scores)):\n",
    "            if i1 != i2 and dominates(scores[i2], scores[i1]):\n",
    "                is_pareto[i1] = False\n",
    "                break\n",
    "\n",
    "    # Update the 'Pareto' column based on the Pareto front\n",
    "    df.loc[is_pareto, 'Pareto'] = 'Yes'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_best_values(df):\n",
    "    # Identify the maximum values for each specified column\n",
    "    max_values = {\n",
    "        'accuracy': df['accuracy'].max(),\n",
    "        'topic_recall': df['topic_recall'].max(),\n",
    "        'overall_balanced_accuracy': df['overall_balanced_accuracy'].max(),\n",
    "        'topic_balanced_accuracy': df['topic_balanced_accuracy'].max(),\n",
    "        'topic_F1': df['topic_F1'].max(),\n",
    "        'overall_F1': df['overall_F1'].max(),\n",
    "        'overall_recall': df['overall_recall'].max()\n",
    "    }\n",
    "    \n",
    "    # Function to apply to each row to determine the best columns\n",
    "    def check_best(row):\n",
    "        return [col for col, max_val in max_values.items() if row[col] == max_val]\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df['best'] = df.apply(check_best, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def post_process(df, bch_class_df):\n",
    "    global topic_number\n",
    "    topic_name = topic_dict[topic_number]\n",
    "\n",
    "    bch_topic_recall = bch_class_df.loc[topic_name, 'recall']\n",
    "    bch_topic_balanced_accuracy = bch_class_df.loc[topic_name, 'Balanced Accuracy']\n",
    "    bch_overall_balanced_accuracy = bch_class_df.loc['accuracy', 'Balanced Accuracy']\n",
    "    bch_overall_F1_score = bch_class_df.loc['accuracy', 'f1-score']\n",
    "\n",
    "    # Calculate improvements\n",
    "    df['imp_topic_recall'] = df['topic_recall'] - bch_topic_recall\n",
    "    df['imp_topic_balanced_accuracy'] = df['topic_balanced_accuracy'] - bch_topic_balanced_accuracy\n",
    "    df['imp_overall_balanced_accuracy'] = df['overall_balanced_accuracy'] - bch_overall_balanced_accuracy\n",
    "    df['imp_overall_F1'] = df['overall_F1'] - bch_overall_F1_score\n",
    "\n",
    "    # Calculate cumulative retraining_time\n",
    "    df['cumulative_time'] = df['retraining_time'].cumsum()\n",
    "\n",
    "    # Calculate max and average improvements\n",
    "    df['max_topic_recall_imp'] = df[['imp_topic_recall']].max(axis=1).cummax()\n",
    "    df['average_topic_recall_imp'] = df[['imp_topic_recall']].mean(axis=1).expanding().mean()\n",
    "\n",
    "    df['max_topic_balanced_acc_imp'] = df[['imp_topic_balanced_accuracy']].max(axis=1).cummax()\n",
    "    df['average_topic_balanced_acc_imp'] = df[['imp_topic_balanced_accuracy']].mean(axis=1).expanding().mean()\n",
    "\n",
    "    df['max_overall_balanced_acc_imp'] = df[['imp_overall_balanced_accuracy']].max(axis=1).cummax()\n",
    "    df['average_overall_balanced_acc_imp'] = df[['imp_overall_balanced_accuracy']].mean(axis=1).expanding().mean()\n",
    "\n",
    "    df['max_overall_F1_improvement'] = df[['imp_overall_F1']].max(axis=1).cummax()\n",
    "    df['average_overall_F1_improvement'] = df[['imp_overall_F1']].mean(axis=1).expanding().mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.778471Z",
     "iopub.status.busy": "2025-02-08T23:38:19.778100Z",
     "iopub.status.idle": "2025-02-08T23:38:19.801687Z",
     "shell.execute_reply": "2025-02-08T23:38:19.800732Z"
    }
   },
   "outputs": [],
   "source": [
    "def bch_classification_report_to_df(report, y_true, y_pred):\n",
    "    global bch_class_df\n",
    "    global topic_dict\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    order_labels = list(topic_dict.values())\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    labels = df.index[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Extracting TP, FP, TN, FN for each class\n",
    "    TP = cm.diagonal()\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    sens = sum(TP) / (sum(TP)+sum(FN))\n",
    "    spec = sum(TN) / (sum(TN)+sum(FP))\n",
    "    \n",
    "    # Calculate Sensitivity (same as recall)\n",
    "    df['Sensitivity'] = df['recall']\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Assign computed specificity to dataframe except for the last three rows\n",
    "    df.loc[df.index[:-3], 'Specificity'] = specificity\n",
    "    \n",
    "    # Handling special cases\n",
    "    # Set 'accuracy' row sensitivity and specificity to the accuracy value\n",
    "    accuracy = df.loc['accuracy', 'precision']  # assuming 'precision' contains the accuracy\n",
    "    df.loc['accuracy', ['Sensitivity', 'Specificity']] = sens, spec\n",
    "    \n",
    "    # Calculate 'macro avg' and 'weighted avg' for sensitivity and specificity\n",
    "    df.loc['macro avg', 'Sensitivity'] = df.iloc[:-3]['Sensitivity'].mean()\n",
    "    df.loc['weighted avg', 'Sensitivity'] = np.average(df.iloc[:-3]['Sensitivity'], weights=df.iloc[:-3]['support'])\n",
    "    \n",
    "    df.loc['macro avg', 'Specificity'] = df.iloc[:-3]['Specificity'].mean()\n",
    "    df.loc['weighted avg', 'Specificity'] = np.average(df.iloc[:-3]['Specificity'], weights=df.iloc[:-3]['support'])\n",
    "\n",
    "    # Calculate Balanced Accuracy for each row, including special averages\n",
    "    df['Balanced Accuracy'] = (df['Sensitivity'] + df['Specificity']) / 2\n",
    "    \n",
    "    df.loc['accuracy', 'precision'] = sum(TP) / (sum(TP) + sum(FP))\n",
    "    df.loc['accuracy', 'recall'] = sum(TP) / (sum(TP) + sum(FN))\n",
    "    df.loc['accuracy', 'f1-score'] = 2* sum(TP) / (2 * sum(TP) + sum(FP) + sum(FN))\n",
    "    \n",
    "    # Calculate and append TP, FP, TN, FN metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"TN\": TN,\n",
    "        \"FN\": FN\n",
    "    }, index=labels)\n",
    "\n",
    "    # Merge the new metrics into the existing DataFrame\n",
    "    df = df.merge(metrics_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "\n",
    "    # Reorder DataFrame based on specified order labels\n",
    "    df = df.reindex(order_labels[:-1] + ['accuracy', 'macro avg', 'weighted avg'])\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_bch(X_train_re, X_test_re, Y_train_re, Y_test_re, catboost_params, itr0_path):\n",
    "    global X_test_re_Test\n",
    "    global Y_test_re_Test\n",
    "    CPU_monitor_memory_usage()\n",
    "    monitor_gpu_memory()\n",
    "    bch_dict = {}\n",
    "\n",
    "    train_pool_re = Pool(\n",
    "        X_train_re[[\"text\", \"area_TEIS\"]],\n",
    "        Y_train_re,\n",
    "        text_features=[\"text\"],\n",
    "        cat_features=[\"area_TEIS\"]\n",
    "    )\n",
    "    valid_pool_re = Pool(\n",
    "        X_test_re[[\"text\", \"area_TEIS\"]],\n",
    "        Y_test_re,\n",
    "        text_features=[\"text\"],\n",
    "        cat_features=[\"area_TEIS\"]\n",
    "    )\n",
    "\n",
    "    # Model Training\n",
    "    model_re = CatBoostClassifier(**catboost_params)\n",
    "    start_time = time.time()  # Start timing\n",
    "    model_re.fit(train_pool_re, eval_set=valid_pool_re)\n",
    "    training_time = time.time() - start_time  # End timing\n",
    "\n",
    "    # Save the retrain performances\n",
    "    val_predictions = model_re.predict(X_test_re[[\"text\", \"area_TEIS\"]])\n",
    "    val_accuracy = accuracy_score(Y_test_re, val_predictions)\n",
    "    val_report = classification_report(Y_test_re, val_predictions, digits=3, output_dict=True)\n",
    "    print(val_accuracy)\n",
    "    # print(report)\n",
    "    val_classification_df = bch_classification_report_to_df(val_report, Y_test_re, val_predictions)\n",
    "    # print(classification_df)\n",
    "    val_classification_df.to_pickle(f\"{itr0_path}/Validation_Benchmark_M0_Classdf_0.pkl\")\n",
    "    val_classification_df.to_csv(f\"{itr0_path}/Validation_Benchmark_M0_Classdf_0.csv\", index=True)\n",
    "\n",
    "    # Save the retrain performances\n",
    "    predictions = model_re.predict(X_test_re_Test[[\"text\", \"area_TEIS\"]])\n",
    "    accuracy = accuracy_score(Y_test_re_Test, predictions)\n",
    "    report = classification_report(Y_test_re_Test, predictions, digits=3, output_dict=True)\n",
    "    print(accuracy)\n",
    "    # print(report)\n",
    "    classification_df = bch_classification_report_to_df(report, Y_test_re_Test, predictions)\n",
    "    # print(classification_df)\n",
    "\n",
    "    classification_df.to_pickle(f\"{itr0_path}/Benchmark_M0_Classdf_0.pkl\")\n",
    "    classification_df.to_csv(f\"{itr0_path}/Benchmark_M0_Classdf_0.csv\", index=True)\n",
    "\n",
    "    bch_dict['model'] = model_re\n",
    "    bch_dict['classification_df'] = classification_df\n",
    "    bch_dict['accuracy'] = accuracy\n",
    "    bch_dict['retraining_time'] = training_time\n",
    "\n",
    "    return bch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:38:19.808113Z",
     "iopub.status.busy": "2025-02-08T23:38:19.807746Z",
     "iopub.status.idle": "2025-02-09T00:46:23.366807Z",
     "shell.execute_reply": "2025-02-09T00:46:23.365587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.0%\n",
      "Current GPU memory usage: 1.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TBB Warning: The number of workers is currently limited to 11. The request for 95 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8438299232736572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453964194373401\n",
      "CPU Current memory usage: 1.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.850383631713555, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8447890025575447, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.5671641791044776)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8511828644501279, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8427109974424553, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8502237851662404, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8502237851662404, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8444693094629157, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 0, Evaluations: 20\n",
      "Best Fitness: (0.6268656716417911, 0)\n",
      "Worst Fitness: (0.5671641791044776, 0)\n",
      "Mean Overall Accuracy: 0.5992537313432835\n",
      "Mean Email security and attachments. Recall: 0.0\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "Pareto Front Selections:---------------------\n",
      "Selection 1: [26870, 21903, 3620, 32693, 25850, 977, 15021, 6843, 35799, 12567, 13129, 29489, 4860, 31778, 14500, 38565, 8907, 13870, 13270, 22046, 35696, 12162, 13162, 4935, 7404, 4714, 23907, 8293, 33479, 8662, 6752, 17124, 3367, 15379, 2773, 1079, 19208, 28767, 6812, 17612, 31052, 23155, 22316, 21656, 23038, 37120, 10794, 24661, 18294, 33437, 7648, 18966, 4699, 19190, 5211, 15454, 23302, 20855, 576, 6538, 17801, 26447, 15597, 5226, 12176, 3512, 29594, 20472, 968, 34681, 10873, 9680, 24223, 10615, 3375, 7496, 19188, 2880, 14830, 11667, 1096, 4517, 331, 24604, 8869, 8341, 29563, 38329, 25911, 3888, 15278, 29635, 12175, 10476, 2783, 24631, 1373, 20968, 1413, 3156, 2895, 30230, 3191, 8694, 27936, 24944, 5563, 1215, 3153, 35733, 29694, 1622, 19729, 3333, 14141, 15080, 9419, 29097, 924, 13660, 7647, 35189] \n",
      "Fitness: (0.6268656716417911, 0)\n",
      "---\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20234/3904353796.py:838: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  gen_stats_df = pd.concat([gen_stats_df, new_row_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8516624040920716, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8430306905370843, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8524616368286445, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 1.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8449488491048593, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8486253196930946, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8519820971867008, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8383951406649617, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8441496163682864, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8407928388746803, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8452685421994884, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8531010230179028, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8500639386189258, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8486253196930946, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8414322250639387, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8436700767263428, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8439897698209718, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8483056265984654, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8428708439897699, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8427109974424553, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8495843989769821, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8441496163682864, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.850383631713555, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8462276214833759, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8494245524296675, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8441496163682864, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8526214833759591, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8523017902813299, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 2.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8494245524296675, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8452685421994884, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.851502557544757, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.850383631713555, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.84846547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8495843989769821, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8441496163682864, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8483056265984654, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8467071611253197, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8406329923273658, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "316\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8475063938618926, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8516624040920716, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845108695652174, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8470268542199488, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8444693094629157, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.842071611253197, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8444693094629157, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8449488491048593, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8523017902813299, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8452685421994884, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 3.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.6417910447761194)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8556585677749361, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8452685421994884, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8486253196930946, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8470268542199488, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8523017902813299, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8529411764705882, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8494245524296675, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8502237851662404, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8510230179028133, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8494245524296675, 0.6417910447761194)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.84846547314578, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8510230179028133, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.850383631713555, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8524616368286445, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8446291560102301, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8526214833759591, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8475063938618926, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8438299232736572, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8374360613810742, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.84846547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8439897698209718, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8462276214833759, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 4.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.850383631713555, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8486253196930946, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8427109974424553, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8467071611253197, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8483056265984654, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.842391304347826, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8459079283887468, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8513427109974424, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8502237851662404, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.844309462915601, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8414322250639387, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8475063938618926, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8499040920716112, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8475063938618926, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8523017902813299, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8387148337595908, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8483056265984654, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.6417910447761194)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8479859335038363, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.844309462915601, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 5.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8519820971867008, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8438299232736572, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8542199488491049, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8417519181585678, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.845428388746803, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8505434782608695, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.0%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8383951406649617, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8463874680306905, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8467071611253197, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8500639386189258, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.1%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8492647058823529, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8524616368286445, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.2%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8516624040920716, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8475063938618926, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.3%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847346547314578, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8460677749360613, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8467071611253197, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.582089552238806)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8462276214833759, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.4%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8508631713554987, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8457480818414322, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8462276214833759, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.840153452685422, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8455882352941176, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8435102301790282, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8487851662404092, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.5%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8481457800511509, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8489450127877238, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8468670076726342, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8497442455242967, 0.6417910447761194)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.6%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8404731457800512, 0.5970149253731343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8465473145780051, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8491048593350383, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8500639386189258, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8486253196930946, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8449488491048593, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.7%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8476662404092071, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8500639386189258, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8550191815856778, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.8%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.843190537084399, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8471867007672634, 0.6119402985074627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8478260869565217, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8500639386189258, 0.6268656716417911)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n",
      "CPU Current memory usage: 6.9%\n",
      "Current GPU memory usage: 2.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          precision    recall  f1-score  \\\n",
      "IT support and assistance.                 0.978748  0.974828  0.976784   \n",
      "Account activation and access issues.      0.912478  0.932271  0.922268   \n",
      "Password and device security.              0.926454  0.910464  0.918390   \n",
      "Printer issues and troubleshooting.        0.884615  0.818505  0.850277   \n",
      "HP Dock connectivity issues.               0.786792  0.782364  0.784572   \n",
      "Employee documentation and errors.         0.573086  0.662198  0.614428   \n",
      "\"Access and login issues\"                  0.757869  0.792405  0.774752   \n",
      "Opening and managing files/devices.        0.678414  0.652542  0.665227   \n",
      "Mobile email and VPN setup.                0.610465  0.755396  0.675241   \n",
      "IT support and communication.              0.568690  0.531343  0.549383   \n",
      "Error handling in RPG programming.         1.000000  0.946809  0.972678   \n",
      "Email security and attachments.            0.569231  0.578125  0.573643   \n",
      "Humanitarian aid for Ukraine.              0.736842  0.311111  0.437500   \n",
      "Internet connectivity issues in offices.   0.654206  0.518519  0.578512   \n",
      "Improving integration with Infojobs.       0.854839  0.452991  0.592179   \n",
      "accuracy                                   0.852430  0.852430  0.852430   \n",
      "macro avg                                  0.766182  0.707991  0.725722   \n",
      "weighted avg                               0.854766  0.852430  0.851465   \n",
      "\n",
      "                                             support  Sensitivity  \\\n",
      "IT support and assistance.                1748.00000     0.974828   \n",
      "Account activation and access issues.     2259.00000     0.932271   \n",
      "Password and device security.              927.00000     0.910464   \n",
      "Printer issues and troubleshooting.        281.00000     0.818505   \n",
      "HP Dock connectivity issues.               533.00000     0.782364   \n",
      "Employee documentation and errors.         373.00000     0.662198   \n",
      "\"Access and login issues\"                  395.00000     0.792405   \n",
      "Opening and managing files/devices.        236.00000     0.652542   \n",
      "Mobile email and VPN setup.                278.00000     0.755396   \n",
      "IT support and communication.              335.00000     0.531343   \n",
      "Error handling in RPG programming.          94.00000     0.946809   \n",
      "Email security and attachments.             64.00000     0.578125   \n",
      "Humanitarian aid for Ukraine.               45.00000     0.311111   \n",
      "Internet connectivity issues in offices.   135.00000     0.518519   \n",
      "Improving integration with Infojobs.       117.00000     0.452991   \n",
      "accuracy                                     0.85243     0.852430   \n",
      "macro avg                                 7820.00000     0.707991   \n",
      "weighted avg                              7820.00000     0.852430   \n",
      "\n",
      "                                          Specificity  Balanced Accuracy  \\\n",
      "IT support and assistance.                   0.993906           0.984367   \n",
      "Account activation and access issues.        0.963676           0.947973   \n",
      "Password and device security.                0.990280           0.950372   \n",
      "Printer issues and troubleshooting.          0.996021           0.907263   \n",
      "HP Dock connectivity issues.                 0.984493           0.883428   \n",
      "Employee documentation and errors.           0.975292           0.818745   \n",
      "\"Access and login issues\"                    0.986532           0.889469   \n",
      "Opening and managing files/devices.          0.990374           0.821458   \n",
      "Mobile email and VPN setup.                  0.982233           0.868814   \n",
      "IT support and communication.                0.981964           0.756654   \n",
      "Error handling in RPG programming.           1.000000           0.973404   \n",
      "Email security and attachments.              0.996390           0.787257   \n",
      "Humanitarian aid for Ukraine.                0.999357           0.655234   \n",
      "Internet connectivity issues in offices.     0.995185           0.756852   \n",
      "Improving integration with Infojobs.         0.998832           0.725912   \n",
      "accuracy                                     0.989459           0.920944   \n",
      "macro avg                                    0.988969           0.848480   \n",
      "weighted avg                                 0.982105           0.917267   \n",
      "\n",
      "                                          Diff precision  Diff recall  \\\n",
      "IT support and assistance.                     -0.003197     0.010297   \n",
      "Account activation and access issues.           0.008067     0.006640   \n",
      "Password and device security.                   0.015894    -0.001079   \n",
      "Printer issues and troubleshooting.            -0.016570     0.007117   \n",
      "HP Dock connectivity issues.                    0.006074     0.007505   \n",
      "Employee documentation and errors.              0.002930    -0.024129   \n",
      "\"Access and login issues\"                       0.048119     0.000000   \n",
      "Opening and managing files/devices.             0.020081    -0.016949   \n",
      "Mobile email and VPN setup.                     0.000856     0.025180   \n",
      "IT support and communication.                   0.001576     0.026866   \n",
      "Error handling in RPG programming.              0.000000     0.000000   \n",
      "Email security and attachments.                -0.152991    -0.031250   \n",
      "Humanitarian aid for Ukraine.                  -0.025063    -0.044444   \n",
      "Internet connectivity issues in offices.       -0.045794     0.000000   \n",
      "Improving integration with Infojobs.           -0.020161     0.034188   \n",
      "accuracy                                        0.005243     0.005243   \n",
      "macro avg                                      -0.010679    -0.000004   \n",
      "weighted avg                                    0.004104     0.005243   \n",
      "\n",
      "                                          Diff f1-score  Diff support  \\\n",
      "IT support and assistance.                     0.003624      0.000000   \n",
      "Account activation and access issues.          0.007370      0.000000   \n",
      "Password and device security.                  0.007338      0.000000   \n",
      "Printer issues and troubleshooting.           -0.003655      0.000000   \n",
      "HP Dock connectivity issues.                   0.006794      0.000000   \n",
      "Employee documentation and errors.            -0.008443      0.000000   \n",
      "\"Access and login issues\"                      0.025949      0.000000   \n",
      "Opening and managing files/devices.            0.001361      0.000000   \n",
      "Mobile email and VPN setup.                    0.010757      0.000000   \n",
      "IT support and communication.                  0.015417      0.000000   \n",
      "Error handling in RPG programming.             0.000000      0.000000   \n",
      "Email security and attachments.               -0.087374      0.000000   \n",
      "Humanitarian aid for Ukraine.                 -0.047348      0.000000   \n",
      "Internet connectivity issues in offices.      -0.017232      0.000000   \n",
      "Improving integration with Infojobs.           0.025705      0.000000   \n",
      "accuracy                                       0.005243      0.005243   \n",
      "macro avg                                     -0.003982      0.000000   \n",
      "weighted avg                                   0.005232      0.000000   \n",
      "\n",
      "                                          Diff Sensitivity  Diff Specificity  \\\n",
      "IT support and assistance.                        0.010297         -0.000988   \n",
      "Account activation and access issues.             0.006640          0.003417   \n",
      "Password and device security.                    -0.001079          0.002321   \n",
      "Printer issues and troubleshooting.               0.007117         -0.000663   \n",
      "HP Dock connectivity issues.                      0.007505          0.000412   \n",
      "Employee documentation and errors.               -0.024129          0.001209   \n",
      "\"Access and login issues\"                         0.000000          0.003771   \n",
      "Opening and managing files/devices.              -0.016949          0.001187   \n",
      "Mobile email and VPN setup.                       0.025180         -0.000530   \n",
      "IT support and communication.                     0.026866         -0.000802   \n",
      "Error handling in RPG programming.                0.000000          0.000000   \n",
      "Email security and attachments.                  -0.031250         -0.001676   \n",
      "Humanitarian aid for Ukraine.                    -0.044444          0.000000   \n",
      "Internet connectivity issues in offices.          0.000000         -0.000911   \n",
      "Improving integration with Infojobs.              0.034188         -0.000260   \n",
      "accuracy                                          0.005243          0.000374   \n",
      "macro avg                                        -0.000004          0.000432   \n",
      "weighted avg                                      0.005243          0.001243   \n",
      "\n",
      "                                          Diff Balanced Accuracy  Accuracy  \\\n",
      "IT support and assistance.                              0.004655  0.974828   \n",
      "Account activation and access issues.                   0.005028  0.932271   \n",
      "Password and device security.                           0.000621  0.910464   \n",
      "Printer issues and troubleshooting.                     0.003227  0.818505   \n",
      "HP Dock connectivity issues.                            0.003958  0.782364   \n",
      "Employee documentation and errors.                     -0.011460  0.662198   \n",
      "\"Access and login issues\"                               0.001886  0.792405   \n",
      "Opening and managing files/devices.                    -0.007881  0.652542   \n",
      "Mobile email and VPN setup.                             0.012325  0.755396   \n",
      "IT support and communication.                           0.013032  0.531343   \n",
      "Error handling in RPG programming.                      0.000000  0.946809   \n",
      "Email security and attachments.                        -0.016463  0.578125   \n",
      "Humanitarian aid for Ukraine.                          -0.022222  0.311111   \n",
      "Internet connectivity issues in offices.               -0.000455  0.518519   \n",
      "Improving integration with Infojobs.                    0.016964  0.452991   \n",
      "accuracy                                                0.002809  0.852430   \n",
      "macro avg                                               0.000214  0.725722   \n",
      "weighted avg                                            0.003243  0.851465   \n",
      "\n",
      "                                              TP     FP      TN     FN  \n",
      "IT support and assistance.                1704.0   37.0  6035.0   44.0  \n",
      "Account activation and access issues.     2106.0  202.0  5359.0  153.0  \n",
      "Password and device security.              844.0   67.0  6826.0   83.0  \n",
      "Printer issues and troubleshooting.        230.0   30.0  7509.0   51.0  \n",
      "HP Dock connectivity issues.               417.0  113.0  7174.0  116.0  \n",
      "Employee documentation and errors.         247.0  184.0  7263.0  126.0  \n",
      "\"Access and login issues\"                  313.0  100.0  7325.0   82.0  \n",
      "Opening and managing files/devices.        154.0   73.0  7511.0   82.0  \n",
      "Mobile email and VPN setup.                210.0  134.0  7408.0   68.0  \n",
      "IT support and communication.              178.0  135.0  7350.0  157.0  \n",
      "Error handling in RPG programming.          89.0    0.0  7726.0    5.0  \n",
      "Email security and attachments.             37.0   28.0  7728.0   27.0  \n",
      "Humanitarian aid for Ukraine.               14.0    5.0  7770.0   31.0  \n",
      "Internet connectivity issues in offices.    70.0   37.0  7648.0   65.0  \n",
      "Improving integration with Infojobs.        53.0    9.0  7694.0   64.0  \n",
      "accuracy                                     NaN    NaN     NaN    NaN  \n",
      "macro avg                                    NaN    NaN     NaN    NaN  \n",
      "weighted avg                                 NaN    NaN     NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run = 1\n",
    "    rand = 10\n",
    "    \n",
    "    run_path = f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}\"\n",
    "    itr0_path = f\"{run_path}/Iteration_0\"\n",
    "    os.makedirs(itr0_path, exist_ok=True)\n",
    "\n",
    "    # Load Data\n",
    "    data = pd.read_csv(f'D:/AutoGeTS/Data/tickets_topics.csv',lineterminator='\\n')\n",
    "    data_topic = data.dropna().reset_index()\n",
    "    data_topic = data_topic.rename(columns={'index': 'index_meta'})\n",
    "\n",
    "    X_train_r_both, X_test_re_Test, Y_train_r_both, Y_test_re_Test = train_test_split(data_topic, data_topic.topic_name, test_size = 0.2,random_state = 42)\n",
    "        \n",
    "    # Further split the training set to create a validation set\n",
    "    X_train_r, X_test_re, Y_train_r, Y_test_re = train_test_split(\n",
    "        X_train_r_both, \n",
    "        Y_train_r_both, \n",
    "        test_size=0.2,  # 20% of the initial training set, which is 16% of the original data\n",
    "        random_state=rand\n",
    "    )\n",
    "\n",
    "    catboost_params = {'iterations': 300, 'learning_rate': 0.2, 'depth': 8, 'l2_leaf_reg': 1, \n",
    "                        'bagging_temperature': 1, 'random_strength': 1, 'border_count': 254, \n",
    "                        'eval_metric': 'TotalF1', 'task_type': 'GPU', 'early_stopping_rounds': 20, 'use_best_model': True, 'verbose': 0, 'random_seed': rand}\n",
    "\n",
    "    topic_dict = {\"T1\": \"IT support and assistance.\",\"T2\": \"Account activation and access issues.\",\"T3\": \"Password and device security.\",\n",
    "                \"T4\": \"Printer issues and troubleshooting.\",\"T5\": \"HP Dock connectivity issues.\",\"T6\": \"Employee documentation and errors.\",\n",
    "                \"T7\": \"\\\"Access and login issues\\\"\",\"T8\": \"Opening and managing files/devices.\",\"T9\": \"Mobile email and VPN setup.\",\n",
    "                \"T10\": \"IT support and communication.\",\"T11\": \"Error handling in RPG programming.\",\"T12\": \"Email security and attachments.\",\n",
    "                \"T13\": \"Humanitarian aid for Ukraine.\",\"T14\": \"Internet connectivity issues in offices.\",\"T15\": \"Improving integration with Infojobs.\", \"Acc\": \"accuracy\"}\n",
    "    \n",
    "    bch_dict = train_bch(X_train_r, X_test_re, Y_train_r, Y_test_re, catboost_params, itr0_path)\n",
    "    for iteration in [2]:\n",
    "        # if iteration <= 1:\n",
    "        #     continue\n",
    "        if iteration == 2:\n",
    "            topic_number = \"T12\"\n",
    "        # elif iteration == 2:\n",
    "        #     topic_number = \"T12\"\n",
    "        # for topic_number in [\"T13\"]: # \"T11\", \"T15\"\n",
    "        # iteration = 1\n",
    "        prev_itr = iteration - 1\n",
    "\n",
    "        with open(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{prev_itr}/T12_CR_imp_max.pkl\", \"rb\") as file:\n",
    "            t12_cr_imp_max = pickle.load(file)\n",
    "\n",
    "        gpu_hours = 1\n",
    "\n",
    "        if topic_number in []:\n",
    "            metric_name = \"overall_balanced_accuracy\" # \"recall\", \"Balanced Accuracy\", \"overall_balanced_accuracy\", \"overall_f1-score\"\n",
    "        elif topic_number in [\"T1\"]:\n",
    "            metric_name = \"overall_f1-score\"\n",
    "        elif topic_number in [\"T11\", \"T13\", \"T14\", \"T15\"]:\n",
    "            metric_name = \"Balanced Accuracy\"\n",
    "        elif topic_number in [\"T12\"]:\n",
    "            metric_name = \"recall\"\n",
    "\n",
    "        nsgaii_results_path = f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{iteration}/{topic_number}_{metric_name}_GPU{gpu_hours}h_GA\"\n",
    "        os.makedirs(nsgaii_results_path, exist_ok=True)\n",
    "\n",
    "        fold_pfs_df = pd.DataFrame() \n",
    "        gen_eval_df = pd.DataFrame() \n",
    "        gen_pfs_df = pd.DataFrame() \n",
    "        test_gen_eval_df = pd.DataFrame() \n",
    "\n",
    "        init_counter = 0\n",
    "        generation_counter = -1\n",
    "        history_IndexesList_dict = {}\n",
    "        history_pareto_selections_list = []\n",
    "        gen_stats_df = pd.DataFrame(columns=[\"Generation\", \"Number of Evaluations\", \"Best Fitness\", \"Worst Fitness\", \"Mean Overall Accuracy\", \"Mean Topic Recall\", \"Pareto Front Selections\"])\n",
    "\n",
    "        if iteration > 1:\n",
    "            bch_class_df = pd.read_pickle(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{prev_itr}/Bch_Itr_{prev_itr}.pkl\")\n",
    "            bch_filtered_columns = [col for col in bch_class_df.columns if not col.startswith(\"Diff\") and col != \"Accuracy\"]\n",
    "            bch_class_df = bch_class_df[bch_filtered_columns]\n",
    "        else:\n",
    "            bch_class_df = pd.read_pickle(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_0/Benchmark_M0_Classdf_0.pkl\")\n",
    "\n",
    "        bch_m0 = pd.read_pickle(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_0/Benchmark_M0_Classdf_0.pkl\")\n",
    "        \n",
    "        if iteration > 1:\n",
    "            prev_itr_X_train_re = pd.read_pickle(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{prev_itr}/X_train_re_itr_{prev_itr}.pkl\")\n",
    "            X_train_r = prev_itr_X_train_re\n",
    "            prev_itr_Y_train_re = pd.read_pickle(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{prev_itr}/Y_train_re_itr_{prev_itr}.pkl\")\n",
    "            Y_train_r = prev_itr_Y_train_re\n",
    "        \n",
    "        low_recall_topics_order = [[\"Acc\", \"T13\"], [\"T15\", \"T12\", \"T10\", \"T14\"], [\"T6\", \"T8\", \"T9\", \"T7\", \"T5\", \"T4\"], [\"T3\", \"T2\", \"T11\", \"T1\"]]\n",
    "        large_size_topics_order = [[\"Acc\"], [\"T2\", \"T1\", \"T3\"], [\"T5\", \"T7\", \"T6\", \"T10\", \"T4\", \"T9\", \"T8\"], [\"T14\", \"T15\", \"T11\", \"T12\", \"T13\"]]\n",
    "        topic_group_probabilities = [0.4, 0.3, 0.2, 0.1]\n",
    "        topic_name_cases_order = [[topic_dict[topic_number] for topic_number in sublist] for sublist in low_recall_topics_order]\n",
    "        topic_name_sizes_order = [[topic_dict[topic_number] for topic_number in sublist] for sublist in large_size_topics_order]\n",
    "\n",
    "\n",
    "        \"\"\"Section below are changable parameters/inputs----------------------------\"\"\"\n",
    "        # topic_number = \"T13\"\n",
    "        syn_number = 1\n",
    "\n",
    "        total_gpu_seconds = gpu_hours * 60 * 60\n",
    "        \n",
    "        # Added synthetic data path\n",
    "        if topic_number in [\"T1\", \"T2\"]:\n",
    "            data_syn_raw = pd.read_pickle(f'D:/AutoGeTS/Synthetic_Data/{topic_number}-synthesis-{syn_number}.pkl')\n",
    "        else:\n",
    "            data_syn_raw = pd.read_csv(f'D:/AutoGeTS/Synthetic_Data/{topic_number}-synthesis-{syn_number}.csv',lineterminator='\\n')\n",
    "        data_syn = data_syn_raw[[\"index_meta\", \"text\", \"area_TEIS\", 'topic_name', \"sample\"]].dropna()\n",
    "\n",
    "        Xmode = \"\" # \"\", \"Xnp\"\n",
    "        Smode = \"Sovl\"\n",
    "        population_size = 20\n",
    "        num_selected = 20\n",
    "        max_generations = 15\n",
    "        tournament_size = 3\n",
    "        crossover_rate = 0.7\n",
    "        initial_mutation_rate = 0.3\n",
    "        maximize=True\n",
    "\n",
    "        NSGA_II_results_name = f\"{topic_number}_{Xmode}_LexClS-{Smode}_PopSize{population_size}_NumSel{num_selected}_MaxGen{max_generations}_CR{crossover_rate}_MR{initial_mutation_rate}\"\n",
    "        gen_stats_df_name = f\"NSGAII-Gen-Stats_{topic_number}_{Xmode}_LexClS-{Smode}_PopSize{population_size}_NumSel{num_selected}_MaxGen{max_generations}_CR{crossover_rate}_MR{initial_mutation_rate}\"\n",
    "        history_dict_name = f\"NSGAII-Retrain-Dict_{topic_number}_{Xmode}_LexClS-{Smode}_PopSize{population_size}_NumSel{num_selected}_MaxGen{max_generations}_CR{crossover_rate}_MR{initial_mutation_rate}\"\n",
    "        final_pop_name = f\"NSGAII-FinalPop_{topic_number}_{Xmode}_LexClS-{Smode}_PopSize{population_size}_NumSel{num_selected}_MaxGen{max_generations}_CR{crossover_rate}_MR{initial_mutation_rate}\"\n",
    "        history_pareto_lists_name = f\"NSGAII-HistoryPareto-List_{topic_number}_{Xmode}_LexClS-{Smode}_PopSize{population_size}_NumSel{num_selected}_MaxGen{max_generations}_CR{crossover_rate}_MR{initial_mutation_rate}\"\n",
    "        \n",
    "        # catboost_params = {'iterations': 300, 'learning_rate': 0.2, 'depth': 6, 'l2_leaf_reg': 1, \n",
    "        #                    'bagging_temperature': 1, 'random_strength': 1, 'border_count': 254, \n",
    "        #                    'eval_metric': 'TotalF1', 'task_type': 'GPU', 'early_stopping_rounds': 20, 'use_best_model': True, 'verbose': 1, 'random_seed': 0}\n",
    "\n",
    "        # catboost_params = {'iterations': 300, 'learning_rate': 0.2, 'depth': 8, 'l2_leaf_reg': 3, \n",
    "        #                    'bagging_temperature': 1, 'random_strength': 1, 'border_count': 254, \n",
    "        #                    'eval_metric': 'TotalF1', 'task_type': 'GPU', 'early_stopping_rounds': 20, 'use_best_model': True, 'verbose': 1, 'random_seed': 0}\n",
    "\n",
    "        # catboost_params = {'iterations': 300, 'learning_rate': 0.5, 'depth': 6, 'l2_leaf_reg': 10, \n",
    "        #                    'bagging_temperature': 1, 'random_strength': 1, 'border_count': 254, \n",
    "        #                    'eval_metric': 'TotalF1', 'task_type': 'GPU', 'early_stopping_rounds': 20, 'use_best_model': True, 'verbose': 1, 'random_seed': 0}\n",
    "\n",
    "        \"\"\"-----------------------------------\"\"\"\n",
    "\n",
    "        topic_name = topic_dict[topic_number]\n",
    "        clean_topic_name = clean_folder_name(topic_name)\n",
    "\n",
    "        sum_GPU_seconds = 0\n",
    "        GPU_limit = False\n",
    "\n",
    "        class_data_pool = X_train_r[X_train_r['topic_name'] == topic_name]\n",
    "        class_index_meta_pool = class_data_pool['index_meta'].tolist()\n",
    "\n",
    "        args={\"data_syn\": data_syn,\n",
    "            \"max_generations\": max_generations,\n",
    "            \"num_selected\": num_selected,  # Or another suitable size\n",
    "            \"crossover_rate\": crossover_rate,\n",
    "            \"initial_mutation_rate\": initial_mutation_rate,\n",
    "            \"class_index_meta_pool\": class_index_meta_pool,\n",
    "                }\n",
    "\n",
    "\n",
    "        final_pop = run_nsga2(args, population_size=population_size, maximize=maximize, max_generations=max_generations, num_selected=num_selected, seed=42)\n",
    "        if GPU_limit == True:\n",
    "            gen_eval_df  = find_pareto_front(gen_eval_df)\n",
    "            gen_eval_df = find_best_values(gen_eval_df)\n",
    "            gen_eval_df = post_process(gen_eval_df, bch_class_df)\n",
    "            gen_eval_df.to_csv(f'{nsgaii_results_path}/GenAllEvals_{NSGA_II_results_name}.csv', index=True)\n",
    "            gen_eval_df.to_pickle(f'{nsgaii_results_path}/GenAllEvals_{NSGA_II_results_name}.pkl')\n",
    "            # break\n",
    "            test_gen_eval_df  = find_pareto_front(test_gen_eval_df)\n",
    "            test_gen_eval_df = find_best_values(test_gen_eval_df)\n",
    "            test_gen_eval_df = post_process(test_gen_eval_df, bch_class_df)\n",
    "            test_gen_eval_df.to_csv(f'{nsgaii_results_path}/test_GenAllEvals_{NSGA_II_results_name}.csv', index=True)\n",
    "            test_gen_eval_df.to_pickle(f'{nsgaii_results_path}/test_GenAllEvals_{NSGA_II_results_name}.pkl')\n",
    "            # break\n",
    "        \n",
    "        iteration_repo = f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{iteration}/\"\n",
    "        no_improve_repo = os.path.join(iteration_repo, \"Itr_No_Improve\")\n",
    "\n",
    "        os.makedirs(iteration_repo, exist_ok=True)  # Always need iteration_repo\n",
    "        \n",
    "        def store_classification_df(df, repo_path, iteration):\n",
    "            \"\"\"Helper to store classification DataFrame in CSV/PKL format.\"\"\"\n",
    "            csv_path = os.path.join(repo_path, f\"Bch_Itr_{iteration}.csv\")\n",
    "            pkl_path = os.path.join(repo_path, f\"Bch_Itr_{iteration}.pkl\")\n",
    "            df.to_csv(csv_path, index=True)\n",
    "            df.to_pickle(pkl_path)\n",
    "\n",
    "        def store_xy_data(X, Y, repo_path, iteration):\n",
    "            \"\"\"Helper to store X_train_re/Y_train_re in PKL format.\"\"\"\n",
    "            X_path = os.path.join(repo_path, f\"X_train_re_itr_{iteration}.pkl\")\n",
    "            Y_path = os.path.join(repo_path, f\"Y_train_re_itr_{iteration}.pkl\")\n",
    "            X.to_pickle(X_path)\n",
    "            Y.to_pickle(Y_path)\n",
    "\n",
    "        \"\"\"Extract best model and append synthetics\"\"\"\n",
    "        # # Find the index of the row with the largest value in the 'max_overall_balanced_acc_imp' column\n",
    "        # index_of_max_imp = test_gen_eval_df['T12_CR_Imp'].idxmax()\n",
    "        # print(index_of_max_imp)\n",
    "\n",
    "        # # Retrieve the row corresponding to this index\n",
    "        # row_with_largest_value = test_gen_eval_df.loc[index_of_max_imp]\n",
    "\n",
    "        constrained_df = test_gen_eval_df[\n",
    "            (test_gen_eval_df['T12_CR_Imp'] >= 0) & \n",
    "            (test_gen_eval_df['T7_CR_Imp'] >= 0) & \n",
    "            (test_gen_eval_df['OF1_Imp'] >= 0)\n",
    "        ]\n",
    "\n",
    "        if constrained_df.empty:\n",
    "            # Create the \"constrained_df_empty\" folder\n",
    "            empty_folder_path = os.path.join(iteration_repo, \"constrained_df_empty\")\n",
    "            os.makedirs(empty_folder_path, exist_ok=True)\n",
    "            \n",
    "            # If there's no row that satisfies the requirement,\n",
    "            # then we do the \"no improvement\" logic:\n",
    "            store_classification_df(bch_class_df, iteration_repo, iteration)\n",
    "            store_xy_data(prev_itr_X_train_re, prev_itr_Y_train_re, iteration_repo, iteration)\n",
    "\n",
    "            with open(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{iteration}/T12_CR_imp_max.pkl\", \"wb\") as file:\n",
    "                pickle.dump(t12_cr_imp_max, file)\n",
    "            \n",
    "        else:\n",
    "            # Sort descending by T12_CR_Imp, then topic_balanced_accuracy, then overall_balanced_accuracy\n",
    "            sorted_df = constrained_df.sort_values(\n",
    "                by=[\"Super_Obj_T12T7_CR\", \"T12_CR_Imp\", \"Super_Obj_T12T7_CBA\", \"overall_balanced_accuracy\"],\n",
    "                ascending=False\n",
    "            )\n",
    "\n",
    "            # Pick the first row from the sorted DataFrame\n",
    "            row_with_largest_value = sorted_df.iloc[0]\n",
    "\n",
    "            largest_value = row_with_largest_value[\"T12_CR_Imp\"]\n",
    "            if t12_cr_imp_max < largest_value:\n",
    "                t12_cr_imp_max = largest_value\n",
    "            \n",
    "            with open(f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{iteration}/T12_CR_imp_max.pkl\", \"wb\") as file:\n",
    "                pickle.dump(t12_cr_imp_max, file)\n",
    "\n",
    "            filtered_syn_df = data_syn[data_syn['index_meta'].isin(row_with_largest_value['retrained_dots_list'])]\n",
    "\n",
    "            X_train_re = pd.concat([X_train_r, filtered_syn_df.drop(columns=['topic_name'])])\n",
    "            Y_train_re = pd.concat([Y_train_r, filtered_syn_df['topic_name']])\n",
    "\n",
    "            train_pool_re = Pool(\n",
    "                X_train_re[[\"text\", \"area_TEIS\"]],\n",
    "                Y_train_re,\n",
    "                text_features=[\"text\"],\n",
    "                cat_features=[\"area_TEIS\"]\n",
    "            )\n",
    "            valid_pool_re = Pool(\n",
    "                X_test_re[[\"text\", \"area_TEIS\"]],\n",
    "                Y_test_re,\n",
    "                text_features=[\"text\"],\n",
    "                cat_features=[\"area_TEIS\"]\n",
    "            )\n",
    "\n",
    "            catboost_params = catboost_params\n",
    "                        \n",
    "            # Model Training\n",
    "            model_re = CatBoostClassifier(**catboost_params)\n",
    "            # start_time = time.time()  # Start timing\n",
    "            model_re.fit(train_pool_re, eval_set=valid_pool_re)\n",
    "            # training_time = time.time() - start_time  # End timing\n",
    "\n",
    "            # Save the retrain performances\n",
    "            predictions = model_re.predict(X_test_re_Test[[\"text\", \"area_TEIS\"]])\n",
    "            accuracy = accuracy_score(Y_test_re_Test, predictions)\n",
    "            report = classification_report(Y_test_re_Test, predictions, digits=6, output_dict=True)\n",
    "            classification_df = classification_report_to_df(report, Y_test_re_Test, predictions)\n",
    "\n",
    "            print(classification_df)\n",
    "\n",
    "            def store_classification_df(df, repo_path, iteration):\n",
    "                \"\"\"Helper to store classification DataFrame in CSV/PKL format.\"\"\"\n",
    "                csv_path = os.path.join(repo_path, f\"Bch_Itr_{iteration}.csv\")\n",
    "                pkl_path = os.path.join(repo_path, f\"Bch_Itr_{iteration}.pkl\")\n",
    "                df.to_csv(csv_path, index=True)\n",
    "                df.to_pickle(pkl_path)\n",
    "\n",
    "            def store_xy_data(X, Y, repo_path, iteration):\n",
    "                \"\"\"Helper to store X_train_re/Y_train_re in PKL format.\"\"\"\n",
    "                X_path = os.path.join(repo_path, f\"X_train_re_itr_{iteration}.pkl\")\n",
    "                Y_path = os.path.join(repo_path, f\"Y_train_re_itr_{iteration}.pkl\")\n",
    "                X.to_pickle(X_path)\n",
    "                Y.to_pickle(Y_path)\n",
    "\n",
    "            # --- Your main code ---\n",
    "            iteration_repo = f\"D:/Ensemble_Multi_Objective/MultiObj_Ensemble_C12C7_CR_A/A_Run_{run}/Iteration_{iteration}/\"\n",
    "            no_improve_repo = os.path.join(iteration_repo, \"Itr_No_Improve\")\n",
    "\n",
    "            os.makedirs(iteration_repo, exist_ok=True)  # Always need iteration_repo\n",
    "\n",
    "            # Compute the metrics just once\n",
    "            diff_recall_mean = (\n",
    "                classification_df.loc[topic_dict[\"T12\"], 'Diff recall']\n",
    "                + classification_df.loc[topic_dict[\"T7\"], 'Diff recall']\n",
    "            ) / 2\n",
    "\n",
    "            diff_bal_acc_mean = (\n",
    "                classification_df.loc[topic_dict[\"T12\"], 'Diff Balanced Accuracy']\n",
    "                + classification_df.loc[topic_dict[\"T7\"], 'Diff Balanced Accuracy']\n",
    "            ) / 2\n",
    "\n",
    "            diff_f1_accuracy = classification_df.loc[\"accuracy\", 'Diff f1-score']\n",
    "\n",
    "            # Decide if \"improved\" or \"no improvement\"\n",
    "            #  -- Condition from your code:\n",
    "            #     A) diff_recall_mean > 0\n",
    "            #     B) diff_recall_mean == 0 and (diff_bal_acc_mean > 0 or diff_f1_accuracy > 0)\n",
    "            #     Otherwise, no improvement\n",
    "            improved = False\n",
    "            if diff_recall_mean > 0:\n",
    "                improved = True\n",
    "            elif diff_recall_mean == 0:\n",
    "                if (diff_bal_acc_mean > 0) or (diff_f1_accuracy > 0):\n",
    "                    improved = True\n",
    "\n",
    "            if improved:\n",
    "                # Same saving logic for improved scenario\n",
    "                store_classification_df(classification_df, iteration_repo, iteration)\n",
    "                store_xy_data(X_train_re, Y_train_re, iteration_repo, iteration)\n",
    "            else:\n",
    "                # No improvement scenario\n",
    "                os.makedirs(no_improve_repo, exist_ok=True)\n",
    "                # Save classification_df to the no-improve repo\n",
    "                store_classification_df(classification_df, no_improve_repo, iteration)\n",
    "                store_xy_data(X_train_re, Y_train_re, no_improve_repo, iteration)\n",
    "\n",
    "                # Also save bch_class_df and previous iteration's data to iteration_repo\n",
    "                store_classification_df(bch_class_df, iteration_repo, iteration)\n",
    "                store_xy_data(prev_itr_X_train_re, prev_itr_Y_train_re, iteration_repo, iteration)\n",
    "            # with open(f'{nsgaii_results_path}/{history_dict_name}.pkl', 'wb') as file:\n",
    "            #     pickle.dump(history_IndexesList_dict, file)\n",
    "\n",
    "            # with open(f'{nsgaii_results_path}/{final_pop_name}.pkl', 'wb') as file:\n",
    "            #     pickle.dump(final_pop, file)\n",
    "\n",
    "            # with open(f'{nsgaii_results_path}/{history_pareto_lists_name}.pkl', 'wb') as file:\n",
    "            #     pickle.dump(history_pareto_selections_list, file)\n",
    "            \n",
    "            # gen_stats_df = all_pareto_observer(history_pareto_selections_list)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
