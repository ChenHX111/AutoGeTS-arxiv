{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_PCA_YZ_df = pd.read_pickle(\"D:/Oxford/Second Year/AutoGeTS/Data/Train_PCA_YZ_withPred_0.pkl\")\n",
    "\n",
    "# topic_dict = {\"T1\": \"IT support and assistance.\",\"T2\": \"Account activation and access issues.\",\"T3\": \"Password and device security.\",\n",
    "#                     \"T4\": \"Printer issues and troubleshooting.\",\"T5\": \"HP Dock connectivity issues.\",\"T6\": \"Employee documentation and errors.\",\n",
    "#                     \"T7\": \"\\\"Access and login issues\\\"\",\"T8\": \"Opening and managing files/devices.\",\"T9\": \"Mobile email and VPN setup.\",\n",
    "#                     \"T10\": \"IT support and communication.\",\"T11\": \"Error handling in RPG programming.\",\"T12\": \"Email security and attachments.\",\n",
    "#                     \"T13\": \"Humanitarian aid for Ukraine.\",\"T14\": \"Internet connectivity issues in offices.\",\"T15\": \"Improving integration with Infojobs.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TREC-6'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"TREC-6\"\"\"\n",
    "# train_PCA_YZ_df = pd.read_pickle(\"D:/Oxford/Second Year/AutoGeTS_Revisions/Additional_Datasets/TREC-6/Data/Train_PCA_YZ_withPred_0.pkl\")\n",
    "\n",
    "# topic_dict = {'ABBR': 'ABBR', 'DESC': 'DESC', 'ENTY': 'ENTY', 'HUM': 'HUM', 'LOC': 'LOC', 'NUM': 'NUM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Amazon Review\"\"\"\n",
    "train_PCA_YZ_df = pd.read_pickle(\"D:/Oxford/Second Year/AutoGeTS_Revisions/Additional_Datasets/Amazon_Review/Data/Train_PCA_YZ_withPred_0.pkl\")\n",
    "\n",
    "topic_dict = {'R1': '1', 'R2': '2', 'R3': '3', 'R4': '4', 'R5': '5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"For Inetum\"\"\"\n",
    "# # Add this at the beginning of the script\n",
    "# class Config:\n",
    "#     \"\"\"Configuration class for save directories\"\"\"\n",
    "#     def __init__(self, base_dir=\"synthetic_data\"):\n",
    "#         self.base_dir = base_dir\n",
    "#         self.eda_dir = os.path.join(base_dir, \"EDA\")\n",
    "#         self.bt_dir = os.path.join(base_dir, \"Back-translation\")\n",
    "\n",
    "# def setup_folders(config):\n",
    "#     \"\"\"Create necessary folders for storing results\"\"\"\n",
    "#     os.makedirs(config.base_dir, exist_ok=True)\n",
    "#     os.makedirs(config.eda_dir, exist_ok=True)\n",
    "#     os.makedirs(config.bt_dir, exist_ok=True)\n",
    "\n",
    "# class TopicDataFrame:\n",
    "#     \"\"\"Class to handle topic dataframes with incremental saves in both CSV and PKL formats\"\"\"\n",
    "#     def __init__(self, topic_num, save_dir):\n",
    "#         self.topic_num = topic_num\n",
    "#         self.save_dir = save_dir\n",
    "#         self.csv_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.csv\")\n",
    "#         self.pkl_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.pkl\")\n",
    "#         self.columns = ['index_meta', 'topic_name', 'sample', 'area_TEIS', 'text']\n",
    "        \n",
    "#         # Load existing data if any, otherwise create new DataFrame\n",
    "#         if os.path.exists(self.pkl_filename):\n",
    "#             with open(self.pkl_filename, 'rb') as f:\n",
    "#                 self.df = pickle.load(f)\n",
    "#         elif os.path.exists(self.csv_filename):\n",
    "#             self.df = pd.read_csv(self.csv_filename)\n",
    "#         else:\n",
    "#             self.df = pd.DataFrame(columns=self.columns)\n",
    "    \n",
    "#     def append_and_save(self, new_row):\n",
    "#         \"\"\"Append a new row and save immediately in both formats\"\"\"\n",
    "#         # Convert new_row dictionary to DataFrame\n",
    "#         new_row_df = pd.DataFrame([new_row])\n",
    "        \n",
    "#         # Concatenate with existing DataFrame\n",
    "#         self.df = pd.concat([self.df, new_row_df], ignore_index=True)\n",
    "        \n",
    "#         # Save as CSV\n",
    "#         self.df.to_csv(self.csv_filename, index=False)\n",
    "        \n",
    "#         # Save as PKL\n",
    "#         with open(self.pkl_filename, 'wb') as f:\n",
    "#             pickle.dump(self.df, f)\n",
    "\n",
    "# def initialize_topic_dfs(topic_dict, save_dir):\n",
    "#     \"\"\"Initialize TopicDataFrame objects for each topic\"\"\"\n",
    "#     topic_dfs = {}\n",
    "#     for topic_num in topic_dict.keys():\n",
    "#         topic_dfs[topic_num] = TopicDataFrame(topic_num, save_dir)\n",
    "#     return topic_dfs\n",
    "\n",
    "# def setup_language_models():\n",
    "#     \"\"\"No need to setup models with deep-translator\"\"\"\n",
    "#     return None\n",
    "\n",
    "# # def eda_augment(text, num_aug=5):\n",
    "# #     \"\"\"Perform EDA augmentation for Spanish/Portuguese text\"\"\"\n",
    "# #     # Initialize augmenter with Spanish WordNet\n",
    "# #     aug = SynonymAug(aug_src='wordnet', lang='spa')  # Use 'por' for Portuguese\n",
    "    \n",
    "# #     try:\n",
    "# #         augmented_texts = aug.augment(text, n=num_aug)\n",
    "# #         return augmented_texts\n",
    "# #     except:\n",
    "# #         # Fallback: if augmentation fails, return slight modifications of original text\n",
    "# #         return [text + \" \" + suffix for suffix in [\n",
    "# #             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "# #         ]]\n",
    "\n",
    "# # def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5):\n",
    "# #     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "# #     augmented_texts = []\n",
    "    \n",
    "# #     try:\n",
    "# #         # Translate to English\n",
    "# #         en_text = model_to_en.generate(\n",
    "# #             **tokenizer_to_en(text, return_tensors=\"pt\", padding=True),\n",
    "# #             num_beams=5,\n",
    "# #             num_return_sequences=num_aug,\n",
    "# #             max_length=256\n",
    "# #         )\n",
    "        \n",
    "# #         # Translate back to Spanish/Portuguese\n",
    "# #         for en in en_text:\n",
    "# #             es_text = model_to_es.generate(\n",
    "# #                 **tokenizer_to_es(tokenizer_to_en.decode(en, skip_special_tokens=True), \n",
    "# #                                 return_tensors=\"pt\", padding=True),\n",
    "# #                 num_beams=1,\n",
    "# #                 max_length=256\n",
    "# #             )\n",
    "# #             augmented_texts.append(tokenizer_to_es.decode(es_text[0], skip_special_tokens=True))\n",
    "            \n",
    "# #         return augmented_texts\n",
    "# #     except:\n",
    "# #         # Fallback: if translation fails, return slight modifications\n",
    "# #         return [text + \" \" + suffix for suffix in [\n",
    "# #             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "# #         ]]\n",
    "\n",
    "# def eda_augment(text, num_aug=5, alpha=0.1, seed=42):\n",
    "#     \"\"\"\n",
    "#     Perform Easy Data Augmentation for Spanish/Portuguese text with robust error handling\n",
    "#     \"\"\"\n",
    "#     # Set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     # Add input validation\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return [text] * num_aug\n",
    "    \n",
    "#     def get_synonyms(word, lang='spa'):  # Use 'por' for Portuguese\n",
    "#         try:\n",
    "#             aug = SynonymAug(aug_src='wordnet', lang=lang)\n",
    "#             synonyms = aug.augment(word)[0]\n",
    "#             return synonyms if isinstance(synonyms, list) else [synonyms]\n",
    "#         except:\n",
    "#             return [word]\n",
    "    \n",
    "#     def synonym_replacement(words, n):\n",
    "#         if not words:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         random_word_list = list(set([word for word in words if word.isalnum()]))\n",
    "#         if not random_word_list:  # If no alphanumeric words found\n",
    "#             return words\n",
    "#         n = min(n, len(random_word_list))\n",
    "#         random_idx = random.sample(range(len(random_word_list)), n)\n",
    "        \n",
    "#         for idx in random_idx:\n",
    "#             word = random_word_list[idx]\n",
    "#             synonyms = get_synonyms(word)\n",
    "#             if synonyms:\n",
    "#                 random_syn = random.choice(synonyms)\n",
    "#                 new_words = [random_syn if w == word else w for w in new_words]\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_insertion(words, n):\n",
    "#         if not words:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         # Get list of alphanumeric words\n",
    "#         alnum_words = [w for w in words if w.isalnum()]\n",
    "#         if not alnum_words:  # If no alphanumeric words found\n",
    "#             return words\n",
    "            \n",
    "#         for _ in range(n):\n",
    "#             try:\n",
    "#                 random_word = random.choice(alnum_words)\n",
    "#                 synonyms = get_synonyms(random_word)\n",
    "#                 if synonyms:\n",
    "#                     random_syn = random.choice(synonyms)\n",
    "#                     random_idx = random.randint(0, len(new_words))\n",
    "#                     new_words.insert(random_idx, random_syn)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: random_insertion failed with error {str(e)}\")\n",
    "#                 continue\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_swap(words, n):\n",
    "#         if len(words) < 2:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         for _ in range(n):\n",
    "#             try:\n",
    "#                 idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "#                 new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: random_swap failed with error {str(e)}\")\n",
    "#                 continue\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_deletion(words, p):\n",
    "#         if len(words) <= 1:\n",
    "#             return words\n",
    "#         try:\n",
    "#             new_words = [word for word in words if random.random() > p or not word.isalnum()]\n",
    "#             if not new_words:\n",
    "#                 return words\n",
    "#             return new_words\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: random_deletion failed with error {str(e)}\")\n",
    "#             return words\n",
    "\n",
    "#     # Split text into words and handle empty input\n",
    "#     words = text.strip().split()\n",
    "#     if not words:\n",
    "#         return [text] * num_aug\n",
    "    \n",
    "#     num_words = len(words)\n",
    "#     augmented_texts = []\n",
    "    \n",
    "#     # Calculate number of words to modify for each operation\n",
    "#     n_sr = max(1, int(alpha * num_words))\n",
    "#     n_ri = max(1, int(alpha * num_words))\n",
    "#     n_rs = max(1, int(alpha * num_words))\n",
    "    \n",
    "#     operations = ['sr', 'ri', 'rs', 'rd']\n",
    "#     fallback_suffixes = [\n",
    "#         \" por favor\", \" gracias\", \" si es posible\", \n",
    "#         \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "#         \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "#     ]\n",
    "    \n",
    "#     for _ in range(num_aug):\n",
    "#         try:\n",
    "#             operation = random.choice(operations)\n",
    "            \n",
    "#             if operation == 'sr':\n",
    "#                 new_words = synonym_replacement(words, n_sr)\n",
    "#             elif operation == 'ri':\n",
    "#                 new_words = random_insertion(words, n_ri)\n",
    "#             elif operation == 'rs':\n",
    "#                 new_words = random_swap(words, n_rs)\n",
    "#             else:  # rd\n",
    "#                 new_words = random_deletion(words, alpha)\n",
    "            \n",
    "#             new_text = ' '.join(new_words)\n",
    "#             if new_text.strip():  # Only add if not empty\n",
    "#                 augmented_texts.append(new_text)\n",
    "#             else:\n",
    "#                 augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: augmentation failed with error {str(e)}\")\n",
    "#             augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     # Ensure we have enough augmentations\n",
    "#     while len(augmented_texts) < num_aug:\n",
    "#         augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     return augmented_texts[:num_aug]  # Ensure we return exactly num_aug samples\n",
    "\n",
    "# # def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5, seed=42):\n",
    "# #     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "# #     # Set seed\n",
    "# #     random.seed(seed)\n",
    "# #     np.random.seed(seed)\n",
    "# #     torch.manual_seed(seed)\n",
    "\n",
    "# #     augmented_texts = []\n",
    "    \n",
    "# #     try:\n",
    "# #         # Translate to English\n",
    "# #         inputs = tokenizer_to_en(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "# #         en_texts = model_to_en.generate(\n",
    "# #             **inputs,\n",
    "# #             num_beams=5,\n",
    "# #             num_return_sequences=num_aug,\n",
    "# #             max_length=512,\n",
    "# #             temperature=0.8\n",
    "# #         )\n",
    "        \n",
    "# #         # Translate back to Spanish/Portuguese\n",
    "# #         for en_text in en_texts:\n",
    "# #             decoded_en = tokenizer_to_en.decode(en_text, skip_special_tokens=True)\n",
    "# #             inputs_es = tokenizer_to_es(decoded_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "# #             es_text = model_to_es.generate(\n",
    "# #                 **inputs_es,\n",
    "# #                 num_beams=3,\n",
    "# #                 max_length=512,\n",
    "# #                 temperature=0.8\n",
    "# #             )\n",
    "# #             decoded_es = tokenizer_to_es.decode(es_text[0], skip_special_tokens=True)\n",
    "# #             augmented_texts.append(decoded_es)\n",
    "            \n",
    "# #         return augmented_texts\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Back-translation error: {str(e)}\")\n",
    "# #         # Fallback: return variations with added suffixes\n",
    "# #         fallback_texts = []\n",
    "# #         suffixes = [\" por favor\", \" gracias\", \" si es posible\", \" urgente\", \" cuando pueda\"]\n",
    "# #         for suffix in suffixes[:num_aug]:\n",
    "# #             fallback_texts.append(text + suffix)\n",
    "# #         return fallback_texts\n",
    "\n",
    "# def back_translation_augment(text, num_aug=5, seed=42):\n",
    "#     \"\"\"\n",
    "#     Perform back-translation augmentation using Google Translate\n",
    "#     Uses different intermediate languages to create variations\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "    \n",
    "#     # List of intermediate languages to create variations\n",
    "#     intermediate_languages = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'gl']\n",
    "#     source_lang = 'es'  # assuming Spanish is source language\n",
    "    \n",
    "#     augmented_texts = []\n",
    "#     used_langs = random.sample(intermediate_languages, min(num_aug, len(intermediate_languages)))\n",
    "    \n",
    "#     for lang in used_langs:\n",
    "#         try:\n",
    "#             # Translate to intermediate language\n",
    "#             translator_to = GoogleTranslator(source=source_lang, target=lang)\n",
    "#             intermediate_text = translator_to.translate(text)\n",
    "            \n",
    "#             # Add small delay to avoid rate limiting\n",
    "#             time.sleep(0.05)\n",
    "            \n",
    "#             # Translate back to Spanish\n",
    "#             translator_back = GoogleTranslator(source=lang, target=source_lang)\n",
    "#             back_translated = translator_back.translate(intermediate_text)\n",
    "            \n",
    "#             # Add small delay to avoid rate limiting\n",
    "#             time.sleep(0.05)\n",
    "            \n",
    "#             if back_translated and back_translated.strip():\n",
    "#                 augmented_texts.append(back_translated)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Translation error with language {lang}: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     # If we couldn't generate enough augmentations, add variations of the original\n",
    "#     fallback_suffixes = [\n",
    "#         \" por favor\", \" gracias\", \" si es posible\", \n",
    "#         \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "#         \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "#     ]\n",
    "    \n",
    "#     while len(augmented_texts) < num_aug:\n",
    "#         augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     return augmented_texts[:num_aug]\n",
    "\n",
    "# # def process_dataframe(df, topic_dict, method='EDA', config=None):\n",
    "# #     \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "# #     # Setup\n",
    "# #     save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "# #     topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "# #     if method == 'Back-translation':\n",
    "# #         model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es = setup_language_models()\n",
    "    \n",
    "# #     # Process each row\n",
    "# #     for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "# #         topic_name = row['topic_name']\n",
    "# #         topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "        \n",
    "# #         # Generate augmented texts\n",
    "# #         if method == 'EDA':\n",
    "# #             augmented_texts = eda_augment(row['text'])\n",
    "# #         else:  # Back-translation\n",
    "# #             augmented_texts = back_translation_augment(\n",
    "# #                 row['text'], model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es\n",
    "# #             )\n",
    "        \n",
    "# #         # Create and save new rows immediately\n",
    "# #         for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "# #             new_row = {\n",
    "# #                 'index_meta': row['index_meta'],\n",
    "# #                 'topic_name': row['topic_name'],\n",
    "# #                 'sample': sample_num,\n",
    "# #                 'area_TEIS': row['area_TEIS'],\n",
    "# #                 'text': aug_text\n",
    "# #             }\n",
    "# #             topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "# #     return topic_dfs\n",
    "\n",
    "# def process_dataframe(df, topic_dict, method='EDA', config=None, seed=42):\n",
    "#     \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "#     # Set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "#     # Setup\n",
    "#     save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "#     topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "#     # Process each row\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "#         # Set a different seed for each row\n",
    "#         row_seed = seed + idx\n",
    "#         random.seed(row_seed)\n",
    "        \n",
    "#         topic_name = row['topic_name']\n",
    "#         topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "\n",
    "#         # Check if this index_meta already exists in the synthetic df\n",
    "#         current_df = topic_dfs[topic_num].df\n",
    "#         if row['index_meta'] in current_df['index_meta'].values:\n",
    "#             print(f\"Skipping {method} augmentation for index_meta {row['index_meta']} as it already exists\")\n",
    "#             continue\n",
    "        \n",
    "#         # Generate augmented texts\n",
    "#         if method == 'EDA':\n",
    "#             augmented_texts = eda_augment(row['text'], seed=row_seed)\n",
    "#         else:  # Back-translation\n",
    "#             augmented_texts = back_translation_augment(row['text'], seed=row_seed)\n",
    "        \n",
    "#         # Create and save new rows immediately\n",
    "#         for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "#             new_row = {\n",
    "#                 'index_meta': row['index_meta'],\n",
    "#                 'topic_name': row['topic_name'],\n",
    "#                 'sample': sample_num,\n",
    "#                 'area_TEIS': row['area_TEIS'],\n",
    "#                 'text': aug_text\n",
    "#             }\n",
    "#             topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "#     return topic_dfs\n",
    "\n",
    "# def get_key_by_value(dictionary, search_value):\n",
    "#     for key, value in dictionary.items():\n",
    "#         if value == search_value:\n",
    "#             return key\n",
    "#     return None  # Return None if value not found\n",
    "\n",
    "# def main(train_df, topic_dict, base_save_dir=\"synthetic_data\", seed=42):\n",
    "#     \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "#     # Set initial seed\n",
    "#     set_global_seeds(seed)\n",
    "\n",
    "#     # Initialize configuration\n",
    "#     config = Config(base_save_dir)\n",
    "    \n",
    "#     # Create folders\n",
    "#     setup_folders(config)\n",
    "    \n",
    "#     # Process with EDA\n",
    "#     print(\"Processing with EDA...\")\n",
    "#     eda_topic_dfs = process_dataframe(train_df, topic_dict, method='EDA', config=config, seed=seed)\n",
    "    \n",
    "#     # Process with Back-translation\n",
    "#     print(\"Processing with Back-translation...\")\n",
    "#     bt_topic_dfs = process_dataframe(train_df, topic_dict, method='Back-translation', config=config, seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"For TREC-6\"\"\"\n",
    "# # Add this at the beginning of the script\n",
    "# class Config:\n",
    "#     \"\"\"Configuration class for save directories\"\"\"\n",
    "#     def __init__(self, base_dir=\"synthetic_data\"):\n",
    "#         self.base_dir = base_dir\n",
    "#         self.eda_dir = os.path.join(base_dir, \"EDA\")\n",
    "#         self.bt_dir = os.path.join(base_dir, \"Back-translation\")\n",
    "\n",
    "# def setup_folders(config):\n",
    "#     \"\"\"Create necessary folders for storing results\"\"\"\n",
    "#     os.makedirs(config.base_dir, exist_ok=True)\n",
    "#     os.makedirs(config.eda_dir, exist_ok=True)\n",
    "#     os.makedirs(config.bt_dir, exist_ok=True)\n",
    "\n",
    "# class TopicDataFrame:\n",
    "#     \"\"\"Class to handle topic dataframes with incremental saves in both CSV and PKL formats\"\"\"\n",
    "#     def __init__(self, topic_num, save_dir):\n",
    "#         self.topic_num = topic_num\n",
    "#         self.save_dir = save_dir\n",
    "#         self.csv_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.csv\")\n",
    "#         self.pkl_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.pkl\")\n",
    "#         self.columns = ['index_meta', 'TREC-6_Label', 'sample', 'text']\n",
    "        \n",
    "#         # Load existing data if any, otherwise create new DataFrame\n",
    "#         if os.path.exists(self.pkl_filename):\n",
    "#             with open(self.pkl_filename, 'rb') as f:\n",
    "#                 self.df = pickle.load(f)\n",
    "#         elif os.path.exists(self.csv_filename):\n",
    "#             self.df = pd.read_csv(self.csv_filename)\n",
    "#         else:\n",
    "#             self.df = pd.DataFrame(columns=self.columns)\n",
    "    \n",
    "#     def append_and_save(self, new_row):\n",
    "#         \"\"\"Append a new row and save immediately in both formats\"\"\"\n",
    "#         # Convert new_row dictionary to DataFrame\n",
    "#         new_row_df = pd.DataFrame([new_row])\n",
    "        \n",
    "#         # Concatenate with existing DataFrame\n",
    "#         self.df = pd.concat([self.df, new_row_df], ignore_index=True)\n",
    "        \n",
    "#         # Save as CSV\n",
    "#         self.df.to_csv(self.csv_filename, index=False)\n",
    "        \n",
    "#         # Save as PKL\n",
    "#         with open(self.pkl_filename, 'wb') as f:\n",
    "#             pickle.dump(self.df, f)\n",
    "\n",
    "# def initialize_topic_dfs(topic_dict, save_dir):\n",
    "#     \"\"\"Initialize TopicDataFrame objects for each topic\"\"\"\n",
    "#     topic_dfs = {}\n",
    "#     for topic_num in topic_dict.keys():\n",
    "#         topic_dfs[topic_num] = TopicDataFrame(topic_num, save_dir)\n",
    "#     return topic_dfs\n",
    "\n",
    "# def setup_language_models():\n",
    "#     \"\"\"No need to setup models with deep-translator\"\"\"\n",
    "#     return None\n",
    "\n",
    "# # def eda_augment(text, num_aug=5):\n",
    "# #     \"\"\"Perform EDA augmentation for Spanish/Portuguese text\"\"\"\n",
    "# #     # Initialize augmenter with Spanish WordNet\n",
    "# #     aug = SynonymAug(aug_src='wordnet', lang='spa')  # Use 'por' for Portuguese\n",
    "    \n",
    "# #     try:\n",
    "# #         augmented_texts = aug.augment(text, n=num_aug)\n",
    "# #         return augmented_texts\n",
    "# #     except:\n",
    "# #         # Fallback: if augmentation fails, return slight modifications of original text\n",
    "# #         return [text + \" \" + suffix for suffix in [\n",
    "# #             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "# #         ]]\n",
    "\n",
    "# # def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5):\n",
    "# #     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "# #     augmented_texts = []\n",
    "    \n",
    "# #     try:\n",
    "# #         # Translate to English\n",
    "# #         en_text = model_to_en.generate(\n",
    "# #             **tokenizer_to_en(text, return_tensors=\"pt\", padding=True),\n",
    "# #             num_beams=5,\n",
    "# #             num_return_sequences=num_aug,\n",
    "# #             max_length=256\n",
    "# #         )\n",
    "        \n",
    "# #         # Translate back to Spanish/Portuguese\n",
    "# #         for en in en_text:\n",
    "# #             es_text = model_to_es.generate(\n",
    "# #                 **tokenizer_to_es(tokenizer_to_en.decode(en, skip_special_tokens=True), \n",
    "# #                                 return_tensors=\"pt\", padding=True),\n",
    "# #                 num_beams=1,\n",
    "# #                 max_length=256\n",
    "# #             )\n",
    "# #             augmented_texts.append(tokenizer_to_es.decode(es_text[0], skip_special_tokens=True))\n",
    "            \n",
    "# #         return augmented_texts\n",
    "# #     except:\n",
    "# #         # Fallback: if translation fails, return slight modifications\n",
    "# #         return [text + \" \" + suffix for suffix in [\n",
    "# #             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "# #         ]]\n",
    "\n",
    "# def eda_augment(text, num_aug=5, alpha=0.1, seed=42):\n",
    "#     \"\"\"\n",
    "#     Perform Easy Data Augmentation for Spanish/Portuguese text with robust error handling\n",
    "#     \"\"\"\n",
    "#     # Set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     # Add input validation\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return [text] * num_aug\n",
    "    \n",
    "#     def get_synonyms(word, lang='spa'):  # Use 'por' for Portuguese\n",
    "#         try:\n",
    "#             aug = SynonymAug(aug_src='wordnet', lang=lang)\n",
    "#             synonyms = aug.augment(word)[0]\n",
    "#             return synonyms if isinstance(synonyms, list) else [synonyms]\n",
    "#         except:\n",
    "#             return [word]\n",
    "    \n",
    "#     def synonym_replacement(words, n):\n",
    "#         if not words:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         random_word_list = list(set([word for word in words if word.isalnum()]))\n",
    "#         if not random_word_list:  # If no alphanumeric words found\n",
    "#             return words\n",
    "#         n = min(n, len(random_word_list))\n",
    "#         random_idx = random.sample(range(len(random_word_list)), n)\n",
    "        \n",
    "#         for idx in random_idx:\n",
    "#             word = random_word_list[idx]\n",
    "#             synonyms = get_synonyms(word)\n",
    "#             if synonyms:\n",
    "#                 random_syn = random.choice(synonyms)\n",
    "#                 new_words = [random_syn if w == word else w for w in new_words]\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_insertion(words, n):\n",
    "#         if not words:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         # Get list of alphanumeric words\n",
    "#         alnum_words = [w for w in words if w.isalnum()]\n",
    "#         if not alnum_words:  # If no alphanumeric words found\n",
    "#             return words\n",
    "            \n",
    "#         for _ in range(n):\n",
    "#             try:\n",
    "#                 random_word = random.choice(alnum_words)\n",
    "#                 synonyms = get_synonyms(random_word)\n",
    "#                 if synonyms:\n",
    "#                     random_syn = random.choice(synonyms)\n",
    "#                     random_idx = random.randint(0, len(new_words))\n",
    "#                     new_words.insert(random_idx, random_syn)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: random_insertion failed with error {str(e)}\")\n",
    "#                 continue\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_swap(words, n):\n",
    "#         if len(words) < 2:\n",
    "#             return words\n",
    "#         new_words = words.copy()\n",
    "#         for _ in range(n):\n",
    "#             try:\n",
    "#                 idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "#                 new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: random_swap failed with error {str(e)}\")\n",
    "#                 continue\n",
    "#         return new_words\n",
    "    \n",
    "#     def random_deletion(words, p):\n",
    "#         if len(words) <= 1:\n",
    "#             return words\n",
    "#         try:\n",
    "#             new_words = [word for word in words if random.random() > p or not word.isalnum()]\n",
    "#             if not new_words:\n",
    "#                 return words\n",
    "#             return new_words\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: random_deletion failed with error {str(e)}\")\n",
    "#             return words\n",
    "\n",
    "#     # Split text into words and handle empty input\n",
    "#     words = text.strip().split()\n",
    "#     if not words:\n",
    "#         return [text] * num_aug\n",
    "    \n",
    "#     num_words = len(words)\n",
    "#     augmented_texts = []\n",
    "    \n",
    "#     # Calculate number of words to modify for each operation\n",
    "#     n_sr = max(1, int(alpha * num_words))\n",
    "#     n_ri = max(1, int(alpha * num_words))\n",
    "#     n_rs = max(1, int(alpha * num_words))\n",
    "    \n",
    "#     operations = ['sr', 'ri', 'rs', 'rd']\n",
    "#     fallback_suffixes = [\n",
    "#         \" por favor\", \" gracias\", \" si es posible\", \n",
    "#         \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "#         \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "#     ]\n",
    "    \n",
    "#     for _ in range(num_aug):\n",
    "#         try:\n",
    "#             operation = random.choice(operations)\n",
    "            \n",
    "#             if operation == 'sr':\n",
    "#                 new_words = synonym_replacement(words, n_sr)\n",
    "#             elif operation == 'ri':\n",
    "#                 new_words = random_insertion(words, n_ri)\n",
    "#             elif operation == 'rs':\n",
    "#                 new_words = random_swap(words, n_rs)\n",
    "#             else:  # rd\n",
    "#                 new_words = random_deletion(words, alpha)\n",
    "            \n",
    "#             new_text = ' '.join(new_words)\n",
    "#             if new_text.strip():  # Only add if not empty\n",
    "#                 augmented_texts.append(new_text)\n",
    "#             else:\n",
    "#                 augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: augmentation failed with error {str(e)}\")\n",
    "#             augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     # Ensure we have enough augmentations\n",
    "#     while len(augmented_texts) < num_aug:\n",
    "#         augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     return augmented_texts[:num_aug]  # Ensure we return exactly num_aug samples\n",
    "\n",
    "# # def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5, seed=42):\n",
    "# #     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "# #     # Set seed\n",
    "# #     random.seed(seed)\n",
    "# #     np.random.seed(seed)\n",
    "# #     torch.manual_seed(seed)\n",
    "\n",
    "# #     augmented_texts = []\n",
    "    \n",
    "# #     try:\n",
    "# #         # Translate to English\n",
    "# #         inputs = tokenizer_to_en(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "# #         en_texts = model_to_en.generate(\n",
    "# #             **inputs,\n",
    "# #             num_beams=5,\n",
    "# #             num_return_sequences=num_aug,\n",
    "# #             max_length=512,\n",
    "# #             temperature=0.8\n",
    "# #         )\n",
    "        \n",
    "# #         # Translate back to Spanish/Portuguese\n",
    "# #         for en_text in en_texts:\n",
    "# #             decoded_en = tokenizer_to_en.decode(en_text, skip_special_tokens=True)\n",
    "# #             inputs_es = tokenizer_to_es(decoded_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "# #             es_text = model_to_es.generate(\n",
    "# #                 **inputs_es,\n",
    "# #                 num_beams=3,\n",
    "# #                 max_length=512,\n",
    "# #                 temperature=0.8\n",
    "# #             )\n",
    "# #             decoded_es = tokenizer_to_es.decode(es_text[0], skip_special_tokens=True)\n",
    "# #             augmented_texts.append(decoded_es)\n",
    "            \n",
    "# #         return augmented_texts\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Back-translation error: {str(e)}\")\n",
    "# #         # Fallback: return variations with added suffixes\n",
    "# #         fallback_texts = []\n",
    "# #         suffixes = [\" por favor\", \" gracias\", \" si es posible\", \" urgente\", \" cuando pueda\"]\n",
    "# #         for suffix in suffixes[:num_aug]:\n",
    "# #             fallback_texts.append(text + suffix)\n",
    "# #         return fallback_texts\n",
    "\n",
    "# def back_translation_augment(text, num_aug=5, seed=42):\n",
    "#     \"\"\"\n",
    "#     Perform back-translation augmentation using Google Translate\n",
    "#     Uses different intermediate languages to create variations\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "    \n",
    "#     # List of intermediate languages to create variations\n",
    "#     intermediate_languages = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'gl']\n",
    "#     source_lang = 'es'  # assuming Spanish is source language\n",
    "    \n",
    "#     augmented_texts = []\n",
    "#     used_langs = random.sample(intermediate_languages, min(num_aug, len(intermediate_languages)))\n",
    "    \n",
    "#     for lang in used_langs:\n",
    "#         try:\n",
    "#             # Translate to intermediate language\n",
    "#             translator_to = GoogleTranslator(source=source_lang, target=lang)\n",
    "#             intermediate_text = translator_to.translate(text)\n",
    "            \n",
    "#             # Add small delay to avoid rate limiting\n",
    "#             time.sleep(0.05)\n",
    "            \n",
    "#             # Translate back to Spanish\n",
    "#             translator_back = GoogleTranslator(source=lang, target=source_lang)\n",
    "#             back_translated = translator_back.translate(intermediate_text)\n",
    "            \n",
    "#             # Add small delay to avoid rate limiting\n",
    "#             time.sleep(0.05)\n",
    "            \n",
    "#             if back_translated and back_translated.strip():\n",
    "#                 augmented_texts.append(back_translated)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Translation error with language {lang}: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     # If we couldn't generate enough augmentations, add variations of the original\n",
    "#     fallback_suffixes = [\n",
    "#         \" por favor\", \" gracias\", \" si es posible\", \n",
    "#         \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "#         \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "#     ]\n",
    "    \n",
    "#     while len(augmented_texts) < num_aug:\n",
    "#         augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "#     return augmented_texts[:num_aug]\n",
    "\n",
    "# # def process_dataframe(df, topic_dict, method='EDA', config=None):\n",
    "# #     \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "# #     # Setup\n",
    "# #     save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "# #     topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "# #     if method == 'Back-translation':\n",
    "# #         model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es = setup_language_models()\n",
    "    \n",
    "# #     # Process each row\n",
    "# #     for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "# #         topic_name = row['topic_name']\n",
    "# #         topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "        \n",
    "# #         # Generate augmented texts\n",
    "# #         if method == 'EDA':\n",
    "# #             augmented_texts = eda_augment(row['text'])\n",
    "# #         else:  # Back-translation\n",
    "# #             augmented_texts = back_translation_augment(\n",
    "# #                 row['text'], model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es\n",
    "# #             )\n",
    "        \n",
    "# #         # Create and save new rows immediately\n",
    "# #         for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "# #             new_row = {\n",
    "# #                 'index_meta': row['index_meta'],\n",
    "# #                 'topic_name': row['topic_name'],\n",
    "# #                 'sample': sample_num,\n",
    "# #                 'area_TEIS': row['area_TEIS'],\n",
    "# #                 'text': aug_text\n",
    "# #             }\n",
    "# #             topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "# #     return topic_dfs\n",
    "\n",
    "# def process_dataframe(df, topic_dict, method='EDA', config=None, seed=42):\n",
    "#     \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "#     # Set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "#     # Setup\n",
    "#     save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "#     topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "#     # Process each row\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "#         # Set a different seed for each row\n",
    "#         row_seed = seed + idx\n",
    "#         random.seed(row_seed)\n",
    "        \n",
    "#         topic_name = row['TREC-6_Label']\n",
    "#         topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "\n",
    "#         # Check if this index_meta already exists in the synthetic df\n",
    "#         current_df = topic_dfs[topic_num].df\n",
    "#         if row['index_meta'] in current_df['index_meta'].values:\n",
    "#             print(f\"Skipping {method} augmentation for index_meta {row['index_meta']} as it already exists\")\n",
    "#             continue\n",
    "        \n",
    "#         # Generate augmented texts\n",
    "#         if method == 'EDA':\n",
    "#             augmented_texts = eda_augment(row['text'], seed=row_seed)\n",
    "#         else:  # Back-translation\n",
    "#             augmented_texts = back_translation_augment(row['text'], seed=row_seed)\n",
    "        \n",
    "#         # Create and save new rows immediately\n",
    "#         for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "#             new_row = {\n",
    "#                 'index_meta': row['index_meta'],\n",
    "#                 'TREC-6_Label': row['TREC-6_Label'],\n",
    "#                 'sample': sample_num,\n",
    "#                 # 'area_TEIS': row['area_TEIS'],\n",
    "#                 'text': aug_text\n",
    "#             }\n",
    "#             topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "#     return topic_dfs\n",
    "\n",
    "# def get_key_by_value(dictionary, search_value):\n",
    "#     for key, value in dictionary.items():\n",
    "#         if value == search_value:\n",
    "#             return key\n",
    "#     return None  # Return None if value not found\n",
    "\n",
    "# def main(train_df, topic_dict, base_save_dir=\"synthetic_data\", seed=42):\n",
    "#     \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "#     # Set initial seed\n",
    "#     set_global_seeds(seed)\n",
    "\n",
    "#     # Initialize configuration\n",
    "#     config = Config(base_save_dir)\n",
    "    \n",
    "#     # Create folders\n",
    "#     setup_folders(config)\n",
    "    \n",
    "#     # Process with EDA\n",
    "#     print(\"Processing with EDA...\")\n",
    "#     eda_topic_dfs = process_dataframe(train_df, topic_dict, method='EDA', config=config, seed=seed)\n",
    "    \n",
    "#     # Process with Back-translation\n",
    "#     print(\"Processing with Back-translation...\")\n",
    "#     bt_topic_dfs = process_dataframe(train_df, topic_dict, method='Back-translation', config=config, seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For Amazon Review\"\"\"\n",
    "# Add this at the beginning of the script\n",
    "class Config:\n",
    "    \"\"\"Configuration class for save directories\"\"\"\n",
    "    def __init__(self, base_dir=\"synthetic_data\"):\n",
    "        self.base_dir = base_dir\n",
    "        self.eda_dir = os.path.join(base_dir, \"EDA\")\n",
    "        self.bt_dir = os.path.join(base_dir, \"Back-translation\")\n",
    "\n",
    "def setup_folders(config):\n",
    "    \"\"\"Create necessary folders for storing results\"\"\"\n",
    "    os.makedirs(config.base_dir, exist_ok=True)\n",
    "    os.makedirs(config.eda_dir, exist_ok=True)\n",
    "    os.makedirs(config.bt_dir, exist_ok=True)\n",
    "\n",
    "class TopicDataFrame:\n",
    "    \"\"\"Class to handle topic dataframes with incremental saves in both CSV and PKL formats\"\"\"\n",
    "    def __init__(self, topic_num, save_dir):\n",
    "        self.topic_num = topic_num\n",
    "        self.save_dir = save_dir\n",
    "        self.csv_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.csv\")\n",
    "        self.pkl_filename = os.path.join(save_dir, f\"{topic_num}_synthetic.pkl\")\n",
    "        self.columns = ['index_meta', 'rating', 'sample', 'title', 'text']\n",
    "        \n",
    "        # Load existing data if any, otherwise create new DataFrame\n",
    "        if os.path.exists(self.pkl_filename):\n",
    "            with open(self.pkl_filename, 'rb') as f:\n",
    "                self.df = pickle.load(f)\n",
    "        elif os.path.exists(self.csv_filename):\n",
    "            self.df = pd.read_csv(self.csv_filename)\n",
    "        else:\n",
    "            self.df = pd.DataFrame(columns=self.columns)\n",
    "    \n",
    "    def append_and_save(self, new_row):\n",
    "        \"\"\"Append a new row and save immediately in both formats\"\"\"\n",
    "        # Convert new_row dictionary to DataFrame\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        \n",
    "        # Concatenate with existing DataFrame\n",
    "        self.df = pd.concat([self.df, new_row_df], ignore_index=True)\n",
    "        \n",
    "        # Save as CSV\n",
    "        self.df.to_csv(self.csv_filename, index=False)\n",
    "        \n",
    "        # Save as PKL\n",
    "        with open(self.pkl_filename, 'wb') as f:\n",
    "            pickle.dump(self.df, f)\n",
    "\n",
    "def initialize_topic_dfs(topic_dict, save_dir):\n",
    "    \"\"\"Initialize TopicDataFrame objects for each topic\"\"\"\n",
    "    topic_dfs = {}\n",
    "    for topic_num in topic_dict.keys():\n",
    "        topic_dfs[topic_num] = TopicDataFrame(topic_num, save_dir)\n",
    "    return topic_dfs\n",
    "\n",
    "def setup_language_models():\n",
    "    \"\"\"No need to setup models with deep-translator\"\"\"\n",
    "    return None\n",
    "\n",
    "# def eda_augment(text, num_aug=5):\n",
    "#     \"\"\"Perform EDA augmentation for Spanish/Portuguese text\"\"\"\n",
    "#     # Initialize augmenter with Spanish WordNet\n",
    "#     aug = SynonymAug(aug_src='wordnet', lang='spa')  # Use 'por' for Portuguese\n",
    "    \n",
    "#     try:\n",
    "#         augmented_texts = aug.augment(text, n=num_aug)\n",
    "#         return augmented_texts\n",
    "#     except:\n",
    "#         # Fallback: if augmentation fails, return slight modifications of original text\n",
    "#         return [text + \" \" + suffix for suffix in [\n",
    "#             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "#         ]]\n",
    "\n",
    "# def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5):\n",
    "#     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "#     augmented_texts = []\n",
    "    \n",
    "#     try:\n",
    "#         # Translate to English\n",
    "#         en_text = model_to_en.generate(\n",
    "#             **tokenizer_to_en(text, return_tensors=\"pt\", padding=True),\n",
    "#             num_beams=5,\n",
    "#             num_return_sequences=num_aug,\n",
    "#             max_length=256\n",
    "#         )\n",
    "        \n",
    "#         # Translate back to Spanish/Portuguese\n",
    "#         for en in en_text:\n",
    "#             es_text = model_to_es.generate(\n",
    "#                 **tokenizer_to_es(tokenizer_to_en.decode(en, skip_special_tokens=True), \n",
    "#                                 return_tensors=\"pt\", padding=True),\n",
    "#                 num_beams=1,\n",
    "#                 max_length=256\n",
    "#             )\n",
    "#             augmented_texts.append(tokenizer_to_es.decode(es_text[0], skip_special_tokens=True))\n",
    "            \n",
    "#         return augmented_texts\n",
    "#     except:\n",
    "#         # Fallback: if translation fails, return slight modifications\n",
    "#         return [text + \" \" + suffix for suffix in [\n",
    "#             \"por favor\", \"gracias\", \"si es posible\", \"urgente\", \"cuando pueda\"\n",
    "#         ]]\n",
    "\n",
    "def eda_augment(text, num_aug=5, alpha=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Perform Easy Data Augmentation for Spanish/Portuguese text with robust error handling\n",
    "    \"\"\"\n",
    "    # Set seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Add input validation\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [text] * num_aug\n",
    "    \n",
    "    def get_synonyms(word, lang='spa'):  # Use 'por' for Portuguese\n",
    "        try:\n",
    "            aug = SynonymAug(aug_src='wordnet', lang=lang)\n",
    "            synonyms = aug.augment(word)[0]\n",
    "            return synonyms if isinstance(synonyms, list) else [synonyms]\n",
    "        except:\n",
    "            return [word]\n",
    "    \n",
    "    def synonym_replacement(words, n):\n",
    "        if not words:\n",
    "            return words\n",
    "        new_words = words.copy()\n",
    "        random_word_list = list(set([word for word in words if word.isalnum()]))\n",
    "        if not random_word_list:  # If no alphanumeric words found\n",
    "            return words\n",
    "        n = min(n, len(random_word_list))\n",
    "        random_idx = random.sample(range(len(random_word_list)), n)\n",
    "        \n",
    "        for idx in random_idx:\n",
    "            word = random_word_list[idx]\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                random_syn = random.choice(synonyms)\n",
    "                new_words = [random_syn if w == word else w for w in new_words]\n",
    "        return new_words\n",
    "    \n",
    "    def random_insertion(words, n):\n",
    "        if not words:\n",
    "            return words\n",
    "        new_words = words.copy()\n",
    "        # Get list of alphanumeric words\n",
    "        alnum_words = [w for w in words if w.isalnum()]\n",
    "        if not alnum_words:  # If no alphanumeric words found\n",
    "            return words\n",
    "            \n",
    "        for _ in range(n):\n",
    "            try:\n",
    "                random_word = random.choice(alnum_words)\n",
    "                synonyms = get_synonyms(random_word)\n",
    "                if synonyms:\n",
    "                    random_syn = random.choice(synonyms)\n",
    "                    random_idx = random.randint(0, len(new_words))\n",
    "                    new_words.insert(random_idx, random_syn)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: random_insertion failed with error {str(e)}\")\n",
    "                continue\n",
    "        return new_words\n",
    "    \n",
    "    def random_swap(words, n):\n",
    "        if len(words) < 2:\n",
    "            return words\n",
    "        new_words = words.copy()\n",
    "        for _ in range(n):\n",
    "            try:\n",
    "                idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "                new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: random_swap failed with error {str(e)}\")\n",
    "                continue\n",
    "        return new_words\n",
    "    \n",
    "    def random_deletion(words, p):\n",
    "        if len(words) <= 1:\n",
    "            return words\n",
    "        try:\n",
    "            new_words = [word for word in words if random.random() > p or not word.isalnum()]\n",
    "            if not new_words:\n",
    "                return words\n",
    "            return new_words\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: random_deletion failed with error {str(e)}\")\n",
    "            return words\n",
    "\n",
    "    # Split text into words and handle empty input\n",
    "    words = text.strip().split()\n",
    "    if not words:\n",
    "        return [text] * num_aug\n",
    "    \n",
    "    num_words = len(words)\n",
    "    augmented_texts = []\n",
    "    \n",
    "    # Calculate number of words to modify for each operation\n",
    "    n_sr = max(1, int(alpha * num_words))\n",
    "    n_ri = max(1, int(alpha * num_words))\n",
    "    n_rs = max(1, int(alpha * num_words))\n",
    "    \n",
    "    operations = ['sr', 'ri', 'rs', 'rd']\n",
    "    fallback_suffixes = [\n",
    "        \" por favor\", \" gracias\", \" si es posible\", \n",
    "        \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "        \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "    ]\n",
    "    \n",
    "    for _ in range(num_aug):\n",
    "        try:\n",
    "            operation = random.choice(operations)\n",
    "            \n",
    "            if operation == 'sr':\n",
    "                new_words = synonym_replacement(words, n_sr)\n",
    "            elif operation == 'ri':\n",
    "                new_words = random_insertion(words, n_ri)\n",
    "            elif operation == 'rs':\n",
    "                new_words = random_swap(words, n_rs)\n",
    "            else:  # rd\n",
    "                new_words = random_deletion(words, alpha)\n",
    "            \n",
    "            new_text = ' '.join(new_words)\n",
    "            if new_text.strip():  # Only add if not empty\n",
    "                augmented_texts.append(new_text)\n",
    "            else:\n",
    "                augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: augmentation failed with error {str(e)}\")\n",
    "            augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "    # Ensure we have enough augmentations\n",
    "    while len(augmented_texts) < num_aug:\n",
    "        augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "    return augmented_texts[:num_aug]  # Ensure we return exactly num_aug samples\n",
    "\n",
    "# def back_translation_augment(text, model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es, num_aug=5, seed=42):\n",
    "#     \"\"\"Perform back-translation augmentation\"\"\"\n",
    "#     # Set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "#     augmented_texts = []\n",
    "    \n",
    "#     try:\n",
    "#         # Translate to English\n",
    "#         inputs = tokenizer_to_en(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#         en_texts = model_to_en.generate(\n",
    "#             **inputs,\n",
    "#             num_beams=5,\n",
    "#             num_return_sequences=num_aug,\n",
    "#             max_length=512,\n",
    "#             temperature=0.8\n",
    "#         )\n",
    "        \n",
    "#         # Translate back to Spanish/Portuguese\n",
    "#         for en_text in en_texts:\n",
    "#             decoded_en = tokenizer_to_en.decode(en_text, skip_special_tokens=True)\n",
    "#             inputs_es = tokenizer_to_es(decoded_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#             es_text = model_to_es.generate(\n",
    "#                 **inputs_es,\n",
    "#                 num_beams=3,\n",
    "#                 max_length=512,\n",
    "#                 temperature=0.8\n",
    "#             )\n",
    "#             decoded_es = tokenizer_to_es.decode(es_text[0], skip_special_tokens=True)\n",
    "#             augmented_texts.append(decoded_es)\n",
    "            \n",
    "#         return augmented_texts\n",
    "#     except Exception as e:\n",
    "#         print(f\"Back-translation error: {str(e)}\")\n",
    "#         # Fallback: return variations with added suffixes\n",
    "#         fallback_texts = []\n",
    "#         suffixes = [\" por favor\", \" gracias\", \" si es posible\", \" urgente\", \" cuando pueda\"]\n",
    "#         for suffix in suffixes[:num_aug]:\n",
    "#             fallback_texts.append(text + suffix)\n",
    "#         return fallback_texts\n",
    "\n",
    "def back_translation_augment(text, num_aug=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform back-translation augmentation using Google Translate\n",
    "    Uses different intermediate languages to create variations\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # List of intermediate languages to create variations\n",
    "    intermediate_languages = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'gl']\n",
    "    source_lang = 'es'  # assuming Spanish is source language\n",
    "    \n",
    "    augmented_texts = []\n",
    "    used_langs = random.sample(intermediate_languages, min(num_aug, len(intermediate_languages)))\n",
    "    \n",
    "    for lang in used_langs:\n",
    "        try:\n",
    "            # Translate to intermediate language\n",
    "            translator_to = GoogleTranslator(source=source_lang, target=lang)\n",
    "            intermediate_text = translator_to.translate(text)\n",
    "            \n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "            # Translate back to Spanish\n",
    "            translator_back = GoogleTranslator(source=lang, target=source_lang)\n",
    "            back_translated = translator_back.translate(intermediate_text)\n",
    "            \n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "            if back_translated and back_translated.strip():\n",
    "                augmented_texts.append(back_translated)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Translation error with language {lang}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we couldn't generate enough augmentations, add variations of the original\n",
    "    fallback_suffixes = [\n",
    "        \" por favor\", \" gracias\", \" si es posible\", \n",
    "        \" urgente\", \" cuando pueda\", \" necesito ayuda\",\n",
    "        \" importante\", \" consulta\", \" duda\", \" solicitud\"\n",
    "    ]\n",
    "    \n",
    "    while len(augmented_texts) < num_aug:\n",
    "        augmented_texts.append(text + random.choice(fallback_suffixes))\n",
    "    \n",
    "    return augmented_texts[:num_aug]\n",
    "\n",
    "# def process_dataframe(df, topic_dict, method='EDA', config=None):\n",
    "#     \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "#     # Setup\n",
    "#     save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "#     topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "#     if method == 'Back-translation':\n",
    "#         model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es = setup_language_models()\n",
    "    \n",
    "#     # Process each row\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "#         topic_name = row['topic_name']\n",
    "#         topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "        \n",
    "#         # Generate augmented texts\n",
    "#         if method == 'EDA':\n",
    "#             augmented_texts = eda_augment(row['text'])\n",
    "#         else:  # Back-translation\n",
    "#             augmented_texts = back_translation_augment(\n",
    "#                 row['text'], model_to_en, model_to_es, tokenizer_to_en, tokenizer_to_es\n",
    "#             )\n",
    "        \n",
    "#         # Create and save new rows immediately\n",
    "#         for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "#             new_row = {\n",
    "#                 'index_meta': row['index_meta'],\n",
    "#                 'topic_name': row['topic_name'],\n",
    "#                 'sample': sample_num,\n",
    "#                 'area_TEIS': row['area_TEIS'],\n",
    "#                 'text': aug_text\n",
    "#             }\n",
    "#             topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "#     return topic_dfs\n",
    "\n",
    "def process_dataframe(df, topic_dict, method='EDA', config=None, seed=42):\n",
    "    \"\"\"Process the dataframe and generate synthetic data\"\"\"\n",
    "    # Set seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Setup\n",
    "    save_dir = config.eda_dir if method == 'EDA' else config.bt_dir\n",
    "    topic_dfs = initialize_topic_dfs(topic_dict, save_dir)\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {method}\"):\n",
    "        # Set a different seed for each row\n",
    "        row_seed = seed + idx\n",
    "        random.seed(row_seed)\n",
    "        \n",
    "        topic_name = row['rating']\n",
    "        topic_num = get_key_by_value(topic_dict, topic_name)\n",
    "\n",
    "        # Check if this index_meta already exists in the synthetic df\n",
    "        current_df = topic_dfs[topic_num].df\n",
    "        if row['index_meta'] in current_df['index_meta'].values:\n",
    "            print(f\"Skipping {method} augmentation for index_meta {row['index_meta']} as it already exists\")\n",
    "            continue\n",
    "        \n",
    "        # Generate augmented texts\n",
    "        if method == 'EDA':\n",
    "            augmented_texts = eda_augment(row['text'], seed=row_seed)\n",
    "        else:  # Back-translation\n",
    "            augmented_texts = back_translation_augment(row['text'], seed=row_seed)\n",
    "        \n",
    "        # Create and save new rows immediately\n",
    "        for sample_num, aug_text in enumerate(augmented_texts, 1):\n",
    "            new_row = {\n",
    "                'index_meta': row['index_meta'],\n",
    "                'rating': row['rating'],\n",
    "                'sample': sample_num,\n",
    "                'title': row['title'],\n",
    "                'text': aug_text\n",
    "            }\n",
    "            topic_dfs[topic_num].append_and_save(new_row)\n",
    "    \n",
    "    return topic_dfs\n",
    "\n",
    "def get_key_by_value(dictionary, search_value):\n",
    "    for key, value in dictionary.items():\n",
    "        if value == search_value:\n",
    "            return key\n",
    "    return None  # Return None if value not found\n",
    "\n",
    "def main(train_df, topic_dict, base_save_dir=\"synthetic_data\", seed=42):\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    # Set initial seed\n",
    "    set_global_seeds(seed)\n",
    "\n",
    "    # Initialize configuration\n",
    "    config = Config(base_save_dir)\n",
    "    \n",
    "    # Create folders\n",
    "    setup_folders(config)\n",
    "    \n",
    "    # Process with EDA\n",
    "    print(\"Processing with EDA...\")\n",
    "    eda_topic_dfs = process_dataframe(train_df, topic_dict, method='EDA', config=config, seed=seed)\n",
    "    \n",
    "    # Process with Back-translation\n",
    "    print(\"Processing with Back-translation...\")\n",
    "    bt_topic_dfs = process_dataframe(train_df, topic_dict, method='Back-translation', config=config, seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with EDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EDA:   0%|          | 0/6400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EDA: 100%|██████████| 6400/6400 [46:43<00:00,  2.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with Back-translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Back-translation:   1%|          | 56/6400 [05:22<10:09:31,  5.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHENHA~1\\AppData\\Local\\Temp/ipykernel_21560/1560922478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Run the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_PCA_YZ_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBASE_SAVE_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\CHENHA~1\\AppData\\Local\\Temp/ipykernel_21560/3969989526.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(train_df, topic_dict, base_save_dir, seed)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;31m# Process with Back-translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Processing with Back-translation...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m     \u001b[0mbt_topic_dfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Back-translation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CHENHA~1\\AppData\\Local\\Temp/ipykernel_21560/3969989526.py\u001b[0m in \u001b[0;36mprocess_dataframe\u001b[1;34m(df, topic_dict, method, config, seed)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0maugmented_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meda_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Back-translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0maugmented_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_translation_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;31m# Create and save new rows immediately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CHENHA~1\\AppData\\Local\\Temp/ipykernel_21560/3969989526.py\u001b[0m in \u001b[0;36mback_translation_augment\u001b[1;34m(text, num_aug, seed)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;31m# Translate to intermediate language\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mtranslator_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoogleTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[0mintermediate_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;31m# Add small delay to avoid rate limiting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\deep_translator\\google.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayload_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             response = requests.get(\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             )\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\";\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your save directory here\n",
    "    BASE_SAVE_DIR = \"D:/Oxford/Second Year/AutoGeTS_Revisions/Additional_Datasets/Amazon_Review/Synthetic_Data\"\n",
    "    SEED = 42\n",
    "    \n",
    "    # Run the pipeline\n",
    "    main(train_PCA_YZ_df, topic_dict, base_save_dir=BASE_SAVE_DIR, seed=SEED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
