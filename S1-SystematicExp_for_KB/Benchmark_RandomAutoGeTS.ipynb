{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import pynvml\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean folder names\n",
    "def clean_folder_name(folder_name):\n",
    "    # Remove invalid characters\n",
    "    cleaned_name = re.sub(r'[<>:\"/\\\\|?*]', '', folder_name)\n",
    "    # Remove trailing dots and spaces\n",
    "    cleaned_name = cleaned_name.rstrip('. ')\n",
    "    return cleaned_name\n",
    "\n",
    "\n",
    "def CPU_monitor_memory_usage():\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_usage = memory_info.percent\n",
    "        \n",
    "    print(f\"CPU Current memory usage: {memory_usage}%\")\n",
    "\n",
    "    if memory_usage >= 95:\n",
    "        print(\"CPU Memory usage is too high. Pausing execution...\")\n",
    "        gc.collect()  # Trigger garbage collection manually\n",
    "        while memory_usage > 30:\n",
    "            time.sleep(10)\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            memory_usage = memory_info.percent\n",
    "        print(\"CPU Memory usage is low enough. Resuming execution...\")\n",
    "\n",
    "    # time.sleep(5)\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    # Initialize NVML\n",
    "    pynvml.nvmlInit()\n",
    "    \n",
    "    try:\n",
    "        # Get handle for the first GPU\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        # Get memory info\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        total_memory = mem_info.total\n",
    "        used_memory = mem_info.used\n",
    "\n",
    "        # Calculate the percentage of GPU memory used\n",
    "        memory_usage = (used_memory / total_memory) * 100\n",
    "        print(f\"Current GPU memory usage: {memory_usage:.2f}%\")\n",
    "\n",
    "        # Check if memory usage is too high\n",
    "        if memory_usage >= 95:\n",
    "            print(\"GPU memory usage is too high. Pausing execution...\")\n",
    "            while memory_usage > 30:\n",
    "                time.sleep(10)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                used_memory = mem_info.used\n",
    "                memory_usage = (used_memory / total_memory) * 100\n",
    "            print(\"GPU memory usage is low enough. Resuming execution...\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_df(report, y_true, y_pred):\n",
    "    global bch_class_df\n",
    "    global topic_dict\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    order_labels = list(topic_dict.values())\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    labels = df.index[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Extracting TP, FP, TN, FN for each class\n",
    "    TP = cm.diagonal()\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    sens = sum(TP) / (sum(TP)+sum(FN))\n",
    "    spec = sum(TN) / (sum(TN)+sum(FP))\n",
    "    \n",
    "    # Calculate Sensitivity (same as recall)\n",
    "    df['Sensitivity'] = df['recall']\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm))\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Assign computed specificity to dataframe except for the last three rows\n",
    "    df.loc[df.index[:-3], 'Specificity'] = specificity\n",
    "    \n",
    "    # Handling special cases\n",
    "    # Set 'accuracy' row sensitivity and specificity to the accuracy value\n",
    "    accuracy = df.loc['accuracy', 'precision']  # assuming 'precision' contains the accuracy\n",
    "    df.loc['accuracy', ['Sensitivity', 'Specificity']] = sens, spec\n",
    "    \n",
    "    # Calculate 'macro avg' and 'weighted avg' for sensitivity and specificity\n",
    "    df.loc['macro avg', 'Sensitivity'] = df.iloc[:-3]['Sensitivity'].mean()\n",
    "    df.loc['weighted avg', 'Sensitivity'] = np.average(df.iloc[:-3]['Sensitivity'], weights=df.iloc[:-3]['support'])\n",
    "    \n",
    "    df.loc['macro avg', 'Specificity'] = df.iloc[:-3]['Specificity'].mean()\n",
    "    df.loc['weighted avg', 'Specificity'] = np.average(df.iloc[:-3]['Specificity'], weights=df.iloc[:-3]['support'])\n",
    "\n",
    "    # Calculate Balanced Accuracy for each row, including special averages\n",
    "    df['Balanced Accuracy'] = (df['Sensitivity'] + df['Specificity']) / 2\n",
    "\n",
    "    df.loc['accuracy', 'precision'] = sum(TP) / (sum(TP) + sum(FP))\n",
    "    df.loc['accuracy', 'recall'] = sum(TP) / (sum(TP) + sum(FN))\n",
    "    df.loc['accuracy', 'f1-score'] = 2* sum(TP) / (2 * sum(TP) + sum(FP) + sum(FN))\n",
    "\n",
    "    columns_to_drop = [col for col in ['TP', 'FP', 'TN', 'FN'] if col in bch_class_df.columns]\n",
    "    bch_class_df_noFr = bch_class_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    diff_df = df - bch_class_df_noFr\n",
    "    # Renaming columns for clarity\n",
    "    diff_df.columns = ['Diff ' + col for col in diff_df.columns]\n",
    "\n",
    "    # Concatenating the original dataframe with the differences\n",
    "    combined_df = pd.concat([df, diff_df], axis=1)\n",
    "\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    combined_df.loc[labels, 'Accuracy'] = class_accuracy\n",
    "    # Copying f1-score to 'Accuracy' for the last three rows\n",
    "    combined_df.loc[['accuracy', 'macro avg', 'weighted avg'], 'Accuracy'] = combined_df.loc[['accuracy', 'macro avg', 'weighted avg'], 'f1-score']\n",
    "\n",
    "    # Calculate and append TP, FP, TN, FN metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"TN\": TN,\n",
    "        \"FN\": FN\n",
    "    }, index=labels)\n",
    "\n",
    "    # Merge the new metrics into the existing DataFrame\n",
    "    combined_df = combined_df.merge(metrics_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Reorder DataFrame based on specified order labels\n",
    "    combined_df = combined_df.reindex(order_labels + ['accuracy', 'macro avg', 'weighted avg'])\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_retraining(data_syn, individual_Segment_dict):\n",
    "    global dots_mode\n",
    "    global history_segments_dict\n",
    "    global X_train_r\n",
    "    global Y_train_r\n",
    "    global X_test_re\n",
    "    global Y_test_re\n",
    "    global catboost_params\n",
    "    global sum_GPU_seconds\n",
    "    global total_gpu_seconds\n",
    "    global GPU_limit\n",
    "\n",
    "    global topic_name\n",
    "    global topic_number\n",
    "    global random_results_path\n",
    "    global random_results_name\n",
    "    global fold_results_df\n",
    "\n",
    "    CPU_monitor_memory_usage()\n",
    "    monitor_gpu_memory()\n",
    "\n",
    "    if dots_mode == \"False\":\n",
    "        syn_original_list = individual_Segment_dict[\"red_dots_list\"]\n",
    "    elif dots_mode == \"Both\":\n",
    "        syn_original_list = individual_Segment_dict[\"red_dots_list\"] + individual_Segment_dict[\"blue_dots_list\"]\n",
    "\n",
    "    segment_key = (topic_name, topic_number, frozenset(syn_original_list))\n",
    "\n",
    "    if len(syn_original_list) == 0:\n",
    "        individual_Segment_dict['model'] = None\n",
    "        individual_Segment_dict['true_labels'] = None\n",
    "        individual_Segment_dict['predicted_labels'] = None\n",
    "        individual_Segment_dict['classification_df'] = None\n",
    "        individual_Segment_dict['fitness_score'] = (None, None)\n",
    "        individual_Segment_dict['number_of_syn_sample'] = None\n",
    "        individual_Segment_dict['retraining_time'] = None\n",
    "        individual_Segment_dict['retrained_dots_list'] = []\n",
    "        return individual_Segment_dict\n",
    "\n",
    "    for previous_segment_key, previous_Segment_dict in history_segments_dict.items():\n",
    "        if syn_original_list == previous_Segment_dict['retrained_dots_list']:\n",
    "            individual_Segment_dict['model'] = previous_Segment_dict['model']\n",
    "            individual_Segment_dict['true_labels'] = previous_Segment_dict['true_labels']\n",
    "            individual_Segment_dict['predicted_labels'] = previous_Segment_dict['predicted_labels']\n",
    "            individual_Segment_dict['classification_df'] = previous_Segment_dict['classification_df']\n",
    "            individual_Segment_dict['fitness_score'] = previous_Segment_dict['fitness_score']\n",
    "            individual_Segment_dict['number_of_syn_sample'] = previous_Segment_dict['number_of_syn_sample']\n",
    "            individual_Segment_dict['retraining_time'] = previous_Segment_dict['retraining_time']\n",
    "            individual_Segment_dict['retrained_dots_list'] = syn_original_list\n",
    "            return individual_Segment_dict\n",
    "    \n",
    "    filtered_syn_df = data_syn[data_syn['index_meta'].isin(syn_original_list)]\n",
    "\n",
    "    X_train_re = pd.concat([X_train_r, filtered_syn_df.drop(columns=['topic_name'])])\n",
    "    Y_train_re = pd.concat([Y_train_r, filtered_syn_df['topic_name']])\n",
    "\n",
    "    train_pool_re = Pool(\n",
    "        X_train_re[[\"text\", \"area_TEIS\"]],\n",
    "        Y_train_re,\n",
    "        text_features=[\"text\"],\n",
    "        cat_features=[\"area_TEIS\"]\n",
    "    )\n",
    "    valid_pool_re = Pool(\n",
    "        X_test_re[[\"text\", \"area_TEIS\"]],\n",
    "        Y_test_re,\n",
    "        text_features=[\"text\"],\n",
    "        cat_features=[\"area_TEIS\"]\n",
    "    )\n",
    "\n",
    "    catboost_params = catboost_params\n",
    "            \n",
    "    # Model Training\n",
    "    model_re = CatBoostClassifier(**catboost_params)\n",
    "    start_time = time.time()  # Start timing\n",
    "    model_re.fit(train_pool_re, eval_set=valid_pool_re)\n",
    "    training_time = time.time() - start_time  # End timing\n",
    "\n",
    "    sum_GPU_seconds += training_time\n",
    "    if sum_GPU_seconds >= total_gpu_seconds:\n",
    "        GPU_limit = True\n",
    "\n",
    "    # Save the retrain performances\n",
    "    predictions = model_re.predict(X_test_re[[\"text\", \"area_TEIS\"]])\n",
    "    accuracy = accuracy_score(Y_test_re, predictions)\n",
    "    report = classification_report(Y_test_re, predictions, digits=3, output_dict=True)\n",
    "    classification_df = classification_report_to_df(report, Y_test_re, predictions)\n",
    "    \n",
    "    fitness_score = (accuracy, classification_df.loc[segment_key[0], 'recall'])\n",
    "            \n",
    "    # Save the trained model, classification_df, and fitness_score\n",
    "    individual_Segment_dict['model'] = model_re\n",
    "    individual_Segment_dict['true_labels'] = []  # Convert to list if Y_test_re is a pandas Series or numpy array\n",
    "    individual_Segment_dict['predicted_labels'] = []  # Convert to list for consistency\n",
    "    individual_Segment_dict['classification_df'] = classification_df\n",
    "    individual_Segment_dict['fitness_score'] = fitness_score\n",
    "    individual_Segment_dict['number_of_syn_sample'] = len(filtered_syn_df)\n",
    "    individual_Segment_dict['retraining_time'] = training_time  # Save the training time\n",
    "    individual_Segment_dict['retrained_dots_list'] = syn_original_list\n",
    "\n",
    "    class_DF_path = f'{random_results_path}/Class_DF'\n",
    "    os.makedirs(class_DF_path, exist_ok=True)\n",
    "\n",
    "    new_row_index = len(fold_results_df)\n",
    "    new_ParamCV_row = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"topic_number\": topic_number,\n",
    "            'fitness_score': fitness_score,\n",
    "            \"accuracy\": fitness_score[0],\n",
    "            \"topic_recall\": fitness_score[1],\n",
    "            'balanced_fitness_score': (classification_df.loc['accuracy', 'Balanced Accuracy'], classification_df.loc[topic_name, 'Balanced Accuracy']),\n",
    "            'overall_balanced_accuracy': classification_df.loc['accuracy', 'Balanced Accuracy'],\n",
    "            'topic_balanced_accuracy': classification_df.loc[topic_name, 'Balanced Accuracy'],\n",
    "            'balanced_acc_rec_score': (classification_df.loc[topic_name, 'Balanced Accuracy'], classification_df.loc[topic_name, 'recall']),\n",
    "            'topic_F1': classification_df.loc[topic_name, 'f1-score'],\n",
    "            'overall_F1': classification_df.loc['accuracy', 'f1-score'],\n",
    "            'overall_recall': classification_df.loc['accuracy', 'recall'],\n",
    "            \"retraining_time\": training_time,\n",
    "            \"number_of_syn_sample\": len(filtered_syn_df),\n",
    "            \"retrained_dots_list\": syn_original_list,\n",
    "            'true_labels': [],\n",
    "            'predicted_labels': [],\n",
    "            \"segment_key\": (topic_name, topic_number, frozenset(syn_original_list)),\n",
    "            'classDF_path': f'{class_DF_path}/{topic_number}_Rand-Bch_{new_row_index}_ClassDF.csv'\n",
    "        }\n",
    "    \n",
    "    classification_df.to_csv(f'{class_DF_path}/{topic_number}_Rand-Bch_{new_row_index}_ClassDF.csv', index=True)\n",
    "    classification_df.to_pickle(f'{class_DF_path}/{topic_number}_Rand-Bch_{new_row_index}_ClassDF.pkl')\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for a single row\n",
    "    new_ParamCV_row_df = pd.DataFrame([new_ParamCV_row])\n",
    "    # Concatenate this new row DataFrame to the existing DataFrame\n",
    "    fold_results_df = pd.concat([fold_results_df, new_ParamCV_row_df], ignore_index=True)\n",
    "    fold_results_df.to_csv(f'{random_results_path}/{random_results_name}_AllEval.csv', index=False)\n",
    "    fold_results_df.to_pickle(f'{random_results_path}/{random_results_name}_AllEval.pkl')\n",
    "\n",
    "    if individual_Segment_dict['fitness_score'] != (None, None):\n",
    "        history_segments_dict[segment_key] = individual_Segment_dict\n",
    "        print(individual_Segment_dict['fitness_score'])\n",
    "\n",
    "    return individual_Segment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dots_lists(class_data_pool, train_PCA_YZ_df, topic_name, iteration, base_seed=42):\n",
    "    \"\"\"\n",
    "    Randomly select index_meta values and classify them into blue/red dots lists based on topic prediction.\n",
    "    Different random selections for each iteration while maintaining reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    class_data_pool: DataFrame with 'index_meta' column for the specific topic\n",
    "    train_PCA_YZ_df: DataFrame with 'topic_name' and 'pred_topic_name' columns\n",
    "    topic_name: String of the current topic name\n",
    "    iteration: Current iteration number (used to generate different but reproducible selections)\n",
    "    base_seed: Base random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    dict: Contains 'blue_dots_list' and 'red_dots_list'\n",
    "    \"\"\"\n",
    "    # Generate a new seed for this iteration that's dependent on both base_seed and iteration\n",
    "    current_seed = base_seed + iteration\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    # Get list of all possible index_meta values\n",
    "    index_meta_pool = class_data_pool['index_meta'].tolist()\n",
    "    \n",
    "    # Randomly decide how many items to select (between 1 and length of pool)\n",
    "    # Using a different seed derivative for the size selection\n",
    "    random.seed(current_seed + 1000)  # Different seed for size selection\n",
    "    num_selections = random.randint(1, len(index_meta_pool))\n",
    "    \n",
    "    # Reset seed for the actual selection\n",
    "    random.seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    # Randomly select index_meta values (with replacement allowed)\n",
    "    selected_indices = np.random.choice(index_meta_pool, \n",
    "                                      size=num_selections, \n",
    "                                      replace=True).tolist()\n",
    "    \n",
    "    # Initialize dictionary for results\n",
    "    individual_Segment_dict = {\n",
    "        \"blue_dots_list\": [],\n",
    "        \"red_dots_list\": []\n",
    "    }\n",
    "    \n",
    "    # Classify each selected index_meta\n",
    "    for index_meta in selected_indices:\n",
    "        # Find the corresponding row in train_PCA_YZ_df\n",
    "        matching_row = train_PCA_YZ_df[train_PCA_YZ_df['index_meta'] == index_meta].iloc[0]\n",
    "        \n",
    "        # Check if predicted topic matches actual topic\n",
    "        if matching_row['topic_name'] == matching_row['pred_topic_name']:\n",
    "            individual_Segment_dict[\"blue_dots_list\"].append(index_meta)\n",
    "        else:\n",
    "            individual_Segment_dict[\"red_dots_list\"].append(index_meta)\n",
    "    \n",
    "    return individual_Segment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominates(score1, score2):\n",
    "    \"\"\"\n",
    "    Determines if one score dominates another.\n",
    "    A score1 dominates score2 if it is better in all the objectives or equal in some and better in at least one.\n",
    "    \"\"\"\n",
    "    return (score1[0] > score2[0] and score1[1] >= score2[1]) or (score1[0] >= score2[0] and score1[1] > score2[1])\n",
    "\n",
    "def find_pareto_front(df):\n",
    "    \"\"\"\n",
    "    Marks rows as 'Yes' if they are on the Pareto front, 'No' otherwise.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Copy DataFrame to avoid modifying the original\n",
    "    df['Pareto'] = 'No'  # Initialize the Pareto column with 'No'\n",
    "    \n",
    "    scores = df['balanced_acc_rec_score'].tolist()\n",
    "    is_pareto = np.ones(len(scores), dtype=bool)  # Initialize all as True\n",
    "    \n",
    "    for i1 in range(len(scores)):\n",
    "        for i2 in range(len(scores)):\n",
    "            if i1 != i2 and dominates(scores[i2], scores[i1]):\n",
    "                is_pareto[i1] = False\n",
    "                break\n",
    "\n",
    "    # Update the 'Pareto' column based on the Pareto front\n",
    "    df.loc[is_pareto, 'Pareto'] = 'Yes'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_best_values(df):\n",
    "    # Identify the maximum values for each specified column\n",
    "    max_values = {\n",
    "        'accuracy': df['accuracy'].max(),\n",
    "        'topic_recall': df['topic_recall'].max(),\n",
    "        'overall_balanced_accuracy': df['overall_balanced_accuracy'].max(),\n",
    "        'topic_balanced_accuracy': df['topic_balanced_accuracy'].max(),\n",
    "        'topic_F1': df['topic_F1'].max(),\n",
    "        'overall_F1': df['overall_F1'].max(),\n",
    "        'overall_recall': df['overall_recall'].max()\n",
    "    }\n",
    "    \n",
    "    # Function to apply to each row to determine the best columns\n",
    "    def check_best(row):\n",
    "        return [col for col, max_val in max_values.items() if row[col] == max_val]\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df['best'] = df.apply(check_best, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_best_values(df):\n",
    "    # Identify the maximum values for each specified column\n",
    "    max_values = {\n",
    "        'accuracy': df['accuracy'].max(),\n",
    "        'topic_recall': df['topic_recall'].max(),\n",
    "        'overall_balanced_accuracy': df['overall_balanced_accuracy'].max(),\n",
    "        'topic_balanced_accuracy': df['topic_balanced_accuracy'].max(),\n",
    "        'topic_F1': df['topic_F1'].max(),\n",
    "        'overall_F1': df['overall_F1'].max(),\n",
    "        'overall_recall': df['overall_recall'].max()\n",
    "    }\n",
    "    \n",
    "    # Function to apply to each row to determine the best columns\n",
    "    def check_best(row):\n",
    "        return [col for col, max_val in max_values.items() if row[col] == max_val]\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df['best'] = df.apply(check_best, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for topic_number in [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\", \"T6\", \"T7\", \"T8\", \"T9\", \"T10\", \"T11\", \"T12\", \"T13\", \"T14\", \"T15\"]:\n",
    "        fold_results_df = pd.DataFrame()\n",
    "\n",
    "        history_segments_dict = {}\n",
    "\n",
    "        # Load Data\n",
    "        data = pd.read_csv(f'D:/AutoGeTS/Data/tickets_topics.csv',lineterminator='\\n')\n",
    "        data_topic = data.dropna().reset_index()\n",
    "        data_topic = data_topic.rename(columns={'index': 'index_meta'})\n",
    "\n",
    "        X_train_r_both, X_test_re, Y_train_r_both, Y_test_re = train_test_split(data_topic, data_topic.topic_name, test_size = 0.2,random_state = 42)\n",
    "        \n",
    "        # Further split the training set to create a validation set\n",
    "        X_train_r, X_test_re_Test, Y_train_r, Y_test_re_Test = train_test_split(\n",
    "            X_train_r_both, \n",
    "            Y_train_r_both, \n",
    "            test_size=0.2,  # 20% of the initial training set, which is 16% of the original data\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        bch_class_df = pd.read_pickle(\"D:/AutoGeTS/Models_and_Performances/Benchmark_M0_Classdf_0.pkl\")\n",
    "\n",
    "        train_PCA_YZ_df = pd.read_pickle(\"D:/AutoGeTS/Data/Train_PCA_YZ_withPred_0.pkl\")\n",
    "        # train_PCA_YZ_df = train_PCA_YZ_df.rename(columns={'index': 'index_meta'})\n",
    "        pca_columns = [col for col in train_PCA_YZ_df.columns if 'PCA_' in col]\n",
    "        pca_pairs = list(itertools.combinations(pca_columns, 2))\n",
    "\n",
    "        topic_dict = {\"T1\": \"IT support and assistance.\",\"T2\": \"Account activation and access issues.\",\"T3\": \"Password and device security.\",\n",
    "                    \"T4\": \"Printer issues and troubleshooting.\",\"T5\": \"HP Dock connectivity issues.\",\"T6\": \"Employee documentation and errors.\",\n",
    "                    \"T7\": \"\\\"Access and login issues\\\"\",\"T8\": \"Opening and managing files/devices.\",\"T9\": \"Mobile email and VPN setup.\",\n",
    "                    \"T10\": \"IT support and communication.\",\"T11\": \"Error handling in RPG programming.\",\"T12\": \"Email security and attachments.\",\n",
    "                    \"T13\": \"Humanitarian aid for Ukraine.\",\"T14\": \"Internet connectivity issues in offices.\",\"T15\": \"Improving integration with Infojobs.\"}\n",
    "\n",
    "        \"\"\"Parameters and Input Section ----------------\"\"\"\n",
    "        # topic_number = \"T13\"\n",
    "        syn_number = 1\n",
    "\n",
    "        dots_mode = \"Both\"\n",
    "\n",
    "        gpu_hours = 1\n",
    "        total_gpu_seconds = gpu_hours * 60 * 60\n",
    "\n",
    "        data_syn_raw = pd.read_pickle(f'D:/AutoGeTS/Synthetic_Data/{topic_number}-synthesis-{syn_number}.pkl')\n",
    "        # # Added synthetic data path\n",
    "        # if topic_number in [\"T1\", \"T2\"]:\n",
    "        #     data_syn_raw = pd.read_pickle(f'D:/AutoGeTS/Synthetic_Data/{topic_number}-synthesis-{syn_number}.pkl')\n",
    "        # else:\n",
    "        #     data_syn_raw = pd.read_csv(f'D:/AutoGeTS/Synthetic_Data/{topic_number}-synthesis-{syn_number}.csv',lineterminator='\\n')\n",
    "        data_syn = data_syn_raw[[\"index_meta\", \"text\", 'topic_name', \"sample\", \"area_TEIS\"]].dropna()\n",
    "\n",
    "        used_pca_pairs = pca_pairs # pca_pairs, [(\"PCA_0\", \"PCA_5\")]\n",
    "\n",
    "        random_results_path = f\"D:/AutoGeTS/LLM_1GPUh/Random_Bch_Results/GPU{gpu_hours}h\"\n",
    "        os.makedirs(random_results_path, exist_ok=True)\n",
    "        random_results_name = f\"Random-Bch_{topic_number}_Mode{dots_mode}\"\n",
    "\n",
    "        catboost_params = {'iterations': 300, 'learning_rate': 0.2, 'depth': 8, 'l2_leaf_reg': 1, \n",
    "                            'bagging_temperature': 1, 'random_strength': 1, 'border_count': 254, \n",
    "                            'eval_metric': 'TotalF1', 'task_type': 'GPU', 'early_stopping_rounds': 20, 'use_best_model': True, 'verbose': 0, 'random_seed': 2}\n",
    "\n",
    "        \"\"\"------------------------------------------------\"\"\"\n",
    "        topic_name = topic_dict[topic_number]\n",
    "        clean_topic_name = clean_folder_name(topic_name)\n",
    "\n",
    "        class_data_pool = X_train_r[X_train_r['topic_name'] == topic_name]\n",
    "        class_index_meta_pool = class_data_pool['index_meta'].tolist()\n",
    "\n",
    "        sum_GPU_seconds = 0\n",
    "        GPU_limit = False\n",
    "\n",
    "        # Initialize iteration counter before the loop\n",
    "        iteration = 0\n",
    "\n",
    "        while GPU_limit is False:\n",
    "            individual_Segment_dict = create_dots_lists(class_data_pool, train_PCA_YZ_df, topic_name, iteration, base_seed=42)\n",
    "\n",
    "            individual_random_bch_dict = segment_retraining(data_syn, individual_Segment_dict)\n",
    "            iteration += 1\n",
    "\n",
    "        if GPU_limit == True:\n",
    "            fold_results_df  = find_pareto_front(fold_results_df)\n",
    "            fold_results_df = find_best_values(fold_results_df)\n",
    "            fold_results_df.to_csv(f'{random_results_path}/RandBchSegs_{random_results_name}.csv', index=False)\n",
    "            fold_results_df.to_pickle(f'{random_results_path}/RandBchSegs_{random_results_name}.pkl')\n",
    "            # break\n",
    "            df = find_best_values(fold_results_df)\n",
    "\n",
    "            bch_topic_recall = bch_class_df.loc[topic_name, 'recall']\n",
    "            bch_topic_balanced_accuracy = bch_class_df.loc[topic_name, 'Balanced Accuracy']\n",
    "            bch_overall_balanced_accuracy = bch_class_df.loc['accuracy', 'Balanced Accuracy']\n",
    "            bch_overall_F1_score = bch_class_df.loc['accuracy', 'f1-score']\n",
    "\n",
    "            # Calculate improvements\n",
    "            df['imp_topic_recall'] = df['topic_recall'] - bch_topic_recall\n",
    "            df['imp_topic_balanced_accuracy'] = df['topic_balanced_accuracy'] - bch_topic_balanced_accuracy\n",
    "            df['imp_overall_balanced_accuracy'] = df['overall_balanced_accuracy'] - bch_overall_balanced_accuracy\n",
    "            df['imp_overall_F1'] = df['overall_F1'] - bch_overall_F1_score\n",
    "\n",
    "            # Calculate cumulative retraining_time\n",
    "            df['cumulative_time'] = df['retraining_time'].cumsum()\n",
    "\n",
    "            # Calculate max and average improvements\n",
    "            df['max_topic_recall_imp'] = df[['imp_topic_recall']].max(axis=1).cummax()\n",
    "            df['average_topic_recall_imp'] = df[['imp_topic_recall']].mean(axis=1).expanding().mean()\n",
    "\n",
    "            df['max_topic_balanced_acc_imp'] = df[['imp_topic_balanced_accuracy']].max(axis=1).cummax()\n",
    "            df['average_topic_balanced_acc_imp'] = df[['imp_topic_balanced_accuracy']].mean(axis=1).expanding().mean()\n",
    "\n",
    "            df['max_overall_balanced_acc_imp'] = df[['imp_overall_balanced_accuracy']].max(axis=1).cummax()\n",
    "            df['average_overall_balanced_acc_imp'] = df[['imp_overall_balanced_accuracy']].mean(axis=1).expanding().mean()\n",
    "\n",
    "            df['max_overall_F1_improvement'] = df[['imp_overall_F1']].max(axis=1).cummax()\n",
    "            df['average_overall_F1_improvement'] = df[['imp_overall_F1']].mean(axis=1).expanding().mean()\n",
    "\n",
    "            # Calculate percentage improvements\n",
    "            def calculate_percentage_improvement(row):\n",
    "                topic_name = row['topic_name']\n",
    "                topic_benchmark = bch_class_df.loc[topic_name, \"Balanced Accuracy\"]\n",
    "                topic_recall_benchmark = bch_class_df.loc[topic_name, \"recall\"]\n",
    "                overall_benchmark = bch_class_df.loc[\"accuracy\", \"Balanced Accuracy\"]\n",
    "                overall_f1_benchmark = bch_class_df.loc[\"accuracy\", \"f1-score\"]\n",
    "                \n",
    "                row['imp_topic_balanced_accuracy_pct'] = (row['imp_topic_balanced_accuracy'] / topic_benchmark) * 100\n",
    "                row['imp_overall_balanced_accuracy_pct'] = (row['imp_overall_balanced_accuracy'] / overall_benchmark) * 100\n",
    "                row['imp_topic_recall_pct'] = (row['imp_topic_recall'] / topic_recall_benchmark) * 100\n",
    "                row['imp_overall_F1_pct'] = (row['imp_overall_F1'] / overall_f1_benchmark) * 100\n",
    "\n",
    "                row['max_imp_topic_balanced_accuracy_pct'] = (row['max_topic_balanced_acc_imp'] / topic_benchmark) * 100\n",
    "                row['max_imp_overall_balanced_accuracy_pct'] = (row['max_overall_balanced_acc_imp'] / overall_benchmark) * 100\n",
    "                row['max_imp_topic_recall_pct'] = (row['max_topic_recall_imp'] / topic_recall_benchmark) * 100\n",
    "                row['max_imp_overall_F1_pct'] = (row['max_overall_F1_improvement'] / overall_f1_benchmark) * 100\n",
    "                \n",
    "                return row\n",
    "\n",
    "            df = df.apply(calculate_percentage_improvement, axis=1)\n",
    "\n",
    "            df.to_csv(f'{random_results_path}/Processed_{random_results_name}_AllEval.csv', index=False)\n",
    "            df.to_pickle(f'{random_results_path}/Processed_{random_results_name}_AllEval.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
